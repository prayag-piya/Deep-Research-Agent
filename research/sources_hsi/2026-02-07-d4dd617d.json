[
    {
        "url": "https://www.geeksforgeeks.org/deep-learning/vision-transformer-vit-architecture/",
        "title": "Vision Transformer (ViT) Architecture",
        "content": "Vision Transformer (ViT) is a deep learning architecture that applies the Transformer model to images. Instead of relying on convolutions,",
        "score": 0.78788257,
        "raw_content": null,
        "id": 0
    },
    {
        "url": "https://manimalakumar-29300.medium.com/generative-ai-model-architecture-part3-vision-transformer-363a0a81a7c2",
        "title": "Generative AI Model Architecture \u2014 Part3: Vision Transformer",
        "content": "# Generative AI Model Architecture \u2014 Part3: Vision Transformer. Vision Transformer (ViT) adapts the Transformer architecture \u2014 originally designed to process sequential data \u2014 to make sense of non-sequential data. There were specialized CNN model architectures for each of the fundamental vision tasks: image classification, object detection, segmentation, etc. ViTs develop a broad understanding of the image content, and overcome such limitations of CNNs. LVMs \u2014 the modern computer vision models \u2014 are increasingly built on ViT-based architectures. Since ViTs process image patches in parallel, position embeddings tell the model where each patch belongs. Self-distillation is a student-teacher architecture, where a student model learns to predict the output of a teacher model. This means the teacher is not a fixed, pre-trained model \u2014 over time, as the student improves, the teacher is updated to be a moving average of the student, ensuring the teacher remains a reliable target and promoting progressive learning. Part 4: Generative AI Model Architecture \u2014 Part4: Vision Language Model.",
        "score": 0.7602419,
        "raw_content": null,
        "id": 1
    },
    {
        "url": "https://blog.roboflow.com/vision-transformers/",
        "title": "Vision Transformers Explained: The Future of Computer ...",
        "content": "Transformers are a powerful deep learning architecture originally developed for natural language processing (NLP) tasks such as machine translation,",
        "score": 0.6659544,
        "raw_content": null,
        "id": 2
    },
    {
        "url": "https://www.codecademy.com/article/vision-transformers-working-architecture-explained",
        "title": "How do Vision Transformers Work? Architecture Explained",
        "content": "Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial. This architectural shift enables ViTs to use the powerful self-attention mechanism that made transformers successful in NLP, allowing them to model long-range dependencies and global relationships in images more effectively than CNNs. The ViT architecture consists of four main components: patch embedding, positional encoding, transformer encoder, and classification head. The original vision transformer architecture proposed in the paper \u201cAn Image Is Worth 16X16 Words\u201d contains different components such as patch embedding, positional encoding, transformer encoder, multilayer perceptron head, etc, and looks as follows:. * **Global feature modeling**: In vision transformers, every patch of an image interacts with every other patch during the self-attention mechanism. It\u2019s a self-supervised learning framework for vision transformers that allows a ViT model to learn robust and invariant features from images without needing human-labeled data. Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial.",
        "score": 0.61363083,
        "raw_content": null,
        "id": 3
    },
    {
        "url": "https://www.coursera.org/articles/transformers-vs-convolutional-neural-networks",
        "title": "Transformers vs. Convolutional Neural Networks",
        "content": "Transformers and convolutional neural networks are both powerful deep learning algorithms for computer vision, but they work differently and have different strengths and weaknesses. If you want to use a deep learning neural network to extract meaningful data from images, you may decide to use a convolutional neural network or a visual transformer. Explore transformers versus convolutional neural networks to learn how each of these models works, what you can use them for, and how they compare to each other. A transformer is a deep learning neural network that uses an encoder/decoder architecture to first break down the input into the base characteristics that define the data, then use its understanding of patterns to rebuild the data with original but similar information. Transformer models and convolutional neural networks are both deep learning models that you can use for computer vision, but they are different in how they empower computer vision.",
        "score": 0.5735118,
        "raw_content": null,
        "id": 4
    },
    {
        "url": "https://www.geeksforgeeks.org/deep-learning/vision-transformer-vit-architecture/",
        "title": "Vision Transformer (ViT) Architecture",
        "content": "Vision Transformer (ViT) is a deep learning architecture that applies the Transformer model to images. Instead of relying on convolutions,",
        "score": 0.78788257,
        "raw_content": null,
        "id": 6
    },
    {
        "url": "https://manimalakumar-29300.medium.com/generative-ai-model-architecture-part3-vision-transformer-363a0a81a7c2",
        "title": "Generative AI Model Architecture \u2014 Part3: Vision Transformer",
        "content": "# Generative AI Model Architecture \u2014 Part3: Vision Transformer. Vision Transformer (ViT) adapts the Transformer architecture \u2014 originally designed to process sequential data \u2014 to make sense of non-sequential data. There were specialized CNN model architectures for each of the fundamental vision tasks: image classification, object detection, segmentation, etc. ViTs develop a broad understanding of the image content, and overcome such limitations of CNNs. LVMs \u2014 the modern computer vision models \u2014 are increasingly built on ViT-based architectures. Since ViTs process image patches in parallel, position embeddings tell the model where each patch belongs. Self-distillation is a student-teacher architecture, where a student model learns to predict the output of a teacher model. This means the teacher is not a fixed, pre-trained model \u2014 over time, as the student improves, the teacher is updated to be a moving average of the student, ensuring the teacher remains a reliable target and promoting progressive learning. Part 4: Generative AI Model Architecture \u2014 Part4: Vision Language Model.",
        "score": 0.7602419,
        "raw_content": null,
        "id": 7
    },
    {
        "url": "https://blog.roboflow.com/vision-transformers/",
        "title": "Vision Transformers Explained: The Future of Computer ...",
        "content": "Transformers are a powerful deep learning architecture originally developed for natural language processing (NLP) tasks such as machine translation,",
        "score": 0.6659544,
        "raw_content": null,
        "id": 8
    },
    {
        "url": "https://www.codecademy.com/article/vision-transformers-working-architecture-explained",
        "title": "How do Vision Transformers Work? Architecture Explained",
        "content": "Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial. This architectural shift enables ViTs to use the powerful self-attention mechanism that made transformers successful in NLP, allowing them to model long-range dependencies and global relationships in images more effectively than CNNs. The ViT architecture consists of four main components: patch embedding, positional encoding, transformer encoder, and classification head. The original vision transformer architecture proposed in the paper \u201cAn Image Is Worth 16X16 Words\u201d contains different components such as patch embedding, positional encoding, transformer encoder, multilayer perceptron head, etc, and looks as follows:. * **Global feature modeling**: In vision transformers, every patch of an image interacts with every other patch during the self-attention mechanism. It\u2019s a self-supervised learning framework for vision transformers that allows a ViT model to learn robust and invariant features from images without needing human-labeled data. Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial.",
        "score": 0.61363083,
        "raw_content": null,
        "id": 9
    },
    {
        "url": "https://www.coursera.org/articles/transformers-vs-convolutional-neural-networks",
        "title": "Transformers vs. Convolutional Neural Networks",
        "content": "Transformers and convolutional neural networks are both powerful deep learning algorithms for computer vision, but they work differently and have different strengths and weaknesses. If you want to use a deep learning neural network to extract meaningful data from images, you may decide to use a convolutional neural network or a visual transformer. Explore transformers versus convolutional neural networks to learn how each of these models works, what you can use them for, and how they compare to each other. A transformer is a deep learning neural network that uses an encoder/decoder architecture to first break down the input into the base characteristics that define the data, then use its understanding of patterns to rebuild the data with original but similar information. Transformer models and convolutional neural networks are both deep learning models that you can use for computer vision, but they are different in how they empower computer vision.",
        "score": 0.5735118,
        "raw_content": null,
        "id": 10
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-34756-z",
        "title": "A multiscale transformer with spatial attention for ...",
        "content": "each tailored to extract valuable insights from the multi-dimensional HSI data. Among these approaches, the k-nearest neighbour18, 11514\u201311521. [...] designs and lightweight transformer modules to further reduce computational costs without compromising overall performance, making the framework even more suitable for HSI classification tasks. [...] To tackle these challenges, we propose MTSA-Net, a novel multiscale transformer framework with spatial attention for HSI classification. The model begins with a 3D convolution layer to extract shallow spectral\u2013spatial features, followed by a 2D convolution layer and a spatial attention module. This design reduces feature redundancy, mitigates inaccuracies that often arise in deeper networks, and emphasizes the most discriminative spatial features, thereby alleviating the limited spatial resolution of HSIs. The refined feature vectors are then processed by multiple parallel transformer encoder branches with varying hidden dimensions, enabling simultaneous modeling of fine-grained local details, intermediate relationships, and global representations. Finally, a multiscale feature fusion",
        "score": 0.98975134,
        "raw_content": null,
        "id": 11
    },
    {
        "url": "https://worldmodelbench.github.io/static/pdfs/7_Adaptive_Attention_Guided_Ma.pdf",
        "title": "Adaptive Attention-Guided Masking in Vision Transformers ...",
        "content": "Art. no.\n5511817. 2, 5, 7 Adaptive Attention-Guided Masking in Vision Transformers for Self-Supervised Hyperspectral Feature Learning Supplementary Material 6. Appendix In recent years, world models like Vision Transformers (ViTs)  have demonstrated impressive performance in various computer vision [6, 8, 13] tasks, including hyper-spectral image (HSI) classification . However, their ap-plication to HSI faces several limitations due to the unique challenges of hyperspectral data.\nChallenges. First, the high dimensionality of HSI, with several spectral bands, complicates processing for conven-tional ViTs, which are optimized for RGB images with only three channels. This necessitates extensive modifications to handle spectral complexity. [...] Hyperspectral image (HSI) classification has rapidly ad-vanced with diverse methods tackling spectral-spatial data challenges.\nEarly models like 2-DCNN  combined simple 2D convolutions with fully connected layers, while SPRN  improved spatial features using attention mech-anisms and residual blocks. 3-DCNN  introduced 3D convolutions to jointly capture spectral and spatial infor-mation.\nHybrid models like HybridSN  merged 2D and 3D CNNs for richer spectral-spatial representations.\nThe emergence of transformer-based approaches marked a pivotal shift, emphasizing efficient feature extraction and long-range dependency modeling. GAHT  and Mor-phFormer  leveraged CNN-transformer hybrids and self-attention for enhanced spectral-spatial learning. [...] 4. Analysis of Results In this section, we comprehensively analyze results in var-ious scenarios with three HSI datasets : Indian Pines, Salinas, and Botswana using the two performance metrics: Overall Accuracy (OA) and Cohen\u2019s Kappa coefficient (\u03ba).",
        "score": 0.98975134,
        "raw_content": null,
        "id": 12
    },
    {
        "url": "https://arxiv.org/html/2502.06894v1",
        "title": "AI-Driven HSI: Multimodality, Fusion, Challenges, and the ...",
        "content": "HSI, helping researchers select the right architecture for their problems. [...] intensity measurements are collected across spectral bands and represented as a 3D data cube. The observed intensity at wavelength for pixel can be expressed as: [...] TABLE IV: Deep Learning Models and their Applications in HSI and Multimodal HSI",
        "score": 0.98934746,
        "raw_content": null,
        "id": 13
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-12489-3",
        "title": "Spectral-spatial wave and frequency interactive ...",
        "content": "The proposed framework is designed for HSI. It utilizes a combination of spectral spatial feature extractor with frequency domain transformer encoder block to extract phase and frequency features. [...] Figure 1 depicts the proposed model, this section will elucidate the model\u2019s structure and it\u2019s functioning. The feature pre-processing stage begins by feeding HSI patches into 3D convolutional layer and 2D convolutional layer for low-level feature extraction after applying Feature Augmentation. Shallow feature maps generated by these layers are then processed using pixel operations to tokenize the features before passing them into the transformer encoder Blocks. The core of the model lies in the transformer Blocks, which are repeated N times to enable interactions between tokens representing different spectral spatial locations in the frequency domain. Finally, a classification head, consisting of a Global Average Pooling (GAP) layer is employed to assign a label to each pixel. [...] (2021).\") treated HSI patches as sequential data from the spectral dimension, utilizing group-spectral embedding and a transformer encoder module to capture locally detailed spectral representations. Roy et al.31, 7831\u20137843.",
        "score": 0.98282325,
        "raw_content": null,
        "id": 14
    },
    {
        "url": "https://arxiv.org/html/2508.08107v1",
        "title": "Hyperspectral Imaging",
        "content": "HSI focuses on the optical window of the electromagnetic spectrum (see Fig. 1A), typically covering wavelengths from 380 to 2500 nm. This window usually encompasses the visible light (400-700 nm), near-infrared (NIR), and shortwave infrared (SWIR) regions, as shown in Fig. 1B. Notably, the inclusion of the 380-400 nm region further enhances sensitivity to pigment absorption and subtle surface reflectance variations, particularly in the violet and near-ultraviolet range. An example scene illustrating typical hyperspectral data is shown in Fig. 1C, highlighting that HSI routinely captures over hundreds of spectral channels at high spectral resolution (commonly 5-10 nm). [...] Transform-based approaches reduce dimensionality by projecting the original data into a lower-dimensional subspace that preserves the most informative spectral or spatial-spectral structures. Among these, principal component analysis (PCA)  and minimum noise fraction (MNF)  are two of the most representative methods, often serving as baseline techniques for illustrating DR in HSI due to their easy-interpretable and friendly-practical linearized modeling. More specifically, let be the spectral vector at the -th pixel, where is the number of spectral bands and is the total number of pixels in the HSI image. PCA seeks an orthogonal transformation that maximizes the variance of the projected data:\n\n|  |  |  |\n --- \n|  |  | (5) |\n|  | [...] Table 1: Comparative summary of different HSI platforms.",
        "score": 0.9693242,
        "raw_content": null,
        "id": 15
    },
    {
        "url": "https://www.geeksforgeeks.org/deep-learning/vision-transformer-vit-architecture/",
        "title": "Vision Transformer (ViT) Architecture",
        "content": "Vision Transformer (ViT) is a deep learning architecture that applies the Transformer model to images. Instead of relying on convolutions,",
        "score": 0.78788257,
        "raw_content": null,
        "id": 16
    },
    {
        "url": "https://manimalakumar-29300.medium.com/generative-ai-model-architecture-part3-vision-transformer-363a0a81a7c2",
        "title": "Generative AI Model Architecture \u2014 Part3: Vision Transformer",
        "content": "# Generative AI Model Architecture \u2014 Part3: Vision Transformer. Vision Transformer (ViT) adapts the Transformer architecture \u2014 originally designed to process sequential data \u2014 to make sense of non-sequential data. There were specialized CNN model architectures for each of the fundamental vision tasks: image classification, object detection, segmentation, etc. ViTs develop a broad understanding of the image content, and overcome such limitations of CNNs. LVMs \u2014 the modern computer vision models \u2014 are increasingly built on ViT-based architectures. Since ViTs process image patches in parallel, position embeddings tell the model where each patch belongs. Self-distillation is a student-teacher architecture, where a student model learns to predict the output of a teacher model. This means the teacher is not a fixed, pre-trained model \u2014 over time, as the student improves, the teacher is updated to be a moving average of the student, ensuring the teacher remains a reliable target and promoting progressive learning. Part 4: Generative AI Model Architecture \u2014 Part4: Vision Language Model.",
        "score": 0.7602419,
        "raw_content": null,
        "id": 17
    },
    {
        "url": "https://blog.roboflow.com/vision-transformers/",
        "title": "Vision Transformers Explained: The Future of Computer ...",
        "content": "Transformers are a powerful deep learning architecture originally developed for natural language processing (NLP) tasks such as machine translation,",
        "score": 0.6659544,
        "raw_content": null,
        "id": 18
    },
    {
        "url": "https://www.codecademy.com/article/vision-transformers-working-architecture-explained",
        "title": "How do Vision Transformers Work? Architecture Explained",
        "content": "Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial. This architectural shift enables ViTs to use the powerful self-attention mechanism that made transformers successful in NLP, allowing them to model long-range dependencies and global relationships in images more effectively than CNNs. The ViT architecture consists of four main components: patch embedding, positional encoding, transformer encoder, and classification head. The original vision transformer architecture proposed in the paper \u201cAn Image Is Worth 16X16 Words\u201d contains different components such as patch embedding, positional encoding, transformer encoder, multilayer perceptron head, etc, and looks as follows:. * **Global feature modeling**: In vision transformers, every patch of an image interacts with every other patch during the self-attention mechanism. It\u2019s a self-supervised learning framework for vision transformers that allows a ViT model to learn robust and invariant features from images without needing human-labeled data. Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial.",
        "score": 0.61363083,
        "raw_content": null,
        "id": 19
    },
    {
        "url": "https://www.coursera.org/articles/transformers-vs-convolutional-neural-networks",
        "title": "Transformers vs. Convolutional Neural Networks",
        "content": "Transformers and convolutional neural networks are both powerful deep learning algorithms for computer vision, but they work differently and have different strengths and weaknesses. If you want to use a deep learning neural network to extract meaningful data from images, you may decide to use a convolutional neural network or a visual transformer. Explore transformers versus convolutional neural networks to learn how each of these models works, what you can use them for, and how they compare to each other. A transformer is a deep learning neural network that uses an encoder/decoder architecture to first break down the input into the base characteristics that define the data, then use its understanding of patterns to rebuild the data with original but similar information. Transformer models and convolutional neural networks are both deep learning models that you can use for computer vision, but they are different in how they empower computer vision.",
        "score": 0.5735118,
        "raw_content": null,
        "id": 20
    },
    {
        "url": "https://www.geeksforgeeks.org/deep-learning/vision-transformer-vit-architecture/",
        "title": "Vision Transformer (ViT) Architecture",
        "content": "Vision Transformer (ViT) is a deep learning architecture that applies the Transformer model to images. Instead of relying on convolutions,",
        "score": 0.78788257,
        "raw_content": null,
        "id": 21
    },
    {
        "url": "https://manimalakumar-29300.medium.com/generative-ai-model-architecture-part3-vision-transformer-363a0a81a7c2",
        "title": "Generative AI Model Architecture \u2014 Part3: Vision Transformer",
        "content": "# Generative AI Model Architecture \u2014 Part3: Vision Transformer. Vision Transformer (ViT) adapts the Transformer architecture \u2014 originally designed to process sequential data \u2014 to make sense of non-sequential data. There were specialized CNN model architectures for each of the fundamental vision tasks: image classification, object detection, segmentation, etc. ViTs develop a broad understanding of the image content, and overcome such limitations of CNNs. LVMs \u2014 the modern computer vision models \u2014 are increasingly built on ViT-based architectures. Since ViTs process image patches in parallel, position embeddings tell the model where each patch belongs. Self-distillation is a student-teacher architecture, where a student model learns to predict the output of a teacher model. This means the teacher is not a fixed, pre-trained model \u2014 over time, as the student improves, the teacher is updated to be a moving average of the student, ensuring the teacher remains a reliable target and promoting progressive learning. Part 4: Generative AI Model Architecture \u2014 Part4: Vision Language Model.",
        "score": 0.7602419,
        "raw_content": null,
        "id": 22
    },
    {
        "url": "https://blog.roboflow.com/vision-transformers/",
        "title": "Vision Transformers Explained: The Future of Computer ...",
        "content": "Transformers are a powerful deep learning architecture originally developed for natural language processing (NLP) tasks such as machine translation,",
        "score": 0.6659544,
        "raw_content": null,
        "id": 23
    },
    {
        "url": "https://www.codecademy.com/article/vision-transformers-working-architecture-explained",
        "title": "How do Vision Transformers Work? Architecture Explained",
        "content": "Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial. This architectural shift enables ViTs to use the powerful self-attention mechanism that made transformers successful in NLP, allowing them to model long-range dependencies and global relationships in images more effectively than CNNs. The ViT architecture consists of four main components: patch embedding, positional encoding, transformer encoder, and classification head. The original vision transformer architecture proposed in the paper \u201cAn Image Is Worth 16X16 Words\u201d contains different components such as patch embedding, positional encoding, transformer encoder, multilayer perceptron head, etc, and looks as follows:. * **Global feature modeling**: In vision transformers, every patch of an image interacts with every other patch during the self-attention mechanism. It\u2019s a self-supervised learning framework for vision transformers that allows a ViT model to learn robust and invariant features from images without needing human-labeled data. Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial.",
        "score": 0.61363083,
        "raw_content": null,
        "id": 24
    },
    {
        "url": "https://www.coursera.org/articles/transformers-vs-convolutional-neural-networks",
        "title": "Transformers vs. Convolutional Neural Networks",
        "content": "Transformers and convolutional neural networks are both powerful deep learning algorithms for computer vision, but they work differently and have different strengths and weaknesses. If you want to use a deep learning neural network to extract meaningful data from images, you may decide to use a convolutional neural network or a visual transformer. Explore transformers versus convolutional neural networks to learn how each of these models works, what you can use them for, and how they compare to each other. A transformer is a deep learning neural network that uses an encoder/decoder architecture to first break down the input into the base characteristics that define the data, then use its understanding of patterns to rebuild the data with original but similar information. Transformer models and convolutional neural networks are both deep learning models that you can use for computer vision, but they are different in how they empower computer vision.",
        "score": 0.5735118,
        "raw_content": null,
        "id": 25
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-34756-z",
        "title": "A multiscale transformer with spatial attention for ...",
        "content": "each tailored to extract valuable insights from the multi-dimensional HSI data. Among these approaches, the k-nearest neighbour18, 11514\u201311521. [...] designs and lightweight transformer modules to further reduce computational costs without compromising overall performance, making the framework even more suitable for HSI classification tasks. [...] To tackle these challenges, we propose MTSA-Net, a novel multiscale transformer framework with spatial attention for HSI classification. The model begins with a 3D convolution layer to extract shallow spectral\u2013spatial features, followed by a 2D convolution layer and a spatial attention module. This design reduces feature redundancy, mitigates inaccuracies that often arise in deeper networks, and emphasizes the most discriminative spatial features, thereby alleviating the limited spatial resolution of HSIs. The refined feature vectors are then processed by multiple parallel transformer encoder branches with varying hidden dimensions, enabling simultaneous modeling of fine-grained local details, intermediate relationships, and global representations. Finally, a multiscale feature fusion",
        "score": 0.98975134,
        "raw_content": null,
        "id": 26
    },
    {
        "url": "https://worldmodelbench.github.io/static/pdfs/7_Adaptive_Attention_Guided_Ma.pdf",
        "title": "Adaptive Attention-Guided Masking in Vision Transformers ...",
        "content": "Art. no.\n5511817. 2, 5, 7 Adaptive Attention-Guided Masking in Vision Transformers for Self-Supervised Hyperspectral Feature Learning Supplementary Material 6. Appendix In recent years, world models like Vision Transformers (ViTs)  have demonstrated impressive performance in various computer vision [6, 8, 13] tasks, including hyper-spectral image (HSI) classification . However, their ap-plication to HSI faces several limitations due to the unique challenges of hyperspectral data.\nChallenges. First, the high dimensionality of HSI, with several spectral bands, complicates processing for conven-tional ViTs, which are optimized for RGB images with only three channels. This necessitates extensive modifications to handle spectral complexity. [...] Hyperspectral image (HSI) classification has rapidly ad-vanced with diverse methods tackling spectral-spatial data challenges.\nEarly models like 2-DCNN  combined simple 2D convolutions with fully connected layers, while SPRN  improved spatial features using attention mech-anisms and residual blocks. 3-DCNN  introduced 3D convolutions to jointly capture spectral and spatial infor-mation.\nHybrid models like HybridSN  merged 2D and 3D CNNs for richer spectral-spatial representations.\nThe emergence of transformer-based approaches marked a pivotal shift, emphasizing efficient feature extraction and long-range dependency modeling. GAHT  and Mor-phFormer  leveraged CNN-transformer hybrids and self-attention for enhanced spectral-spatial learning. [...] 4. Analysis of Results In this section, we comprehensively analyze results in var-ious scenarios with three HSI datasets : Indian Pines, Salinas, and Botswana using the two performance metrics: Overall Accuracy (OA) and Cohen\u2019s Kappa coefficient (\u03ba).",
        "score": 0.98975134,
        "raw_content": null,
        "id": 27
    },
    {
        "url": "https://arxiv.org/html/2502.06894v1",
        "title": "AI-Driven HSI: Multimodality, Fusion, Challenges, and the ...",
        "content": "HSI, helping researchers select the right architecture for their problems. [...] intensity measurements are collected across spectral bands and represented as a 3D data cube. The observed intensity at wavelength for pixel can be expressed as: [...] TABLE IV: Deep Learning Models and their Applications in HSI and Multimodal HSI",
        "score": 0.98934746,
        "raw_content": null,
        "id": 28
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-12489-3",
        "title": "Spectral-spatial wave and frequency interactive ...",
        "content": "The proposed framework is designed for HSI. It utilizes a combination of spectral spatial feature extractor with frequency domain transformer encoder block to extract phase and frequency features. [...] Figure 1 depicts the proposed model, this section will elucidate the model\u2019s structure and it\u2019s functioning. The feature pre-processing stage begins by feeding HSI patches into 3D convolutional layer and 2D convolutional layer for low-level feature extraction after applying Feature Augmentation. Shallow feature maps generated by these layers are then processed using pixel operations to tokenize the features before passing them into the transformer encoder Blocks. The core of the model lies in the transformer Blocks, which are repeated N times to enable interactions between tokens representing different spectral spatial locations in the frequency domain. Finally, a classification head, consisting of a Global Average Pooling (GAP) layer is employed to assign a label to each pixel. [...] (2021).\") treated HSI patches as sequential data from the spectral dimension, utilizing group-spectral embedding and a transformer encoder module to capture locally detailed spectral representations. Roy et al.31, 7831\u20137843.",
        "score": 0.98282325,
        "raw_content": null,
        "id": 29
    },
    {
        "url": "https://arxiv.org/html/2508.08107v1",
        "title": "Hyperspectral Imaging",
        "content": "HSI focuses on the optical window of the electromagnetic spectrum (see Fig. 1A), typically covering wavelengths from 380 to 2500 nm. This window usually encompasses the visible light (400-700 nm), near-infrared (NIR), and shortwave infrared (SWIR) regions, as shown in Fig. 1B. Notably, the inclusion of the 380-400 nm region further enhances sensitivity to pigment absorption and subtle surface reflectance variations, particularly in the violet and near-ultraviolet range. An example scene illustrating typical hyperspectral data is shown in Fig. 1C, highlighting that HSI routinely captures over hundreds of spectral channels at high spectral resolution (commonly 5-10 nm). [...] Transform-based approaches reduce dimensionality by projecting the original data into a lower-dimensional subspace that preserves the most informative spectral or spatial-spectral structures. Among these, principal component analysis (PCA)  and minimum noise fraction (MNF)  are two of the most representative methods, often serving as baseline techniques for illustrating DR in HSI due to their easy-interpretable and friendly-practical linearized modeling. More specifically, let be the spectral vector at the -th pixel, where is the number of spectral bands and is the total number of pixels in the HSI image. PCA seeks an orthogonal transformation that maximizes the variance of the projected data:\n\n|  |  |  |\n --- \n|  |  | (5) |\n|  | [...] Table 1: Comparative summary of different HSI platforms.",
        "score": 0.9693242,
        "raw_content": null,
        "id": 30
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-33943-2",
        "title": "Multi-scale boundary-aware network for remote sensing ...",
        "content": "classification of high-resolution remote sensing images. By simultaneously leveraging the local features of CNN and the global dependencies of Transformer through multi-scale self-attention and bidirectional knowledge distillation, it improves the accuracy of remote sensing semantic segmentation. Fan et al.33:17.\") proposed a Multidimensional Information Fusion Network for high-resolution remote sensing image segmentation by integrating CNN and Transformer and introducing frequency information. By multi-scale integration of local details, global semantics and frequency domain features, the model\u2019s ability to identify fine-grained boundaries and scale-varying targets is enhanced, effectively alleviating complex interference factors such as inter-class ambiguity. The hybrid attention [...] Despite significant advances in remote sensing semantic segmentation achieved by existing CNNs, Transformers, and hybrid approaches, high-resolution imagery remains challenging: Insufficient multi-scale feature characterisation prevents the model from capturing boundary features, leading to blurred segmentation boundaries caused by classification errors at the junctures of different semantic categories. Furthermore, recent research on remote sensing image segmentation has underscored the importance of boundary-aware modelling in dense prediction tasks22.\"), 23.\"). These considerations have prompted us to develop a unified framework that synergistically enhances both multi-scale representation and boundary-aware capabilities. To address these challenges, we propose MSBANet\u2014a specially [...] Traditional CNNs have remarkable effects in extracting local spatial details, but their capabilities in modeling global contexts are limited. Although Transformer-based methods can capture long-range dependencies, they often ignore fine structures and boundary information and have high computational complexity. In semantic segmentation tasks, both global information and local information are crucial for accurately understanding the semantic structure of images. Researchers have begun to combine CNN with Transformer models to fully leverage their respective advantages. Chen et al. proposed CTSeg32.\"), a novel CNN and ViT collaborative segmentation framework following the encoder-decoder architecture for land-use/land-cover classification of high-resolution remote sensing images. By",
        "score": 0.38512084,
        "raw_content": null,
        "id": 31
    },
    {
        "url": "https://www.sciencedirect.com/science/article/abs/pii/S1350449525001173",
        "title": "Edge Feature Enhanced Transformer Network for RGB and ...",
        "content": "In real-world object detection scenarios, factors such as rain, fog, occlusion, varying illumination, and low resolution hinder the ability of visible light imaging to achieve high-precision target detection performance . In comparison to RGB single-modality based object detection, RGB-infrared multi-modality based object detection is able to leverage diverse and complementary information. As there are substantial visual disparities between the two modal images. For instance, RGB images are sensitive to color and texture, where objects exhibit clearer texture details compared to that in infrared images during the day while diminishing under nighttime conditions (see Fig. 1(a) and Fig. 1(b)). Instead, infrared (IR) images are sensitive for temperature variation, they enable to offer [...] images achieve high- precision detection and recognition by extracting local spatial\u2013temporal features. However, convolutional operations is not capable for global information perception and struggle to model dependencies between features. Therefore, there is still potential to improve the fusion quality by taking consideration of the global feature learning. [...] In multispectral object detection, the complementary nature of infrared and visible images remains underexploited due to perceptual differences and spatial misalignment between modalities, posing significant challenges to accurate detection. We propose MRT-DETR (Multispectral RT-DETR), an end-to-end multispectral real-time detection framework that addresses weak alignment and complex lighting conditions by combining brightness-aware weighting with deformable alignment. Built on the RT-DETR backbone, our method introduces an Early-stage Deformable Alignment (EDA) module that learns attention-guided offsets at shallow layers to explicitly align infrared features to visible features in the spatial domain. Additionally, a Dual-Branch Brightness Weighting (DBW) module derives patch-wise",
        "score": 0.3059777,
        "raw_content": null,
        "id": 32
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-34756-z",
        "title": "A multiscale transformer with spatial attention for ...",
        "content": "Article  ADS   Google Scholar\n7. Hong, D. et al. Spectralformer: Rethinking hyperspectral image classification with transformers. IEEE Transactions on Geoscience and Remote Sensing 60, 1\u201315 (2021).\n\n   Article   Google Scholar\n8. Zhou, P., Han, J., Cheng, G. & Zhang, B. Learning compact and discriminative stacked autoencoder for hyperspectral image classification. IEEE Transactions on Geoscience and Remote Sensing 57(7), 4823\u20134833 (2019).\n\n   Article  ADS   Google Scholar\n9. Zheng, X., Yuan, Y. & Lu, X. Dimensionality reduction by spatial-spectral preservation in selected bands. IEEE Transactions on Geoscience and Remote Sensing 55(9), 5185\u20135197 (2017). [...] The Houston 2013 (H-13) dataset was gathered using the ITRES CASI-1500 sensor and covers the University of Houston premises and the neighbouring rural regions. This dataset has been publicly employed for assessing the effectiveness of HSI classification methods. This dataset has an image size of \\(349 \\times 1905\\) pixels and includes 144 spectral bands. It encompasses 15 challenging distinct classes, making it a valuable resource for land cover classification studies. The dataset samples have been divided into training, validation, and test sets, with the allocation details provided in Table 4. [...] Furthermore, a hybrid approach incorporating both CNN and Transformer networks48.\") was proposed to effectively calculate spectral-spatial and semantic features for HSI classification. In66.\"), Swin Transformer was presented, which calculates representations through a shifted window strategy. This approach allows for efficient processing of visual data with diverse scales and higher resolutions. Adopting this strategy effectively addresses the challenges presented by scale and resolution disparities, rendering it well-suited for HSI classification.\n\n## Proposed methodology",
        "score": 0.2640859,
        "raw_content": null,
        "id": 33
    },
    {
        "url": "https://arxiv.org/html/2507.22791v1",
        "title": "Modality-Aware Feature Matching: A Comprehensive ...",
        "content": "In this survey, our discussion addresses unique challenges and methodologies related to single-modal feature matching (e.g., RGB, depth, medical imaging) and cross-modal scenarios (e.g., medical image registration, vision-language integration). We delineate the progression from classical detector-based pipelines towards contemporary detector-free solutions. Figure 1 illustrates the overall pipeline of the survey, clearly mapping the evolution of feature matching methodologies across various data modalities. Furthermore, Figure 2 offers representative examples of the results of the matching of modality-awarecharacteristics.\n\n## 2. RGB Images [...] In summary, deep learning-based methods have greatly advanced the state of RGB image matching. They have produced more repeatable detectors (learning where to detect points that survive viewpoint/illumination changes), more discriminative descriptors (learning how to describe patches for higher distinctiveness), and even entirely new paradigms for matching (learning to match with attention mechanisms, and end-to-end trainable pipelines). Table 1 highlights some of the influential methods from both the handcrafted and deep learning eras. These modern techniques, when evaluated on challenging benchmarks, significantly outperform classic methods in terms of matching precision and robustness, although often at a higher computational cost. Nonetheless, due to their task-specific optimizations [...] ## 2. RGB Images\n\nFeature matching in RGB images traditionally follows a pipeline of feature detection, feature description, and feature matching. Over the decades, a lot of methods have been proposed for each stage of this pipeline, from handcrafted approaches to more recent deep learning-based techniques.\n\n### 2.1. Handcrafted Feature Detectors and Descriptors",
        "score": 0.26285648,
        "raw_content": null,
        "id": 34
    },
    {
        "url": "https://link.springer.com/article/10.1007/s10462-025-11169-y",
        "title": "A review of hyperspectral image classification based on graph ...",
        "content": "\")) of hyperspectral imaging (HSI) pose significant challenges to achieving accurate HSIC (Sawant and Prabukumar 2020 A survey of band selection techniques for hyperspectral image classification. J Spectr Imaging 9(1):5. \n                  \n                  \n                \")). Specifically, the large number of spectral bands and data points increases computational complexity (Ullah et al. 2024 Conventional to deep ensemble methods for hyperspectral image classification: a comprehensive survey. IEEE J Sel Top Appl Earth Obs Remote Sens. [...] ### 2.2 Technologies for hyperspectral image classification\n\nOptical remote sensing images can be categorized into three main types based on the number of spectral bands: panchromatic images with a single band, multiband RGB images, and hyperspectral images (HSI) containing hundreds of continuous, narrow spectral bands (Mather and Tso 2016 Classification methods for remotely sensed data. CRC Press, Boca Raton\")). HSI, with its unique technical advantages, achieves a deep integration of imaging and spectral technologies, enabling comprehensive data acquisition in both two-dimensional spatial geometry and one-dimensional spectral information. [...] Although deep learning has achieved remarkable breakthroughs in hyperspectral image classification (HSIC), the unique characteristics of hyperspectral image (HSI) data still present numerous challenges (Wan et al. 2020a Hyperspectral image classification with context-aware dynamic graph convolutional network. IEEE Trans Geosci Remote Sens 59(1):597\u2013612.",
        "score": 0.21413255,
        "raw_content": null,
        "id": 35
    },
    {
        "url": "https://www.geeksforgeeks.org/deep-learning/vision-transformer-vit-architecture/",
        "title": "Vision Transformer (ViT) Architecture",
        "content": "Vision Transformer (ViT) is a deep learning architecture that applies the Transformer model to images. Instead of relying on convolutions,",
        "score": 0.78788257,
        "raw_content": null,
        "id": 36
    },
    {
        "url": "https://manimalakumar-29300.medium.com/generative-ai-model-architecture-part3-vision-transformer-363a0a81a7c2",
        "title": "Generative AI Model Architecture \u2014 Part3: Vision Transformer",
        "content": "# Generative AI Model Architecture \u2014 Part3: Vision Transformer. Vision Transformer (ViT) adapts the Transformer architecture \u2014 originally designed to process sequential data \u2014 to make sense of non-sequential data. There were specialized CNN model architectures for each of the fundamental vision tasks: image classification, object detection, segmentation, etc. ViTs develop a broad understanding of the image content, and overcome such limitations of CNNs. LVMs \u2014 the modern computer vision models \u2014 are increasingly built on ViT-based architectures. Since ViTs process image patches in parallel, position embeddings tell the model where each patch belongs. Self-distillation is a student-teacher architecture, where a student model learns to predict the output of a teacher model. This means the teacher is not a fixed, pre-trained model \u2014 over time, as the student improves, the teacher is updated to be a moving average of the student, ensuring the teacher remains a reliable target and promoting progressive learning. Part 4: Generative AI Model Architecture \u2014 Part4: Vision Language Model.",
        "score": 0.7602419,
        "raw_content": null,
        "id": 37
    },
    {
        "url": "https://blog.roboflow.com/vision-transformers/",
        "title": "Vision Transformers Explained: The Future of Computer ...",
        "content": "Transformers are a powerful deep learning architecture originally developed for natural language processing (NLP) tasks such as machine translation,",
        "score": 0.6659544,
        "raw_content": null,
        "id": 38
    },
    {
        "url": "https://www.codecademy.com/article/vision-transformers-working-architecture-explained",
        "title": "How do Vision Transformers Work? Architecture Explained",
        "content": "Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial. This architectural shift enables ViTs to use the powerful self-attention mechanism that made transformers successful in NLP, allowing them to model long-range dependencies and global relationships in images more effectively than CNNs. The ViT architecture consists of four main components: patch embedding, positional encoding, transformer encoder, and classification head. The original vision transformer architecture proposed in the paper \u201cAn Image Is Worth 16X16 Words\u201d contains different components such as patch embedding, positional encoding, transformer encoder, multilayer perceptron head, etc, and looks as follows:. * **Global feature modeling**: In vision transformers, every patch of an image interacts with every other patch during the self-attention mechanism. It\u2019s a self-supervised learning framework for vision transformers that allows a ViT model to learn robust and invariant features from images without needing human-labeled data. Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial.",
        "score": 0.61363083,
        "raw_content": null,
        "id": 39
    },
    {
        "url": "https://www.coursera.org/articles/transformers-vs-convolutional-neural-networks",
        "title": "Transformers vs. Convolutional Neural Networks",
        "content": "Transformers and convolutional neural networks are both powerful deep learning algorithms for computer vision, but they work differently and have different strengths and weaknesses. If you want to use a deep learning neural network to extract meaningful data from images, you may decide to use a convolutional neural network or a visual transformer. Explore transformers versus convolutional neural networks to learn how each of these models works, what you can use them for, and how they compare to each other. A transformer is a deep learning neural network that uses an encoder/decoder architecture to first break down the input into the base characteristics that define the data, then use its understanding of patterns to rebuild the data with original but similar information. Transformer models and convolutional neural networks are both deep learning models that you can use for computer vision, but they are different in how they empower computer vision.",
        "score": 0.5735118,
        "raw_content": null,
        "id": 40
    },
    {
        "url": "https://www.geeksforgeeks.org/deep-learning/vision-transformer-vit-architecture/",
        "title": "Vision Transformer (ViT) Architecture",
        "content": "Vision Transformer (ViT) is a deep learning architecture that applies the Transformer model to images. Instead of relying on convolutions,",
        "score": 0.78788257,
        "raw_content": null,
        "id": 41
    },
    {
        "url": "https://manimalakumar-29300.medium.com/generative-ai-model-architecture-part3-vision-transformer-363a0a81a7c2",
        "title": "Generative AI Model Architecture \u2014 Part3: Vision Transformer",
        "content": "# Generative AI Model Architecture \u2014 Part3: Vision Transformer. Vision Transformer (ViT) adapts the Transformer architecture \u2014 originally designed to process sequential data \u2014 to make sense of non-sequential data. There were specialized CNN model architectures for each of the fundamental vision tasks: image classification, object detection, segmentation, etc. ViTs develop a broad understanding of the image content, and overcome such limitations of CNNs. LVMs \u2014 the modern computer vision models \u2014 are increasingly built on ViT-based architectures. Since ViTs process image patches in parallel, position embeddings tell the model where each patch belongs. Self-distillation is a student-teacher architecture, where a student model learns to predict the output of a teacher model. This means the teacher is not a fixed, pre-trained model \u2014 over time, as the student improves, the teacher is updated to be a moving average of the student, ensuring the teacher remains a reliable target and promoting progressive learning. Part 4: Generative AI Model Architecture \u2014 Part4: Vision Language Model.",
        "score": 0.7602419,
        "raw_content": null,
        "id": 42
    },
    {
        "url": "https://blog.roboflow.com/vision-transformers/",
        "title": "Vision Transformers Explained: The Future of Computer ...",
        "content": "Transformers are a powerful deep learning architecture originally developed for natural language processing (NLP) tasks such as machine translation,",
        "score": 0.6659544,
        "raw_content": null,
        "id": 43
    },
    {
        "url": "https://www.codecademy.com/article/vision-transformers-working-architecture-explained",
        "title": "How do Vision Transformers Work? Architecture Explained",
        "content": "Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial. This architectural shift enables ViTs to use the powerful self-attention mechanism that made transformers successful in NLP, allowing them to model long-range dependencies and global relationships in images more effectively than CNNs. The ViT architecture consists of four main components: patch embedding, positional encoding, transformer encoder, and classification head. The original vision transformer architecture proposed in the paper \u201cAn Image Is Worth 16X16 Words\u201d contains different components such as patch embedding, positional encoding, transformer encoder, multilayer perceptron head, etc, and looks as follows:. * **Global feature modeling**: In vision transformers, every patch of an image interacts with every other patch during the self-attention mechanism. It\u2019s a self-supervised learning framework for vision transformers that allows a ViT model to learn robust and invariant features from images without needing human-labeled data. Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial.",
        "score": 0.61363083,
        "raw_content": null,
        "id": 44
    },
    {
        "url": "https://www.coursera.org/articles/transformers-vs-convolutional-neural-networks",
        "title": "Transformers vs. Convolutional Neural Networks",
        "content": "Transformers and convolutional neural networks are both powerful deep learning algorithms for computer vision, but they work differently and have different strengths and weaknesses. If you want to use a deep learning neural network to extract meaningful data from images, you may decide to use a convolutional neural network or a visual transformer. Explore transformers versus convolutional neural networks to learn how each of these models works, what you can use them for, and how they compare to each other. A transformer is a deep learning neural network that uses an encoder/decoder architecture to first break down the input into the base characteristics that define the data, then use its understanding of patterns to rebuild the data with original but similar information. Transformer models and convolutional neural networks are both deep learning models that you can use for computer vision, but they are different in how they empower computer vision.",
        "score": 0.5735118,
        "raw_content": null,
        "id": 45
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-34756-z",
        "title": "A multiscale transformer with spatial attention for ...",
        "content": "each tailored to extract valuable insights from the multi-dimensional HSI data. Among these approaches, the k-nearest neighbour18, 11514\u201311521. [...] designs and lightweight transformer modules to further reduce computational costs without compromising overall performance, making the framework even more suitable for HSI classification tasks. [...] To tackle these challenges, we propose MTSA-Net, a novel multiscale transformer framework with spatial attention for HSI classification. The model begins with a 3D convolution layer to extract shallow spectral\u2013spatial features, followed by a 2D convolution layer and a spatial attention module. This design reduces feature redundancy, mitigates inaccuracies that often arise in deeper networks, and emphasizes the most discriminative spatial features, thereby alleviating the limited spatial resolution of HSIs. The refined feature vectors are then processed by multiple parallel transformer encoder branches with varying hidden dimensions, enabling simultaneous modeling of fine-grained local details, intermediate relationships, and global representations. Finally, a multiscale feature fusion",
        "score": 0.98975134,
        "raw_content": null,
        "id": 46
    },
    {
        "url": "https://worldmodelbench.github.io/static/pdfs/7_Adaptive_Attention_Guided_Ma.pdf",
        "title": "Adaptive Attention-Guided Masking in Vision Transformers ...",
        "content": "Art. no.\n5511817. 2, 5, 7 Adaptive Attention-Guided Masking in Vision Transformers for Self-Supervised Hyperspectral Feature Learning Supplementary Material 6. Appendix In recent years, world models like Vision Transformers (ViTs)  have demonstrated impressive performance in various computer vision [6, 8, 13] tasks, including hyper-spectral image (HSI) classification . However, their ap-plication to HSI faces several limitations due to the unique challenges of hyperspectral data.\nChallenges. First, the high dimensionality of HSI, with several spectral bands, complicates processing for conven-tional ViTs, which are optimized for RGB images with only three channels. This necessitates extensive modifications to handle spectral complexity. [...] Hyperspectral image (HSI) classification has rapidly ad-vanced with diverse methods tackling spectral-spatial data challenges.\nEarly models like 2-DCNN  combined simple 2D convolutions with fully connected layers, while SPRN  improved spatial features using attention mech-anisms and residual blocks. 3-DCNN  introduced 3D convolutions to jointly capture spectral and spatial infor-mation.\nHybrid models like HybridSN  merged 2D and 3D CNNs for richer spectral-spatial representations.\nThe emergence of transformer-based approaches marked a pivotal shift, emphasizing efficient feature extraction and long-range dependency modeling. GAHT  and Mor-phFormer  leveraged CNN-transformer hybrids and self-attention for enhanced spectral-spatial learning. [...] 4. Analysis of Results In this section, we comprehensively analyze results in var-ious scenarios with three HSI datasets : Indian Pines, Salinas, and Botswana using the two performance metrics: Overall Accuracy (OA) and Cohen\u2019s Kappa coefficient (\u03ba).",
        "score": 0.98975134,
        "raw_content": null,
        "id": 47
    },
    {
        "url": "https://arxiv.org/html/2502.06894v1",
        "title": "AI-Driven HSI: Multimodality, Fusion, Challenges, and the ...",
        "content": "HSI, helping researchers select the right architecture for their problems. [...] intensity measurements are collected across spectral bands and represented as a 3D data cube. The observed intensity at wavelength for pixel can be expressed as: [...] TABLE IV: Deep Learning Models and their Applications in HSI and Multimodal HSI",
        "score": 0.98934746,
        "raw_content": null,
        "id": 48
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-12489-3",
        "title": "Spectral-spatial wave and frequency interactive ...",
        "content": "The proposed framework is designed for HSI. It utilizes a combination of spectral spatial feature extractor with frequency domain transformer encoder block to extract phase and frequency features. [...] Figure 1 depicts the proposed model, this section will elucidate the model\u2019s structure and it\u2019s functioning. The feature pre-processing stage begins by feeding HSI patches into 3D convolutional layer and 2D convolutional layer for low-level feature extraction after applying Feature Augmentation. Shallow feature maps generated by these layers are then processed using pixel operations to tokenize the features before passing them into the transformer encoder Blocks. The core of the model lies in the transformer Blocks, which are repeated N times to enable interactions between tokens representing different spectral spatial locations in the frequency domain. Finally, a classification head, consisting of a Global Average Pooling (GAP) layer is employed to assign a label to each pixel. [...] (2021).\") treated HSI patches as sequential data from the spectral dimension, utilizing group-spectral embedding and a transformer encoder module to capture locally detailed spectral representations. Roy et al.31, 7831\u20137843.",
        "score": 0.98282325,
        "raw_content": null,
        "id": 49
    },
    {
        "url": "https://arxiv.org/html/2508.08107v1",
        "title": "Hyperspectral Imaging",
        "content": "HSI focuses on the optical window of the electromagnetic spectrum (see Fig. 1A), typically covering wavelengths from 380 to 2500 nm. This window usually encompasses the visible light (400-700 nm), near-infrared (NIR), and shortwave infrared (SWIR) regions, as shown in Fig. 1B. Notably, the inclusion of the 380-400 nm region further enhances sensitivity to pigment absorption and subtle surface reflectance variations, particularly in the violet and near-ultraviolet range. An example scene illustrating typical hyperspectral data is shown in Fig. 1C, highlighting that HSI routinely captures over hundreds of spectral channels at high spectral resolution (commonly 5-10 nm). [...] Transform-based approaches reduce dimensionality by projecting the original data into a lower-dimensional subspace that preserves the most informative spectral or spatial-spectral structures. Among these, principal component analysis (PCA)  and minimum noise fraction (MNF)  are two of the most representative methods, often serving as baseline techniques for illustrating DR in HSI due to their easy-interpretable and friendly-practical linearized modeling. More specifically, let be the spectral vector at the -th pixel, where is the number of spectral bands and is the total number of pixels in the HSI image. PCA seeks an orthogonal transformation that maximizes the variance of the projected data:\n\n|  |  |  |\n --- \n|  |  | (5) |\n|  | [...] Table 1: Comparative summary of different HSI platforms.",
        "score": 0.9693242,
        "raw_content": null,
        "id": 50
    },
    {
        "url": "https://www.geeksforgeeks.org/deep-learning/vision-transformer-vit-architecture/",
        "title": "Vision Transformer (ViT) Architecture",
        "content": "Vision Transformer (ViT) is a deep learning architecture that applies the Transformer model to images. Instead of relying on convolutions,",
        "score": 0.78788257,
        "raw_content": null,
        "id": 51
    },
    {
        "url": "https://manimalakumar-29300.medium.com/generative-ai-model-architecture-part3-vision-transformer-363a0a81a7c2",
        "title": "Generative AI Model Architecture \u2014 Part3: Vision Transformer",
        "content": "# Generative AI Model Architecture \u2014 Part3: Vision Transformer. Vision Transformer (ViT) adapts the Transformer architecture \u2014 originally designed to process sequential data \u2014 to make sense of non-sequential data. There were specialized CNN model architectures for each of the fundamental vision tasks: image classification, object detection, segmentation, etc. ViTs develop a broad understanding of the image content, and overcome such limitations of CNNs. LVMs \u2014 the modern computer vision models \u2014 are increasingly built on ViT-based architectures. Since ViTs process image patches in parallel, position embeddings tell the model where each patch belongs. Self-distillation is a student-teacher architecture, where a student model learns to predict the output of a teacher model. This means the teacher is not a fixed, pre-trained model \u2014 over time, as the student improves, the teacher is updated to be a moving average of the student, ensuring the teacher remains a reliable target and promoting progressive learning. Part 4: Generative AI Model Architecture \u2014 Part4: Vision Language Model.",
        "score": 0.7602419,
        "raw_content": null,
        "id": 52
    },
    {
        "url": "https://blog.roboflow.com/vision-transformers/",
        "title": "Vision Transformers Explained: The Future of Computer ...",
        "content": "Transformers are a powerful deep learning architecture originally developed for natural language processing (NLP) tasks such as machine translation,",
        "score": 0.6659544,
        "raw_content": null,
        "id": 53
    },
    {
        "url": "https://www.codecademy.com/article/vision-transformers-working-architecture-explained",
        "title": "How do Vision Transformers Work? Architecture Explained",
        "content": "Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial. This architectural shift enables ViTs to use the powerful self-attention mechanism that made transformers successful in NLP, allowing them to model long-range dependencies and global relationships in images more effectively than CNNs. The ViT architecture consists of four main components: patch embedding, positional encoding, transformer encoder, and classification head. The original vision transformer architecture proposed in the paper \u201cAn Image Is Worth 16X16 Words\u201d contains different components such as patch embedding, positional encoding, transformer encoder, multilayer perceptron head, etc, and looks as follows:. * **Global feature modeling**: In vision transformers, every patch of an image interacts with every other patch during the self-attention mechanism. It\u2019s a self-supervised learning framework for vision transformers that allows a ViT model to learn robust and invariant features from images without needing human-labeled data. Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial.",
        "score": 0.61363083,
        "raw_content": null,
        "id": 54
    },
    {
        "url": "https://www.coursera.org/articles/transformers-vs-convolutional-neural-networks",
        "title": "Transformers vs. Convolutional Neural Networks",
        "content": "Transformers and convolutional neural networks are both powerful deep learning algorithms for computer vision, but they work differently and have different strengths and weaknesses. If you want to use a deep learning neural network to extract meaningful data from images, you may decide to use a convolutional neural network or a visual transformer. Explore transformers versus convolutional neural networks to learn how each of these models works, what you can use them for, and how they compare to each other. A transformer is a deep learning neural network that uses an encoder/decoder architecture to first break down the input into the base characteristics that define the data, then use its understanding of patterns to rebuild the data with original but similar information. Transformer models and convolutional neural networks are both deep learning models that you can use for computer vision, but they are different in how they empower computer vision.",
        "score": 0.5735118,
        "raw_content": null,
        "id": 55
    },
    {
        "url": "https://www.geeksforgeeks.org/deep-learning/vision-transformer-vit-architecture/",
        "title": "Vision Transformer (ViT) Architecture",
        "content": "Vision Transformer (ViT) is a deep learning architecture that applies the Transformer model to images. Instead of relying on convolutions,",
        "score": 0.78788257,
        "raw_content": null,
        "id": 56
    },
    {
        "url": "https://manimalakumar-29300.medium.com/generative-ai-model-architecture-part3-vision-transformer-363a0a81a7c2",
        "title": "Generative AI Model Architecture \u2014 Part3: Vision Transformer",
        "content": "# Generative AI Model Architecture \u2014 Part3: Vision Transformer. Vision Transformer (ViT) adapts the Transformer architecture \u2014 originally designed to process sequential data \u2014 to make sense of non-sequential data. There were specialized CNN model architectures for each of the fundamental vision tasks: image classification, object detection, segmentation, etc. ViTs develop a broad understanding of the image content, and overcome such limitations of CNNs. LVMs \u2014 the modern computer vision models \u2014 are increasingly built on ViT-based architectures. Since ViTs process image patches in parallel, position embeddings tell the model where each patch belongs. Self-distillation is a student-teacher architecture, where a student model learns to predict the output of a teacher model. This means the teacher is not a fixed, pre-trained model \u2014 over time, as the student improves, the teacher is updated to be a moving average of the student, ensuring the teacher remains a reliable target and promoting progressive learning. Part 4: Generative AI Model Architecture \u2014 Part4: Vision Language Model.",
        "score": 0.7602419,
        "raw_content": null,
        "id": 57
    },
    {
        "url": "https://blog.roboflow.com/vision-transformers/",
        "title": "Vision Transformers Explained: The Future of Computer ...",
        "content": "Transformers are a powerful deep learning architecture originally developed for natural language processing (NLP) tasks such as machine translation,",
        "score": 0.6659544,
        "raw_content": null,
        "id": 58
    },
    {
        "url": "https://www.codecademy.com/article/vision-transformers-working-architecture-explained",
        "title": "How do Vision Transformers Work? Architecture Explained",
        "content": "Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial. This architectural shift enables ViTs to use the powerful self-attention mechanism that made transformers successful in NLP, allowing them to model long-range dependencies and global relationships in images more effectively than CNNs. The ViT architecture consists of four main components: patch embedding, positional encoding, transformer encoder, and classification head. The original vision transformer architecture proposed in the paper \u201cAn Image Is Worth 16X16 Words\u201d contains different components such as patch embedding, positional encoding, transformer encoder, multilayer perceptron head, etc, and looks as follows:. * **Global feature modeling**: In vision transformers, every patch of an image interacts with every other patch during the self-attention mechanism. It\u2019s a self-supervised learning framework for vision transformers that allows a ViT model to learn robust and invariant features from images without needing human-labeled data. Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial.",
        "score": 0.61363083,
        "raw_content": null,
        "id": 59
    },
    {
        "url": "https://www.coursera.org/articles/transformers-vs-convolutional-neural-networks",
        "title": "Transformers vs. Convolutional Neural Networks",
        "content": "Transformers and convolutional neural networks are both powerful deep learning algorithms for computer vision, but they work differently and have different strengths and weaknesses. If you want to use a deep learning neural network to extract meaningful data from images, you may decide to use a convolutional neural network or a visual transformer. Explore transformers versus convolutional neural networks to learn how each of these models works, what you can use them for, and how they compare to each other. A transformer is a deep learning neural network that uses an encoder/decoder architecture to first break down the input into the base characteristics that define the data, then use its understanding of patterns to rebuild the data with original but similar information. Transformer models and convolutional neural networks are both deep learning models that you can use for computer vision, but they are different in how they empower computer vision.",
        "score": 0.5735118,
        "raw_content": null,
        "id": 60
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-34756-z",
        "title": "A multiscale transformer with spatial attention for ...",
        "content": "each tailored to extract valuable insights from the multi-dimensional HSI data. Among these approaches, the k-nearest neighbour18, 11514\u201311521. [...] designs and lightweight transformer modules to further reduce computational costs without compromising overall performance, making the framework even more suitable for HSI classification tasks. [...] To tackle these challenges, we propose MTSA-Net, a novel multiscale transformer framework with spatial attention for HSI classification. The model begins with a 3D convolution layer to extract shallow spectral\u2013spatial features, followed by a 2D convolution layer and a spatial attention module. This design reduces feature redundancy, mitigates inaccuracies that often arise in deeper networks, and emphasizes the most discriminative spatial features, thereby alleviating the limited spatial resolution of HSIs. The refined feature vectors are then processed by multiple parallel transformer encoder branches with varying hidden dimensions, enabling simultaneous modeling of fine-grained local details, intermediate relationships, and global representations. Finally, a multiscale feature fusion",
        "score": 0.98975134,
        "raw_content": null,
        "id": 61
    },
    {
        "url": "https://worldmodelbench.github.io/static/pdfs/7_Adaptive_Attention_Guided_Ma.pdf",
        "title": "Adaptive Attention-Guided Masking in Vision Transformers ...",
        "content": "Art. no.\n5511817. 2, 5, 7 Adaptive Attention-Guided Masking in Vision Transformers for Self-Supervised Hyperspectral Feature Learning Supplementary Material 6. Appendix In recent years, world models like Vision Transformers (ViTs)  have demonstrated impressive performance in various computer vision [6, 8, 13] tasks, including hyper-spectral image (HSI) classification . However, their ap-plication to HSI faces several limitations due to the unique challenges of hyperspectral data.\nChallenges. First, the high dimensionality of HSI, with several spectral bands, complicates processing for conven-tional ViTs, which are optimized for RGB images with only three channels. This necessitates extensive modifications to handle spectral complexity. [...] Hyperspectral image (HSI) classification has rapidly ad-vanced with diverse methods tackling spectral-spatial data challenges.\nEarly models like 2-DCNN  combined simple 2D convolutions with fully connected layers, while SPRN  improved spatial features using attention mech-anisms and residual blocks. 3-DCNN  introduced 3D convolutions to jointly capture spectral and spatial infor-mation.\nHybrid models like HybridSN  merged 2D and 3D CNNs for richer spectral-spatial representations.\nThe emergence of transformer-based approaches marked a pivotal shift, emphasizing efficient feature extraction and long-range dependency modeling. GAHT  and Mor-phFormer  leveraged CNN-transformer hybrids and self-attention for enhanced spectral-spatial learning. [...] 4. Analysis of Results In this section, we comprehensively analyze results in var-ious scenarios with three HSI datasets : Indian Pines, Salinas, and Botswana using the two performance metrics: Overall Accuracy (OA) and Cohen\u2019s Kappa coefficient (\u03ba).",
        "score": 0.98975134,
        "raw_content": null,
        "id": 62
    },
    {
        "url": "https://arxiv.org/html/2502.06894v1",
        "title": "AI-Driven HSI: Multimodality, Fusion, Challenges, and the ...",
        "content": "HSI, helping researchers select the right architecture for their problems. [...] intensity measurements are collected across spectral bands and represented as a 3D data cube. The observed intensity at wavelength for pixel can be expressed as: [...] TABLE IV: Deep Learning Models and their Applications in HSI and Multimodal HSI",
        "score": 0.98934746,
        "raw_content": null,
        "id": 63
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-12489-3",
        "title": "Spectral-spatial wave and frequency interactive ...",
        "content": "The proposed framework is designed for HSI. It utilizes a combination of spectral spatial feature extractor with frequency domain transformer encoder block to extract phase and frequency features. [...] Figure 1 depicts the proposed model, this section will elucidate the model\u2019s structure and it\u2019s functioning. The feature pre-processing stage begins by feeding HSI patches into 3D convolutional layer and 2D convolutional layer for low-level feature extraction after applying Feature Augmentation. Shallow feature maps generated by these layers are then processed using pixel operations to tokenize the features before passing them into the transformer encoder Blocks. The core of the model lies in the transformer Blocks, which are repeated N times to enable interactions between tokens representing different spectral spatial locations in the frequency domain. Finally, a classification head, consisting of a Global Average Pooling (GAP) layer is employed to assign a label to each pixel. [...] (2021).\") treated HSI patches as sequential data from the spectral dimension, utilizing group-spectral embedding and a transformer encoder module to capture locally detailed spectral representations. Roy et al.31, 7831\u20137843.",
        "score": 0.98282325,
        "raw_content": null,
        "id": 64
    },
    {
        "url": "https://arxiv.org/html/2508.08107v1",
        "title": "Hyperspectral Imaging",
        "content": "HSI focuses on the optical window of the electromagnetic spectrum (see Fig. 1A), typically covering wavelengths from 380 to 2500 nm. This window usually encompasses the visible light (400-700 nm), near-infrared (NIR), and shortwave infrared (SWIR) regions, as shown in Fig. 1B. Notably, the inclusion of the 380-400 nm region further enhances sensitivity to pigment absorption and subtle surface reflectance variations, particularly in the violet and near-ultraviolet range. An example scene illustrating typical hyperspectral data is shown in Fig. 1C, highlighting that HSI routinely captures over hundreds of spectral channels at high spectral resolution (commonly 5-10 nm). [...] Transform-based approaches reduce dimensionality by projecting the original data into a lower-dimensional subspace that preserves the most informative spectral or spatial-spectral structures. Among these, principal component analysis (PCA)  and minimum noise fraction (MNF)  are two of the most representative methods, often serving as baseline techniques for illustrating DR in HSI due to their easy-interpretable and friendly-practical linearized modeling. More specifically, let be the spectral vector at the -th pixel, where is the number of spectral bands and is the total number of pixels in the HSI image. PCA seeks an orthogonal transformation that maximizes the variance of the projected data:\n\n|  |  |  |\n --- \n|  |  | (5) |\n|  | [...] Table 1: Comparative summary of different HSI platforms.",
        "score": 0.9693242,
        "raw_content": null,
        "id": 65
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-33943-2",
        "title": "Multi-scale boundary-aware network for remote sensing ...",
        "content": "classification of high-resolution remote sensing images. By simultaneously leveraging the local features of CNN and the global dependencies of Transformer through multi-scale self-attention and bidirectional knowledge distillation, it improves the accuracy of remote sensing semantic segmentation. Fan et al.33:17.\") proposed a Multidimensional Information Fusion Network for high-resolution remote sensing image segmentation by integrating CNN and Transformer and introducing frequency information. By multi-scale integration of local details, global semantics and frequency domain features, the model\u2019s ability to identify fine-grained boundaries and scale-varying targets is enhanced, effectively alleviating complex interference factors such as inter-class ambiguity. The hybrid attention [...] Despite significant advances in remote sensing semantic segmentation achieved by existing CNNs, Transformers, and hybrid approaches, high-resolution imagery remains challenging: Insufficient multi-scale feature characterisation prevents the model from capturing boundary features, leading to blurred segmentation boundaries caused by classification errors at the junctures of different semantic categories. Furthermore, recent research on remote sensing image segmentation has underscored the importance of boundary-aware modelling in dense prediction tasks22.\"), 23.\"). These considerations have prompted us to develop a unified framework that synergistically enhances both multi-scale representation and boundary-aware capabilities. To address these challenges, we propose MSBANet\u2014a specially [...] Traditional CNNs have remarkable effects in extracting local spatial details, but their capabilities in modeling global contexts are limited. Although Transformer-based methods can capture long-range dependencies, they often ignore fine structures and boundary information and have high computational complexity. In semantic segmentation tasks, both global information and local information are crucial for accurately understanding the semantic structure of images. Researchers have begun to combine CNN with Transformer models to fully leverage their respective advantages. Chen et al. proposed CTSeg32.\"), a novel CNN and ViT collaborative segmentation framework following the encoder-decoder architecture for land-use/land-cover classification of high-resolution remote sensing images. By",
        "score": 0.38512084,
        "raw_content": null,
        "id": 66
    },
    {
        "url": "https://www.sciencedirect.com/science/article/abs/pii/S1350449525001173",
        "title": "Edge Feature Enhanced Transformer Network for RGB and ...",
        "content": "In real-world object detection scenarios, factors such as rain, fog, occlusion, varying illumination, and low resolution hinder the ability of visible light imaging to achieve high-precision target detection performance . In comparison to RGB single-modality based object detection, RGB-infrared multi-modality based object detection is able to leverage diverse and complementary information. As there are substantial visual disparities between the two modal images. For instance, RGB images are sensitive to color and texture, where objects exhibit clearer texture details compared to that in infrared images during the day while diminishing under nighttime conditions (see Fig. 1(a) and Fig. 1(b)). Instead, infrared (IR) images are sensitive for temperature variation, they enable to offer [...] images achieve high- precision detection and recognition by extracting local spatial\u2013temporal features. However, convolutional operations is not capable for global information perception and struggle to model dependencies between features. Therefore, there is still potential to improve the fusion quality by taking consideration of the global feature learning. [...] In multispectral object detection, the complementary nature of infrared and visible images remains underexploited due to perceptual differences and spatial misalignment between modalities, posing significant challenges to accurate detection. We propose MRT-DETR (Multispectral RT-DETR), an end-to-end multispectral real-time detection framework that addresses weak alignment and complex lighting conditions by combining brightness-aware weighting with deformable alignment. Built on the RT-DETR backbone, our method introduces an Early-stage Deformable Alignment (EDA) module that learns attention-guided offsets at shallow layers to explicitly align infrared features to visible features in the spatial domain. Additionally, a Dual-Branch Brightness Weighting (DBW) module derives patch-wise",
        "score": 0.3059777,
        "raw_content": null,
        "id": 67
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-34756-z",
        "title": "A multiscale transformer with spatial attention for ...",
        "content": "Article  ADS   Google Scholar\n7. Hong, D. et al. Spectralformer: Rethinking hyperspectral image classification with transformers. IEEE Transactions on Geoscience and Remote Sensing 60, 1\u201315 (2021).\n\n   Article   Google Scholar\n8. Zhou, P., Han, J., Cheng, G. & Zhang, B. Learning compact and discriminative stacked autoencoder for hyperspectral image classification. IEEE Transactions on Geoscience and Remote Sensing 57(7), 4823\u20134833 (2019).\n\n   Article  ADS   Google Scholar\n9. Zheng, X., Yuan, Y. & Lu, X. Dimensionality reduction by spatial-spectral preservation in selected bands. IEEE Transactions on Geoscience and Remote Sensing 55(9), 5185\u20135197 (2017). [...] The Houston 2013 (H-13) dataset was gathered using the ITRES CASI-1500 sensor and covers the University of Houston premises and the neighbouring rural regions. This dataset has been publicly employed for assessing the effectiveness of HSI classification methods. This dataset has an image size of \\(349 \\times 1905\\) pixels and includes 144 spectral bands. It encompasses 15 challenging distinct classes, making it a valuable resource for land cover classification studies. The dataset samples have been divided into training, validation, and test sets, with the allocation details provided in Table 4. [...] Furthermore, a hybrid approach incorporating both CNN and Transformer networks48.\") was proposed to effectively calculate spectral-spatial and semantic features for HSI classification. In66.\"), Swin Transformer was presented, which calculates representations through a shifted window strategy. This approach allows for efficient processing of visual data with diverse scales and higher resolutions. Adopting this strategy effectively addresses the challenges presented by scale and resolution disparities, rendering it well-suited for HSI classification.\n\n## Proposed methodology",
        "score": 0.2640859,
        "raw_content": null,
        "id": 68
    },
    {
        "url": "https://arxiv.org/html/2507.22791v1",
        "title": "Modality-Aware Feature Matching: A Comprehensive ...",
        "content": "In this survey, our discussion addresses unique challenges and methodologies related to single-modal feature matching (e.g., RGB, depth, medical imaging) and cross-modal scenarios (e.g., medical image registration, vision-language integration). We delineate the progression from classical detector-based pipelines towards contemporary detector-free solutions. Figure 1 illustrates the overall pipeline of the survey, clearly mapping the evolution of feature matching methodologies across various data modalities. Furthermore, Figure 2 offers representative examples of the results of the matching of modality-awarecharacteristics.\n\n## 2. RGB Images [...] In summary, deep learning-based methods have greatly advanced the state of RGB image matching. They have produced more repeatable detectors (learning where to detect points that survive viewpoint/illumination changes), more discriminative descriptors (learning how to describe patches for higher distinctiveness), and even entirely new paradigms for matching (learning to match with attention mechanisms, and end-to-end trainable pipelines). Table 1 highlights some of the influential methods from both the handcrafted and deep learning eras. These modern techniques, when evaluated on challenging benchmarks, significantly outperform classic methods in terms of matching precision and robustness, although often at a higher computational cost. Nonetheless, due to their task-specific optimizations [...] ## 2. RGB Images\n\nFeature matching in RGB images traditionally follows a pipeline of feature detection, feature description, and feature matching. Over the decades, a lot of methods have been proposed for each stage of this pipeline, from handcrafted approaches to more recent deep learning-based techniques.\n\n### 2.1. Handcrafted Feature Detectors and Descriptors",
        "score": 0.26285648,
        "raw_content": null,
        "id": 69
    },
    {
        "url": "https://link.springer.com/article/10.1007/s10462-025-11169-y",
        "title": "A review of hyperspectral image classification based on graph ...",
        "content": "\")) of hyperspectral imaging (HSI) pose significant challenges to achieving accurate HSIC (Sawant and Prabukumar 2020 A survey of band selection techniques for hyperspectral image classification. J Spectr Imaging 9(1):5. \n                  \n                  \n                \")). Specifically, the large number of spectral bands and data points increases computational complexity (Ullah et al. 2024 Conventional to deep ensemble methods for hyperspectral image classification: a comprehensive survey. IEEE J Sel Top Appl Earth Obs Remote Sens. [...] ### 2.2 Technologies for hyperspectral image classification\n\nOptical remote sensing images can be categorized into three main types based on the number of spectral bands: panchromatic images with a single band, multiband RGB images, and hyperspectral images (HSI) containing hundreds of continuous, narrow spectral bands (Mather and Tso 2016 Classification methods for remotely sensed data. CRC Press, Boca Raton\")). HSI, with its unique technical advantages, achieves a deep integration of imaging and spectral technologies, enabling comprehensive data acquisition in both two-dimensional spatial geometry and one-dimensional spectral information. [...] Although deep learning has achieved remarkable breakthroughs in hyperspectral image classification (HSIC), the unique characteristics of hyperspectral image (HSI) data still present numerous challenges (Wan et al. 2020a Hyperspectral image classification with context-aware dynamic graph convolutional network. IEEE Trans Geosci Remote Sens 59(1):597\u2013612.",
        "score": 0.21413255,
        "raw_content": null,
        "id": 70
    },
    {
        "url": "https://eureka.patsnap.com/article/self-attention-in-vision-transformers-why-it-captures-long-range-dependencies",
        "title": "Self-Attention in Vision Transformers: Why It Captures Long-Range Dependencies",
        "content": "Conclusion  \n  \nVision Transformers, with their self-attention mechanisms, have demonstrated a remarkable ability to capture long-range dependencies in images, setting a new standard in computer vision. By considering every part of an image in relation to every other part, ViTs provide a comprehensive understanding that surpasses traditional methods. As research continues to advance, we can expect Vision Transformers to play an increasingly prominent role in the future of image processing, unlocking new possibilities for innovation and application. [...] Capturing Long-Range Dependencies  \n  \nOne of the standout features of self-attention in ViTs is its ability to capture long-range dependencies. Traditional CNNs are constrained by the size of their receptive fields, which limits their ability to consider distant relationships in an image. In contrast, self-attention allows each image patch to attend to all other patches, regardless of their spatial distance. [...] This is achieved by dividing an image into a sequence of patches, treating these patches similarly to words in a sentence. Each patch is linearly embedded into a fixed-size vector, and these vectors are then passed through layers of self-attention. This process allows the model to weigh the importance of different parts of the image when making predictions, effectively capturing both local and global context.  \n  \nCapturing Long-Range Dependencies",
        "score": 0.99985456,
        "raw_content": null,
        "id": 71
    },
    {
        "url": "https://eureka.patsnap.com/article/how-do-transformers-handle-long-range-dependencies",
        "title": "How Do Transformers Handle Long-Range Dependencies?",
        "content": "Self-Attention Mechanism  \n  \nAt the heart of the transformer architecture is the self-attention mechanism. This mechanism allows transformers to weigh the importance of each word in a sequence relative to others, regardless of their distance. Self-attention computes a representation of each word by considering the entire sequence, enabling the model to capture dependencies over long ranges efficiently.  \n  \nThe self-attention mechanism works by creating three vectors for each word: query, key, and value. These vectors are used to calculate attention scores, which determine how much focus should be given to each word in the sequence. This process allows transformers to create context-aware representations of words, making them adept at understanding intricate dependencies. [...] Layer Stacking and Multi-Head Attention  \n  \nTransformers consist of multiple layers, each containing a self-attention mechanism followed by feed-forward neural networks. These layers are stacked on top of each other, enabling the model to learn complex patterns and dependencies. Moreover, transformers employ a technique called multi-head attention, where multiple self-attention mechanisms run in parallel at each layer. This allows the model to focus on different parts of the sequence simultaneously, enhancing its ability to capture a variety of dependencies.  \n  \nHandling Long Sequences [...] Conclusion  \n  \nTransformers have revolutionized the way we handle long-range dependencies in NLP tasks. Their innovative architecture, characterized by self-attention mechanisms, positional encoding, and multi-head attention, allows them to effectively capture complex relationships in sequences. As a result, transformers have set new benchmarks in NLP, proving to be a powerful tool for processing language data over long ranges. With ongoing research and development, the capabilities of transformers continue to expand, promising even more sophisticated handling of long-range dependencies in the future.",
        "score": 0.9994642,
        "raw_content": null,
        "id": 72
    },
    {
        "url": "https://www.mdpi.com/2072-4292/17/21/3532",
        "title": "Advances on Multimodal Remote Sensing Foundation ...",
        "content": "The Vision Transformer (ViT) is a DL model that uses the Transformer network architecture to handle visual image tasks. Its core technologies include the multi-head self-attention mechanism (MHSA) and positional encoding [83,84,85]. The basic principle of the Vision Transformer is to divide the image into several small blocks containing various ground feature pixels, regard each small block as an image sequence unit, and then extract the spatial and temporal features of the image through the encoder\u2013decoder of the Transformer layer. The self-attention mechanism can perform weighted summation for each position in the input image sequence and extract the global dependence of the image sequence by modeling interactions among any pixels in the input image sequence . The Vision Transformer",
        "score": 0.9992084,
        "raw_content": null,
        "id": 73
    },
    {
        "url": "https://towardsai.net/p/machine-learning/attention-is-all-you-need-a-deep-dive-into-the-revolutionary-transformer-architecture",
        "title": "Attention Is All You Need - A Deep Dive into the ...",
        "content": "In summary, attention mechanisms in the Transformer allow the model to dynamically focus on different parts of the input sequence, capturing both local and long-range dependencies. The scaled dot-product attention mechanism is particularly efficient and effective, making it a cornerstone of the Transformer architecture. In the next section, we will explore how self-attention uses this mechanism to capture relationships between words within a sequence.\n\n5. Self-Attention in Detail\n\nSelf-attention is a specific type of attention where the queries, keys, and values all come from the same source. In the context of the Transformer, this means that each position in a sequence can attend to all positions in the same sequence, allowing the model to capture intricate relationships between words. [...] Image 20\nWhy Self-Attention Works\n\nSelf-attention has several key advantages over traditional sequence processing methods:\n\n1.   Captures Long-Range Dependencies: Unlike RNNs, which struggle with long-range dependencies, self-attention connects any two positions directly, regardless of their distance in the sequence.\n\n2. Parallel Computation: All positions can be processed in parallel, unlike the sequential nature of RNNs.\n\n3. Interpretable Attention Weights: The attention weights provide insights into which words the model is focusing on, adding a level of interpretability.\n\n4. Constant Path Length: The number of operations required to connect positions in the network is constant, not dependent on sequence length.\n\nImage 21 [...] Transformer-XL (2019): Extended the Transformer with a segment-level recurrence mechanism and relative positional encoding, enabling it to learn dependencies beyond a fixed-length context.\n   Reformer (2020): Used locality-sensitive hashing to reduce the computational complexity of self-attention from O(n\u00b2) to O(n log n), making it more efficient for long sequences.\n   Linformer (2020): Reduced the complexity of self-attention to O(n) by projecting the length dimension of keys and values.\n   Performer (2020): Approximated the attention matrix using kernelization techniques, reducing memory and computational requirements.\n   Vision Transformer (ViT, 2020): Adapted the Transformer for computer vision by treating image patches as sequence elements.",
        "score": 0.9989183,
        "raw_content": null,
        "id": 74
    },
    {
        "url": "https://arxiv.org/html/2410.16602v3",
        "title": "Foundation Models for Remote Sensing and Earth ...",
        "content": "Transformers. Transformers [25, 26], as depicted in Fig. 6, are designed to process sequence data using self-attention mechanisms, enabling them to capture relationships between data points regardless of their positional distance. Unlike CNNs, which emphasize local features, transformers excel at modeling global dependencies, making them particularly effective for long-range interactions. A significant advantage of transformers in FMs is their ability to integrate across multiple modalities, enabling them to process a diverse range of inputs, such as visual data (e.g., images, depth, thermal) and non-visual data (e.g., text, 3D point clouds, audio). This cross-modality integration paves the way for unified RSFMs capable of handling a wide spectrum of geospatial data modalities.",
        "score": 0.998259,
        "raw_content": null,
        "id": 75
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-34756-z",
        "title": "A multiscale transformer with spatial attention for ...",
        "content": "Similarly, in41, 67 (2017).\"), an extended investigation was conducted on 3D CNNs for spectral\u2013spatial classification, utilizing input cubes with reduced spatial dimensions from HSIs. These models were designed to generate thematic maps by directly processing raw HSIs. Despite the achievements of CNN-based methods in extracting spatial and spectral information, they possess specific constraints. In40, 6232\u20136251 (2016).\"), a 3D CNN was employed to directly and efficiently learn spectral-spatial features from the original HSI, showcasing promising outcomes in terms of classification performance. Furthermore, a hybrid approach incorporating both CNN and Transformer networks48.\") was proposed to effectively calculate spectral-spatial and semantic features for HSI classification. In this work, a novel framework, MTSA-Net, is introduced for HSI classification, harnessing the combined strengths of spatial attention and multiscale transformers to effectively utilize the spatial-spectral information in hyperspectral data. Spectral-spatial feature extraction for hyperspectral image classification: A dimension reduction and deep learning approach. Learning deep hierarchical spatial-spectral features for hyperspectral image classification based on residual 3d\u20132d cnn.",
        "score": 0.9971328,
        "raw_content": null,
        "id": 76
    },
    {
        "url": "https://link.springer.com/article/10.1007/s10462-025-11169-y",
        "title": "A review of hyperspectral image classification based on graph ...",
        "content": "*et al.* A review of hyperspectral image classification based on graph neural networks. Hybrid models based on graph neural networks for hyperspectral image (HSI) classification combine GNNs with other deep learning models. \")) combined the multi-view deep autoencoder (MVDAE) and the semi-supervised graph convolutional network (SSGCN) to propose a novel approach for spectral-spatial classification of HSI, called MV-DNNet. MV-DNNet leverages a small number of labeled samples to integrate spectral and spatial features, significantly enhancing hyperspectral image classification performance. * Bai J, Ding B, Xiao Z, Jiao L, Chen H, Regan AC (2021) Hyperspectral image classification based on deep attention graph convolutional network. * Dong Y, Liu Q, Du B, Zhang L (2022) Weighted feature fusion of convolutional neural network and graph attention network for hyperspectral image classification. * Yang P, Tong L, Qian B, Gao Z, Yu J, Xiao C (2020) Hyperspectral image classification with spectral and spatial graph using inductive representation learning network.",
        "score": 0.99101454,
        "raw_content": null,
        "id": 77
    },
    {
        "url": "https://ieeexplore.ieee.org/iel8/4609443/4609444/10877784.pdf",
        "title": "Frequency-Aware Hierarchical Mamba for Hyperspectral ...",
        "content": "by P Zhuang \u00b7 2025 \u00b7 Cited by 19 \u2014 For HSI classification, MambaHSI [27] utilized Mamba blocks to si- multaneously model long-range spatial interactions and extract spectral features, achieving",
        "score": 0.9522199,
        "raw_content": null,
        "id": 78
    },
    {
        "url": "https://www.sciencedirect.com/science/article/abs/pii/S0925231225011002",
        "title": "A comprehensive survey for Hyperspectral Image ...",
        "content": "by M Ahmad \u00b7 2025 \u00b7 Cited by 49 \u2014 Unlike traditional methods, Hyperspectral Images (HSIs) provide a continuous spectrum through numerous narrow bands, enabling precise material characterization",
        "score": 0.9092645,
        "raw_content": null,
        "id": 79
    },
    {
        "url": "https://www.mdpi.com/2072-4292/17/14/2489",
        "title": "Spatial and Spectral Structure-Aware Mamba Network for ...",
        "content": "Current Mamba-based HSI classification methods [24,25,26,27] typically flatten the 2D spatial structure into the 1D sequence and then use a fixed scanning strategy to extract spatial features from the 1D sequence, which inevitably changes the spatial relationship between pixels, destroys the inherent contextual information in the image, and affects the classification results. Classification maps produced by various methods applied to the Pavia University dataset: (**a**) ground truth; (**b**) 2DCNN; (**c**) 3DCNN; (**d**) HybridSN; (**e**) SPRN; (**f**) SpectralFormer; (**g**) SSFTT; (**h**) MorphFormer; (**i**) GSC-VIT; (**j**) 3DSS-Mamba; (**k**) SS-Mamba; (**l**) SSUM; (**m**) DADFMamba. Classification maps produced by various methods applied to the Pavia University dataset: (**a**) ground truth; (**b**) 2DCNN; (**c**) 3DCNN; (**d**) HybridSN; (**e**) SPRN; (**f**) SpectralFormer; (**g**) SSFTT; (**h**) MorphFormer; (**i**) GSC-VIT; (**j**) 3DSS-Mamba; (**k**) SS-Mamba; (**l**) SSUM; (**m**) DADFMamba. Classification maps produced by various methods applied to the Pavia University dataset: (**a**) ground truth; (**b**) 2DCNN; (**c**) 3DCNN; (**d**) HybridSN; (**e**) SPRN; (**f**) SpectralFormer; (**g**) SSFTT; (**h**) MorphFormer; (**i**) GSC-VIT; (**j**) 3DSS-Mamba; (**k**) SS-Mamba; (**l**) SSUM; (**m**) DADFMamba.",
        "score": 0.8864204,
        "raw_content": null,
        "id": 80
    },
    {
        "url": "https://www.geeksforgeeks.org/deep-learning/vision-transformer-vit-architecture/",
        "title": "Vision Transformer (ViT) Architecture",
        "content": "Vision Transformer (ViT) is a deep learning architecture that applies the Transformer model to images. Instead of relying on convolutions,",
        "score": 0.78788257,
        "raw_content": null,
        "id": 81
    },
    {
        "url": "https://manimalakumar-29300.medium.com/generative-ai-model-architecture-part3-vision-transformer-363a0a81a7c2",
        "title": "Generative AI Model Architecture \u2014 Part3: Vision Transformer",
        "content": "# Generative AI Model Architecture \u2014 Part3: Vision Transformer. Vision Transformer (ViT) adapts the Transformer architecture \u2014 originally designed to process sequential data \u2014 to make sense of non-sequential data. There were specialized CNN model architectures for each of the fundamental vision tasks: image classification, object detection, segmentation, etc. ViTs develop a broad understanding of the image content, and overcome such limitations of CNNs. LVMs \u2014 the modern computer vision models \u2014 are increasingly built on ViT-based architectures. Since ViTs process image patches in parallel, position embeddings tell the model where each patch belongs. Self-distillation is a student-teacher architecture, where a student model learns to predict the output of a teacher model. This means the teacher is not a fixed, pre-trained model \u2014 over time, as the student improves, the teacher is updated to be a moving average of the student, ensuring the teacher remains a reliable target and promoting progressive learning. Part 4: Generative AI Model Architecture \u2014 Part4: Vision Language Model.",
        "score": 0.7602419,
        "raw_content": null,
        "id": 82
    },
    {
        "url": "https://blog.roboflow.com/vision-transformers/",
        "title": "Vision Transformers Explained: The Future of Computer ...",
        "content": "Transformers are a powerful deep learning architecture originally developed for natural language processing (NLP) tasks such as machine translation,",
        "score": 0.6659544,
        "raw_content": null,
        "id": 83
    },
    {
        "url": "https://www.codecademy.com/article/vision-transformers-working-architecture-explained",
        "title": "How do Vision Transformers Work? Architecture Explained",
        "content": "Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial. This architectural shift enables ViTs to use the powerful self-attention mechanism that made transformers successful in NLP, allowing them to model long-range dependencies and global relationships in images more effectively than CNNs. The ViT architecture consists of four main components: patch embedding, positional encoding, transformer encoder, and classification head. The original vision transformer architecture proposed in the paper \u201cAn Image Is Worth 16X16 Words\u201d contains different components such as patch embedding, positional encoding, transformer encoder, multilayer perceptron head, etc, and looks as follows:. * **Global feature modeling**: In vision transformers, every patch of an image interacts with every other patch during the self-attention mechanism. It\u2019s a self-supervised learning framework for vision transformers that allows a ViT model to learn robust and invariant features from images without needing human-labeled data. Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial.",
        "score": 0.61363083,
        "raw_content": null,
        "id": 84
    },
    {
        "url": "https://www.coursera.org/articles/transformers-vs-convolutional-neural-networks",
        "title": "Transformers vs. Convolutional Neural Networks",
        "content": "Transformers and convolutional neural networks are both powerful deep learning algorithms for computer vision, but they work differently and have different strengths and weaknesses. If you want to use a deep learning neural network to extract meaningful data from images, you may decide to use a convolutional neural network or a visual transformer. Explore transformers versus convolutional neural networks to learn how each of these models works, what you can use them for, and how they compare to each other. A transformer is a deep learning neural network that uses an encoder/decoder architecture to first break down the input into the base characteristics that define the data, then use its understanding of patterns to rebuild the data with original but similar information. Transformer models and convolutional neural networks are both deep learning models that you can use for computer vision, but they are different in how they empower computer vision.",
        "score": 0.5735118,
        "raw_content": null,
        "id": 85
    },
    {
        "url": "https://www.geeksforgeeks.org/deep-learning/vision-transformer-vit-architecture/",
        "title": "Vision Transformer (ViT) Architecture",
        "content": "Vision Transformer (ViT) is a deep learning architecture that applies the Transformer model to images. Instead of relying on convolutions,",
        "score": 0.78788257,
        "raw_content": null,
        "id": 86
    },
    {
        "url": "https://manimalakumar-29300.medium.com/generative-ai-model-architecture-part3-vision-transformer-363a0a81a7c2",
        "title": "Generative AI Model Architecture \u2014 Part3: Vision Transformer",
        "content": "# Generative AI Model Architecture \u2014 Part3: Vision Transformer. Vision Transformer (ViT) adapts the Transformer architecture \u2014 originally designed to process sequential data \u2014 to make sense of non-sequential data. There were specialized CNN model architectures for each of the fundamental vision tasks: image classification, object detection, segmentation, etc. ViTs develop a broad understanding of the image content, and overcome such limitations of CNNs. LVMs \u2014 the modern computer vision models \u2014 are increasingly built on ViT-based architectures. Since ViTs process image patches in parallel, position embeddings tell the model where each patch belongs. Self-distillation is a student-teacher architecture, where a student model learns to predict the output of a teacher model. This means the teacher is not a fixed, pre-trained model \u2014 over time, as the student improves, the teacher is updated to be a moving average of the student, ensuring the teacher remains a reliable target and promoting progressive learning. Part 4: Generative AI Model Architecture \u2014 Part4: Vision Language Model.",
        "score": 0.7602419,
        "raw_content": null,
        "id": 87
    },
    {
        "url": "https://blog.roboflow.com/vision-transformers/",
        "title": "Vision Transformers Explained: The Future of Computer ...",
        "content": "Transformers are a powerful deep learning architecture originally developed for natural language processing (NLP) tasks such as machine translation,",
        "score": 0.6659544,
        "raw_content": null,
        "id": 88
    },
    {
        "url": "https://www.codecademy.com/article/vision-transformers-working-architecture-explained",
        "title": "How do Vision Transformers Work? Architecture Explained",
        "content": "Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial. This architectural shift enables ViTs to use the powerful self-attention mechanism that made transformers successful in NLP, allowing them to model long-range dependencies and global relationships in images more effectively than CNNs. The ViT architecture consists of four main components: patch embedding, positional encoding, transformer encoder, and classification head. The original vision transformer architecture proposed in the paper \u201cAn Image Is Worth 16X16 Words\u201d contains different components such as patch embedding, positional encoding, transformer encoder, multilayer perceptron head, etc, and looks as follows:. * **Global feature modeling**: In vision transformers, every patch of an image interacts with every other patch during the self-attention mechanism. It\u2019s a self-supervised learning framework for vision transformers that allows a ViT model to learn robust and invariant features from images without needing human-labeled data. Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial.",
        "score": 0.61363083,
        "raw_content": null,
        "id": 89
    },
    {
        "url": "https://www.coursera.org/articles/transformers-vs-convolutional-neural-networks",
        "title": "Transformers vs. Convolutional Neural Networks",
        "content": "Transformers and convolutional neural networks are both powerful deep learning algorithms for computer vision, but they work differently and have different strengths and weaknesses. If you want to use a deep learning neural network to extract meaningful data from images, you may decide to use a convolutional neural network or a visual transformer. Explore transformers versus convolutional neural networks to learn how each of these models works, what you can use them for, and how they compare to each other. A transformer is a deep learning neural network that uses an encoder/decoder architecture to first break down the input into the base characteristics that define the data, then use its understanding of patterns to rebuild the data with original but similar information. Transformer models and convolutional neural networks are both deep learning models that you can use for computer vision, but they are different in how they empower computer vision.",
        "score": 0.5735118,
        "raw_content": null,
        "id": 90
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-34756-z",
        "title": "A multiscale transformer with spatial attention for ...",
        "content": "each tailored to extract valuable insights from the multi-dimensional HSI data. Among these approaches, the k-nearest neighbour18, 11514\u201311521. [...] designs and lightweight transformer modules to further reduce computational costs without compromising overall performance, making the framework even more suitable for HSI classification tasks. [...] To tackle these challenges, we propose MTSA-Net, a novel multiscale transformer framework with spatial attention for HSI classification. The model begins with a 3D convolution layer to extract shallow spectral\u2013spatial features, followed by a 2D convolution layer and a spatial attention module. This design reduces feature redundancy, mitigates inaccuracies that often arise in deeper networks, and emphasizes the most discriminative spatial features, thereby alleviating the limited spatial resolution of HSIs. The refined feature vectors are then processed by multiple parallel transformer encoder branches with varying hidden dimensions, enabling simultaneous modeling of fine-grained local details, intermediate relationships, and global representations. Finally, a multiscale feature fusion",
        "score": 0.98975134,
        "raw_content": null,
        "id": 91
    },
    {
        "url": "https://worldmodelbench.github.io/static/pdfs/7_Adaptive_Attention_Guided_Ma.pdf",
        "title": "Adaptive Attention-Guided Masking in Vision Transformers ...",
        "content": "Art. no.\n5511817. 2, 5, 7 Adaptive Attention-Guided Masking in Vision Transformers for Self-Supervised Hyperspectral Feature Learning Supplementary Material 6. Appendix In recent years, world models like Vision Transformers (ViTs)  have demonstrated impressive performance in various computer vision [6, 8, 13] tasks, including hyper-spectral image (HSI) classification . However, their ap-plication to HSI faces several limitations due to the unique challenges of hyperspectral data.\nChallenges. First, the high dimensionality of HSI, with several spectral bands, complicates processing for conven-tional ViTs, which are optimized for RGB images with only three channels. This necessitates extensive modifications to handle spectral complexity. [...] Hyperspectral image (HSI) classification has rapidly ad-vanced with diverse methods tackling spectral-spatial data challenges.\nEarly models like 2-DCNN  combined simple 2D convolutions with fully connected layers, while SPRN  improved spatial features using attention mech-anisms and residual blocks. 3-DCNN  introduced 3D convolutions to jointly capture spectral and spatial infor-mation.\nHybrid models like HybridSN  merged 2D and 3D CNNs for richer spectral-spatial representations.\nThe emergence of transformer-based approaches marked a pivotal shift, emphasizing efficient feature extraction and long-range dependency modeling. GAHT  and Mor-phFormer  leveraged CNN-transformer hybrids and self-attention for enhanced spectral-spatial learning. [...] 4. Analysis of Results In this section, we comprehensively analyze results in var-ious scenarios with three HSI datasets : Indian Pines, Salinas, and Botswana using the two performance metrics: Overall Accuracy (OA) and Cohen\u2019s Kappa coefficient (\u03ba).",
        "score": 0.98975134,
        "raw_content": null,
        "id": 92
    },
    {
        "url": "https://arxiv.org/html/2502.06894v1",
        "title": "AI-Driven HSI: Multimodality, Fusion, Challenges, and the ...",
        "content": "HSI, helping researchers select the right architecture for their problems. [...] intensity measurements are collected across spectral bands and represented as a 3D data cube. The observed intensity at wavelength for pixel can be expressed as: [...] TABLE IV: Deep Learning Models and their Applications in HSI and Multimodal HSI",
        "score": 0.98934746,
        "raw_content": null,
        "id": 93
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-12489-3",
        "title": "Spectral-spatial wave and frequency interactive ...",
        "content": "The proposed framework is designed for HSI. It utilizes a combination of spectral spatial feature extractor with frequency domain transformer encoder block to extract phase and frequency features. [...] Figure 1 depicts the proposed model, this section will elucidate the model\u2019s structure and it\u2019s functioning. The feature pre-processing stage begins by feeding HSI patches into 3D convolutional layer and 2D convolutional layer for low-level feature extraction after applying Feature Augmentation. Shallow feature maps generated by these layers are then processed using pixel operations to tokenize the features before passing them into the transformer encoder Blocks. The core of the model lies in the transformer Blocks, which are repeated N times to enable interactions between tokens representing different spectral spatial locations in the frequency domain. Finally, a classification head, consisting of a Global Average Pooling (GAP) layer is employed to assign a label to each pixel. [...] (2021).\") treated HSI patches as sequential data from the spectral dimension, utilizing group-spectral embedding and a transformer encoder module to capture locally detailed spectral representations. Roy et al.31, 7831\u20137843.",
        "score": 0.98282325,
        "raw_content": null,
        "id": 94
    },
    {
        "url": "https://arxiv.org/html/2508.08107v1",
        "title": "Hyperspectral Imaging",
        "content": "HSI focuses on the optical window of the electromagnetic spectrum (see Fig. 1A), typically covering wavelengths from 380 to 2500 nm. This window usually encompasses the visible light (400-700 nm), near-infrared (NIR), and shortwave infrared (SWIR) regions, as shown in Fig. 1B. Notably, the inclusion of the 380-400 nm region further enhances sensitivity to pigment absorption and subtle surface reflectance variations, particularly in the violet and near-ultraviolet range. An example scene illustrating typical hyperspectral data is shown in Fig. 1C, highlighting that HSI routinely captures over hundreds of spectral channels at high spectral resolution (commonly 5-10 nm). [...] Transform-based approaches reduce dimensionality by projecting the original data into a lower-dimensional subspace that preserves the most informative spectral or spatial-spectral structures. Among these, principal component analysis (PCA)  and minimum noise fraction (MNF)  are two of the most representative methods, often serving as baseline techniques for illustrating DR in HSI due to their easy-interpretable and friendly-practical linearized modeling. More specifically, let be the spectral vector at the -th pixel, where is the number of spectral bands and is the total number of pixels in the HSI image. PCA seeks an orthogonal transformation that maximizes the variance of the projected data:\n\n|  |  |  |\n --- \n|  |  | (5) |\n|  | [...] Table 1: Comparative summary of different HSI platforms.",
        "score": 0.9693242,
        "raw_content": null,
        "id": 95
    },
    {
        "url": "https://www.geeksforgeeks.org/deep-learning/vision-transformer-vit-architecture/",
        "title": "Vision Transformer (ViT) Architecture",
        "content": "Vision Transformer (ViT) is a deep learning architecture that applies the Transformer model to images. Instead of relying on convolutions,",
        "score": 0.78788257,
        "raw_content": null,
        "id": 96
    },
    {
        "url": "https://manimalakumar-29300.medium.com/generative-ai-model-architecture-part3-vision-transformer-363a0a81a7c2",
        "title": "Generative AI Model Architecture \u2014 Part3: Vision Transformer",
        "content": "# Generative AI Model Architecture \u2014 Part3: Vision Transformer. Vision Transformer (ViT) adapts the Transformer architecture \u2014 originally designed to process sequential data \u2014 to make sense of non-sequential data. There were specialized CNN model architectures for each of the fundamental vision tasks: image classification, object detection, segmentation, etc. ViTs develop a broad understanding of the image content, and overcome such limitations of CNNs. LVMs \u2014 the modern computer vision models \u2014 are increasingly built on ViT-based architectures. Since ViTs process image patches in parallel, position embeddings tell the model where each patch belongs. Self-distillation is a student-teacher architecture, where a student model learns to predict the output of a teacher model. This means the teacher is not a fixed, pre-trained model \u2014 over time, as the student improves, the teacher is updated to be a moving average of the student, ensuring the teacher remains a reliable target and promoting progressive learning. Part 4: Generative AI Model Architecture \u2014 Part4: Vision Language Model.",
        "score": 0.7602419,
        "raw_content": null,
        "id": 97
    },
    {
        "url": "https://blog.roboflow.com/vision-transformers/",
        "title": "Vision Transformers Explained: The Future of Computer ...",
        "content": "Transformers are a powerful deep learning architecture originally developed for natural language processing (NLP) tasks such as machine translation,",
        "score": 0.6659544,
        "raw_content": null,
        "id": 98
    },
    {
        "url": "https://www.codecademy.com/article/vision-transformers-working-architecture-explained",
        "title": "How do Vision Transformers Work? Architecture Explained",
        "content": "Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial. This architectural shift enables ViTs to use the powerful self-attention mechanism that made transformers successful in NLP, allowing them to model long-range dependencies and global relationships in images more effectively than CNNs. The ViT architecture consists of four main components: patch embedding, positional encoding, transformer encoder, and classification head. The original vision transformer architecture proposed in the paper \u201cAn Image Is Worth 16X16 Words\u201d contains different components such as patch embedding, positional encoding, transformer encoder, multilayer perceptron head, etc, and looks as follows:. * **Global feature modeling**: In vision transformers, every patch of an image interacts with every other patch during the self-attention mechanism. It\u2019s a self-supervised learning framework for vision transformers that allows a ViT model to learn robust and invariant features from images without needing human-labeled data. Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial.",
        "score": 0.61363083,
        "raw_content": null,
        "id": 99
    },
    {
        "url": "https://www.coursera.org/articles/transformers-vs-convolutional-neural-networks",
        "title": "Transformers vs. Convolutional Neural Networks",
        "content": "Transformers and convolutional neural networks are both powerful deep learning algorithms for computer vision, but they work differently and have different strengths and weaknesses. If you want to use a deep learning neural network to extract meaningful data from images, you may decide to use a convolutional neural network or a visual transformer. Explore transformers versus convolutional neural networks to learn how each of these models works, what you can use them for, and how they compare to each other. A transformer is a deep learning neural network that uses an encoder/decoder architecture to first break down the input into the base characteristics that define the data, then use its understanding of patterns to rebuild the data with original but similar information. Transformer models and convolutional neural networks are both deep learning models that you can use for computer vision, but they are different in how they empower computer vision.",
        "score": 0.5735118,
        "raw_content": null,
        "id": 100
    },
    {
        "url": "https://www.geeksforgeeks.org/deep-learning/vision-transformer-vit-architecture/",
        "title": "Vision Transformer (ViT) Architecture",
        "content": "Vision Transformer (ViT) is a deep learning architecture that applies the Transformer model to images. Instead of relying on convolutions,",
        "score": 0.78788257,
        "raw_content": null,
        "id": 101
    },
    {
        "url": "https://manimalakumar-29300.medium.com/generative-ai-model-architecture-part3-vision-transformer-363a0a81a7c2",
        "title": "Generative AI Model Architecture \u2014 Part3: Vision Transformer",
        "content": "# Generative AI Model Architecture \u2014 Part3: Vision Transformer. Vision Transformer (ViT) adapts the Transformer architecture \u2014 originally designed to process sequential data \u2014 to make sense of non-sequential data. There were specialized CNN model architectures for each of the fundamental vision tasks: image classification, object detection, segmentation, etc. ViTs develop a broad understanding of the image content, and overcome such limitations of CNNs. LVMs \u2014 the modern computer vision models \u2014 are increasingly built on ViT-based architectures. Since ViTs process image patches in parallel, position embeddings tell the model where each patch belongs. Self-distillation is a student-teacher architecture, where a student model learns to predict the output of a teacher model. This means the teacher is not a fixed, pre-trained model \u2014 over time, as the student improves, the teacher is updated to be a moving average of the student, ensuring the teacher remains a reliable target and promoting progressive learning. Part 4: Generative AI Model Architecture \u2014 Part4: Vision Language Model.",
        "score": 0.7602419,
        "raw_content": null,
        "id": 102
    },
    {
        "url": "https://blog.roboflow.com/vision-transformers/",
        "title": "Vision Transformers Explained: The Future of Computer ...",
        "content": "Transformers are a powerful deep learning architecture originally developed for natural language processing (NLP) tasks such as machine translation,",
        "score": 0.6659544,
        "raw_content": null,
        "id": 103
    },
    {
        "url": "https://www.codecademy.com/article/vision-transformers-working-architecture-explained",
        "title": "How do Vision Transformers Work? Architecture Explained",
        "content": "Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial. This architectural shift enables ViTs to use the powerful self-attention mechanism that made transformers successful in NLP, allowing them to model long-range dependencies and global relationships in images more effectively than CNNs. The ViT architecture consists of four main components: patch embedding, positional encoding, transformer encoder, and classification head. The original vision transformer architecture proposed in the paper \u201cAn Image Is Worth 16X16 Words\u201d contains different components such as patch embedding, positional encoding, transformer encoder, multilayer perceptron head, etc, and looks as follows:. * **Global feature modeling**: In vision transformers, every patch of an image interacts with every other patch during the self-attention mechanism. It\u2019s a self-supervised learning framework for vision transformers that allows a ViT model to learn robust and invariant features from images without needing human-labeled data. Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial.",
        "score": 0.61363083,
        "raw_content": null,
        "id": 104
    },
    {
        "url": "https://www.coursera.org/articles/transformers-vs-convolutional-neural-networks",
        "title": "Transformers vs. Convolutional Neural Networks",
        "content": "Transformers and convolutional neural networks are both powerful deep learning algorithms for computer vision, but they work differently and have different strengths and weaknesses. If you want to use a deep learning neural network to extract meaningful data from images, you may decide to use a convolutional neural network or a visual transformer. Explore transformers versus convolutional neural networks to learn how each of these models works, what you can use them for, and how they compare to each other. A transformer is a deep learning neural network that uses an encoder/decoder architecture to first break down the input into the base characteristics that define the data, then use its understanding of patterns to rebuild the data with original but similar information. Transformer models and convolutional neural networks are both deep learning models that you can use for computer vision, but they are different in how they empower computer vision.",
        "score": 0.5735118,
        "raw_content": null,
        "id": 105
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-34756-z",
        "title": "A multiscale transformer with spatial attention for ...",
        "content": "each tailored to extract valuable insights from the multi-dimensional HSI data. Among these approaches, the k-nearest neighbour18, 11514\u201311521. [...] designs and lightweight transformer modules to further reduce computational costs without compromising overall performance, making the framework even more suitable for HSI classification tasks. [...] To tackle these challenges, we propose MTSA-Net, a novel multiscale transformer framework with spatial attention for HSI classification. The model begins with a 3D convolution layer to extract shallow spectral\u2013spatial features, followed by a 2D convolution layer and a spatial attention module. This design reduces feature redundancy, mitigates inaccuracies that often arise in deeper networks, and emphasizes the most discriminative spatial features, thereby alleviating the limited spatial resolution of HSIs. The refined feature vectors are then processed by multiple parallel transformer encoder branches with varying hidden dimensions, enabling simultaneous modeling of fine-grained local details, intermediate relationships, and global representations. Finally, a multiscale feature fusion",
        "score": 0.98975134,
        "raw_content": null,
        "id": 106
    },
    {
        "url": "https://worldmodelbench.github.io/static/pdfs/7_Adaptive_Attention_Guided_Ma.pdf",
        "title": "Adaptive Attention-Guided Masking in Vision Transformers ...",
        "content": "Art. no.\n5511817. 2, 5, 7 Adaptive Attention-Guided Masking in Vision Transformers for Self-Supervised Hyperspectral Feature Learning Supplementary Material 6. Appendix In recent years, world models like Vision Transformers (ViTs)  have demonstrated impressive performance in various computer vision [6, 8, 13] tasks, including hyper-spectral image (HSI) classification . However, their ap-plication to HSI faces several limitations due to the unique challenges of hyperspectral data.\nChallenges. First, the high dimensionality of HSI, with several spectral bands, complicates processing for conven-tional ViTs, which are optimized for RGB images with only three channels. This necessitates extensive modifications to handle spectral complexity. [...] Hyperspectral image (HSI) classification has rapidly ad-vanced with diverse methods tackling spectral-spatial data challenges.\nEarly models like 2-DCNN  combined simple 2D convolutions with fully connected layers, while SPRN  improved spatial features using attention mech-anisms and residual blocks. 3-DCNN  introduced 3D convolutions to jointly capture spectral and spatial infor-mation.\nHybrid models like HybridSN  merged 2D and 3D CNNs for richer spectral-spatial representations.\nThe emergence of transformer-based approaches marked a pivotal shift, emphasizing efficient feature extraction and long-range dependency modeling. GAHT  and Mor-phFormer  leveraged CNN-transformer hybrids and self-attention for enhanced spectral-spatial learning. [...] 4. Analysis of Results In this section, we comprehensively analyze results in var-ious scenarios with three HSI datasets : Indian Pines, Salinas, and Botswana using the two performance metrics: Overall Accuracy (OA) and Cohen\u2019s Kappa coefficient (\u03ba).",
        "score": 0.98975134,
        "raw_content": null,
        "id": 107
    },
    {
        "url": "https://arxiv.org/html/2502.06894v1",
        "title": "AI-Driven HSI: Multimodality, Fusion, Challenges, and the ...",
        "content": "HSI, helping researchers select the right architecture for their problems. [...] intensity measurements are collected across spectral bands and represented as a 3D data cube. The observed intensity at wavelength for pixel can be expressed as: [...] TABLE IV: Deep Learning Models and their Applications in HSI and Multimodal HSI",
        "score": 0.98934746,
        "raw_content": null,
        "id": 108
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-12489-3",
        "title": "Spectral-spatial wave and frequency interactive ...",
        "content": "The proposed framework is designed for HSI. It utilizes a combination of spectral spatial feature extractor with frequency domain transformer encoder block to extract phase and frequency features. [...] Figure 1 depicts the proposed model, this section will elucidate the model\u2019s structure and it\u2019s functioning. The feature pre-processing stage begins by feeding HSI patches into 3D convolutional layer and 2D convolutional layer for low-level feature extraction after applying Feature Augmentation. Shallow feature maps generated by these layers are then processed using pixel operations to tokenize the features before passing them into the transformer encoder Blocks. The core of the model lies in the transformer Blocks, which are repeated N times to enable interactions between tokens representing different spectral spatial locations in the frequency domain. Finally, a classification head, consisting of a Global Average Pooling (GAP) layer is employed to assign a label to each pixel. [...] (2021).\") treated HSI patches as sequential data from the spectral dimension, utilizing group-spectral embedding and a transformer encoder module to capture locally detailed spectral representations. Roy et al.31, 7831\u20137843.",
        "score": 0.98282325,
        "raw_content": null,
        "id": 109
    },
    {
        "url": "https://arxiv.org/html/2508.08107v1",
        "title": "Hyperspectral Imaging",
        "content": "HSI focuses on the optical window of the electromagnetic spectrum (see Fig. 1A), typically covering wavelengths from 380 to 2500 nm. This window usually encompasses the visible light (400-700 nm), near-infrared (NIR), and shortwave infrared (SWIR) regions, as shown in Fig. 1B. Notably, the inclusion of the 380-400 nm region further enhances sensitivity to pigment absorption and subtle surface reflectance variations, particularly in the violet and near-ultraviolet range. An example scene illustrating typical hyperspectral data is shown in Fig. 1C, highlighting that HSI routinely captures over hundreds of spectral channels at high spectral resolution (commonly 5-10 nm). [...] Transform-based approaches reduce dimensionality by projecting the original data into a lower-dimensional subspace that preserves the most informative spectral or spatial-spectral structures. Among these, principal component analysis (PCA)  and minimum noise fraction (MNF)  are two of the most representative methods, often serving as baseline techniques for illustrating DR in HSI due to their easy-interpretable and friendly-practical linearized modeling. More specifically, let be the spectral vector at the -th pixel, where is the number of spectral bands and is the total number of pixels in the HSI image. PCA seeks an orthogonal transformation that maximizes the variance of the projected data:\n\n|  |  |  |\n --- \n|  |  | (5) |\n|  | [...] Table 1: Comparative summary of different HSI platforms.",
        "score": 0.9693242,
        "raw_content": null,
        "id": 110
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-33943-2",
        "title": "Multi-scale boundary-aware network for remote sensing ...",
        "content": "classification of high-resolution remote sensing images. By simultaneously leveraging the local features of CNN and the global dependencies of Transformer through multi-scale self-attention and bidirectional knowledge distillation, it improves the accuracy of remote sensing semantic segmentation. Fan et al.33:17.\") proposed a Multidimensional Information Fusion Network for high-resolution remote sensing image segmentation by integrating CNN and Transformer and introducing frequency information. By multi-scale integration of local details, global semantics and frequency domain features, the model\u2019s ability to identify fine-grained boundaries and scale-varying targets is enhanced, effectively alleviating complex interference factors such as inter-class ambiguity. The hybrid attention [...] Despite significant advances in remote sensing semantic segmentation achieved by existing CNNs, Transformers, and hybrid approaches, high-resolution imagery remains challenging: Insufficient multi-scale feature characterisation prevents the model from capturing boundary features, leading to blurred segmentation boundaries caused by classification errors at the junctures of different semantic categories. Furthermore, recent research on remote sensing image segmentation has underscored the importance of boundary-aware modelling in dense prediction tasks22.\"), 23.\"). These considerations have prompted us to develop a unified framework that synergistically enhances both multi-scale representation and boundary-aware capabilities. To address these challenges, we propose MSBANet\u2014a specially [...] Traditional CNNs have remarkable effects in extracting local spatial details, but their capabilities in modeling global contexts are limited. Although Transformer-based methods can capture long-range dependencies, they often ignore fine structures and boundary information and have high computational complexity. In semantic segmentation tasks, both global information and local information are crucial for accurately understanding the semantic structure of images. Researchers have begun to combine CNN with Transformer models to fully leverage their respective advantages. Chen et al. proposed CTSeg32.\"), a novel CNN and ViT collaborative segmentation framework following the encoder-decoder architecture for land-use/land-cover classification of high-resolution remote sensing images. By",
        "score": 0.38512084,
        "raw_content": null,
        "id": 111
    },
    {
        "url": "https://www.sciencedirect.com/science/article/abs/pii/S1350449525001173",
        "title": "Edge Feature Enhanced Transformer Network for RGB and ...",
        "content": "In real-world object detection scenarios, factors such as rain, fog, occlusion, varying illumination, and low resolution hinder the ability of visible light imaging to achieve high-precision target detection performance . In comparison to RGB single-modality based object detection, RGB-infrared multi-modality based object detection is able to leverage diverse and complementary information. As there are substantial visual disparities between the two modal images. For instance, RGB images are sensitive to color and texture, where objects exhibit clearer texture details compared to that in infrared images during the day while diminishing under nighttime conditions (see Fig. 1(a) and Fig. 1(b)). Instead, infrared (IR) images are sensitive for temperature variation, they enable to offer [...] images achieve high- precision detection and recognition by extracting local spatial\u2013temporal features. However, convolutional operations is not capable for global information perception and struggle to model dependencies between features. Therefore, there is still potential to improve the fusion quality by taking consideration of the global feature learning. [...] In multispectral object detection, the complementary nature of infrared and visible images remains underexploited due to perceptual differences and spatial misalignment between modalities, posing significant challenges to accurate detection. We propose MRT-DETR (Multispectral RT-DETR), an end-to-end multispectral real-time detection framework that addresses weak alignment and complex lighting conditions by combining brightness-aware weighting with deformable alignment. Built on the RT-DETR backbone, our method introduces an Early-stage Deformable Alignment (EDA) module that learns attention-guided offsets at shallow layers to explicitly align infrared features to visible features in the spatial domain. Additionally, a Dual-Branch Brightness Weighting (DBW) module derives patch-wise",
        "score": 0.3059777,
        "raw_content": null,
        "id": 112
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-34756-z",
        "title": "A multiscale transformer with spatial attention for ...",
        "content": "Article  ADS   Google Scholar\n7. Hong, D. et al. Spectralformer: Rethinking hyperspectral image classification with transformers. IEEE Transactions on Geoscience and Remote Sensing 60, 1\u201315 (2021).\n\n   Article   Google Scholar\n8. Zhou, P., Han, J., Cheng, G. & Zhang, B. Learning compact and discriminative stacked autoencoder for hyperspectral image classification. IEEE Transactions on Geoscience and Remote Sensing 57(7), 4823\u20134833 (2019).\n\n   Article  ADS   Google Scholar\n9. Zheng, X., Yuan, Y. & Lu, X. Dimensionality reduction by spatial-spectral preservation in selected bands. IEEE Transactions on Geoscience and Remote Sensing 55(9), 5185\u20135197 (2017). [...] The Houston 2013 (H-13) dataset was gathered using the ITRES CASI-1500 sensor and covers the University of Houston premises and the neighbouring rural regions. This dataset has been publicly employed for assessing the effectiveness of HSI classification methods. This dataset has an image size of \\(349 \\times 1905\\) pixels and includes 144 spectral bands. It encompasses 15 challenging distinct classes, making it a valuable resource for land cover classification studies. The dataset samples have been divided into training, validation, and test sets, with the allocation details provided in Table 4. [...] Furthermore, a hybrid approach incorporating both CNN and Transformer networks48.\") was proposed to effectively calculate spectral-spatial and semantic features for HSI classification. In66.\"), Swin Transformer was presented, which calculates representations through a shifted window strategy. This approach allows for efficient processing of visual data with diverse scales and higher resolutions. Adopting this strategy effectively addresses the challenges presented by scale and resolution disparities, rendering it well-suited for HSI classification.\n\n## Proposed methodology",
        "score": 0.2640859,
        "raw_content": null,
        "id": 113
    },
    {
        "url": "https://arxiv.org/html/2507.22791v1",
        "title": "Modality-Aware Feature Matching: A Comprehensive ...",
        "content": "In this survey, our discussion addresses unique challenges and methodologies related to single-modal feature matching (e.g., RGB, depth, medical imaging) and cross-modal scenarios (e.g., medical image registration, vision-language integration). We delineate the progression from classical detector-based pipelines towards contemporary detector-free solutions. Figure 1 illustrates the overall pipeline of the survey, clearly mapping the evolution of feature matching methodologies across various data modalities. Furthermore, Figure 2 offers representative examples of the results of the matching of modality-awarecharacteristics.\n\n## 2. RGB Images [...] In summary, deep learning-based methods have greatly advanced the state of RGB image matching. They have produced more repeatable detectors (learning where to detect points that survive viewpoint/illumination changes), more discriminative descriptors (learning how to describe patches for higher distinctiveness), and even entirely new paradigms for matching (learning to match with attention mechanisms, and end-to-end trainable pipelines). Table 1 highlights some of the influential methods from both the handcrafted and deep learning eras. These modern techniques, when evaluated on challenging benchmarks, significantly outperform classic methods in terms of matching precision and robustness, although often at a higher computational cost. Nonetheless, due to their task-specific optimizations [...] ## 2. RGB Images\n\nFeature matching in RGB images traditionally follows a pipeline of feature detection, feature description, and feature matching. Over the decades, a lot of methods have been proposed for each stage of this pipeline, from handcrafted approaches to more recent deep learning-based techniques.\n\n### 2.1. Handcrafted Feature Detectors and Descriptors",
        "score": 0.26285648,
        "raw_content": null,
        "id": 114
    },
    {
        "url": "https://link.springer.com/article/10.1007/s10462-025-11169-y",
        "title": "A review of hyperspectral image classification based on graph ...",
        "content": "\")) of hyperspectral imaging (HSI) pose significant challenges to achieving accurate HSIC (Sawant and Prabukumar 2020 A survey of band selection techniques for hyperspectral image classification. J Spectr Imaging 9(1):5. \n                  \n                  \n                \")). Specifically, the large number of spectral bands and data points increases computational complexity (Ullah et al. 2024 Conventional to deep ensemble methods for hyperspectral image classification: a comprehensive survey. IEEE J Sel Top Appl Earth Obs Remote Sens. [...] ### 2.2 Technologies for hyperspectral image classification\n\nOptical remote sensing images can be categorized into three main types based on the number of spectral bands: panchromatic images with a single band, multiband RGB images, and hyperspectral images (HSI) containing hundreds of continuous, narrow spectral bands (Mather and Tso 2016 Classification methods for remotely sensed data. CRC Press, Boca Raton\")). HSI, with its unique technical advantages, achieves a deep integration of imaging and spectral technologies, enabling comprehensive data acquisition in both two-dimensional spatial geometry and one-dimensional spectral information. [...] Although deep learning has achieved remarkable breakthroughs in hyperspectral image classification (HSIC), the unique characteristics of hyperspectral image (HSI) data still present numerous challenges (Wan et al. 2020a Hyperspectral image classification with context-aware dynamic graph convolutional network. IEEE Trans Geosci Remote Sens 59(1):597\u2013612.",
        "score": 0.21413255,
        "raw_content": null,
        "id": 115
    },
    {
        "url": "https://www.geeksforgeeks.org/deep-learning/vision-transformer-vit-architecture/",
        "title": "Vision Transformer (ViT) Architecture",
        "content": "Vision Transformer (ViT) is a deep learning architecture that applies the Transformer model to images. Instead of relying on convolutions,",
        "score": 0.78788257,
        "raw_content": null,
        "id": 116
    },
    {
        "url": "https://manimalakumar-29300.medium.com/generative-ai-model-architecture-part3-vision-transformer-363a0a81a7c2",
        "title": "Generative AI Model Architecture \u2014 Part3: Vision Transformer",
        "content": "# Generative AI Model Architecture \u2014 Part3: Vision Transformer. Vision Transformer (ViT) adapts the Transformer architecture \u2014 originally designed to process sequential data \u2014 to make sense of non-sequential data. There were specialized CNN model architectures for each of the fundamental vision tasks: image classification, object detection, segmentation, etc. ViTs develop a broad understanding of the image content, and overcome such limitations of CNNs. LVMs \u2014 the modern computer vision models \u2014 are increasingly built on ViT-based architectures. Since ViTs process image patches in parallel, position embeddings tell the model where each patch belongs. Self-distillation is a student-teacher architecture, where a student model learns to predict the output of a teacher model. This means the teacher is not a fixed, pre-trained model \u2014 over time, as the student improves, the teacher is updated to be a moving average of the student, ensuring the teacher remains a reliable target and promoting progressive learning. Part 4: Generative AI Model Architecture \u2014 Part4: Vision Language Model.",
        "score": 0.7602419,
        "raw_content": null,
        "id": 117
    },
    {
        "url": "https://blog.roboflow.com/vision-transformers/",
        "title": "Vision Transformers Explained: The Future of Computer ...",
        "content": "Transformers are a powerful deep learning architecture originally developed for natural language processing (NLP) tasks such as machine translation,",
        "score": 0.6659544,
        "raw_content": null,
        "id": 118
    },
    {
        "url": "https://www.codecademy.com/article/vision-transformers-working-architecture-explained",
        "title": "How do Vision Transformers Work? Architecture Explained",
        "content": "Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial. This architectural shift enables ViTs to use the powerful self-attention mechanism that made transformers successful in NLP, allowing them to model long-range dependencies and global relationships in images more effectively than CNNs. The ViT architecture consists of four main components: patch embedding, positional encoding, transformer encoder, and classification head. The original vision transformer architecture proposed in the paper \u201cAn Image Is Worth 16X16 Words\u201d contains different components such as patch embedding, positional encoding, transformer encoder, multilayer perceptron head, etc, and looks as follows:. * **Global feature modeling**: In vision transformers, every patch of an image interacts with every other patch during the self-attention mechanism. It\u2019s a self-supervised learning framework for vision transformers that allows a ViT model to learn robust and invariant features from images without needing human-labeled data. Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial.",
        "score": 0.61363083,
        "raw_content": null,
        "id": 119
    },
    {
        "url": "https://www.coursera.org/articles/transformers-vs-convolutional-neural-networks",
        "title": "Transformers vs. Convolutional Neural Networks",
        "content": "Transformers and convolutional neural networks are both powerful deep learning algorithms for computer vision, but they work differently and have different strengths and weaknesses. If you want to use a deep learning neural network to extract meaningful data from images, you may decide to use a convolutional neural network or a visual transformer. Explore transformers versus convolutional neural networks to learn how each of these models works, what you can use them for, and how they compare to each other. A transformer is a deep learning neural network that uses an encoder/decoder architecture to first break down the input into the base characteristics that define the data, then use its understanding of patterns to rebuild the data with original but similar information. Transformer models and convolutional neural networks are both deep learning models that you can use for computer vision, but they are different in how they empower computer vision.",
        "score": 0.5735118,
        "raw_content": null,
        "id": 120
    },
    {
        "url": "https://www.geeksforgeeks.org/deep-learning/vision-transformer-vit-architecture/",
        "title": "Vision Transformer (ViT) Architecture",
        "content": "Vision Transformer (ViT) is a deep learning architecture that applies the Transformer model to images. Instead of relying on convolutions,",
        "score": 0.78788257,
        "raw_content": null,
        "id": 121
    },
    {
        "url": "https://manimalakumar-29300.medium.com/generative-ai-model-architecture-part3-vision-transformer-363a0a81a7c2",
        "title": "Generative AI Model Architecture \u2014 Part3: Vision Transformer",
        "content": "# Generative AI Model Architecture \u2014 Part3: Vision Transformer. Vision Transformer (ViT) adapts the Transformer architecture \u2014 originally designed to process sequential data \u2014 to make sense of non-sequential data. There were specialized CNN model architectures for each of the fundamental vision tasks: image classification, object detection, segmentation, etc. ViTs develop a broad understanding of the image content, and overcome such limitations of CNNs. LVMs \u2014 the modern computer vision models \u2014 are increasingly built on ViT-based architectures. Since ViTs process image patches in parallel, position embeddings tell the model where each patch belongs. Self-distillation is a student-teacher architecture, where a student model learns to predict the output of a teacher model. This means the teacher is not a fixed, pre-trained model \u2014 over time, as the student improves, the teacher is updated to be a moving average of the student, ensuring the teacher remains a reliable target and promoting progressive learning. Part 4: Generative AI Model Architecture \u2014 Part4: Vision Language Model.",
        "score": 0.7602419,
        "raw_content": null,
        "id": 122
    },
    {
        "url": "https://blog.roboflow.com/vision-transformers/",
        "title": "Vision Transformers Explained: The Future of Computer ...",
        "content": "Transformers are a powerful deep learning architecture originally developed for natural language processing (NLP) tasks such as machine translation,",
        "score": 0.6659544,
        "raw_content": null,
        "id": 123
    },
    {
        "url": "https://www.codecademy.com/article/vision-transformers-working-architecture-explained",
        "title": "How do Vision Transformers Work? Architecture Explained",
        "content": "Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial. This architectural shift enables ViTs to use the powerful self-attention mechanism that made transformers successful in NLP, allowing them to model long-range dependencies and global relationships in images more effectively than CNNs. The ViT architecture consists of four main components: patch embedding, positional encoding, transformer encoder, and classification head. The original vision transformer architecture proposed in the paper \u201cAn Image Is Worth 16X16 Words\u201d contains different components such as patch embedding, positional encoding, transformer encoder, multilayer perceptron head, etc, and looks as follows:. * **Global feature modeling**: In vision transformers, every patch of an image interacts with every other patch during the self-attention mechanism. It\u2019s a self-supervised learning framework for vision transformers that allows a ViT model to learn robust and invariant features from images without needing human-labeled data. Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial.",
        "score": 0.61363083,
        "raw_content": null,
        "id": 124
    },
    {
        "url": "https://www.coursera.org/articles/transformers-vs-convolutional-neural-networks",
        "title": "Transformers vs. Convolutional Neural Networks",
        "content": "Transformers and convolutional neural networks are both powerful deep learning algorithms for computer vision, but they work differently and have different strengths and weaknesses. If you want to use a deep learning neural network to extract meaningful data from images, you may decide to use a convolutional neural network or a visual transformer. Explore transformers versus convolutional neural networks to learn how each of these models works, what you can use them for, and how they compare to each other. A transformer is a deep learning neural network that uses an encoder/decoder architecture to first break down the input into the base characteristics that define the data, then use its understanding of patterns to rebuild the data with original but similar information. Transformer models and convolutional neural networks are both deep learning models that you can use for computer vision, but they are different in how they empower computer vision.",
        "score": 0.5735118,
        "raw_content": null,
        "id": 125
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-34756-z",
        "title": "A multiscale transformer with spatial attention for ...",
        "content": "each tailored to extract valuable insights from the multi-dimensional HSI data. Among these approaches, the k-nearest neighbour18, 11514\u201311521. [...] designs and lightweight transformer modules to further reduce computational costs without compromising overall performance, making the framework even more suitable for HSI classification tasks. [...] To tackle these challenges, we propose MTSA-Net, a novel multiscale transformer framework with spatial attention for HSI classification. The model begins with a 3D convolution layer to extract shallow spectral\u2013spatial features, followed by a 2D convolution layer and a spatial attention module. This design reduces feature redundancy, mitigates inaccuracies that often arise in deeper networks, and emphasizes the most discriminative spatial features, thereby alleviating the limited spatial resolution of HSIs. The refined feature vectors are then processed by multiple parallel transformer encoder branches with varying hidden dimensions, enabling simultaneous modeling of fine-grained local details, intermediate relationships, and global representations. Finally, a multiscale feature fusion",
        "score": 0.98975134,
        "raw_content": null,
        "id": 126
    },
    {
        "url": "https://worldmodelbench.github.io/static/pdfs/7_Adaptive_Attention_Guided_Ma.pdf",
        "title": "Adaptive Attention-Guided Masking in Vision Transformers ...",
        "content": "Art. no.\n5511817. 2, 5, 7 Adaptive Attention-Guided Masking in Vision Transformers for Self-Supervised Hyperspectral Feature Learning Supplementary Material 6. Appendix In recent years, world models like Vision Transformers (ViTs)  have demonstrated impressive performance in various computer vision [6, 8, 13] tasks, including hyper-spectral image (HSI) classification . However, their ap-plication to HSI faces several limitations due to the unique challenges of hyperspectral data.\nChallenges. First, the high dimensionality of HSI, with several spectral bands, complicates processing for conven-tional ViTs, which are optimized for RGB images with only three channels. This necessitates extensive modifications to handle spectral complexity. [...] Hyperspectral image (HSI) classification has rapidly ad-vanced with diverse methods tackling spectral-spatial data challenges.\nEarly models like 2-DCNN  combined simple 2D convolutions with fully connected layers, while SPRN  improved spatial features using attention mech-anisms and residual blocks. 3-DCNN  introduced 3D convolutions to jointly capture spectral and spatial infor-mation.\nHybrid models like HybridSN  merged 2D and 3D CNNs for richer spectral-spatial representations.\nThe emergence of transformer-based approaches marked a pivotal shift, emphasizing efficient feature extraction and long-range dependency modeling. GAHT  and Mor-phFormer  leveraged CNN-transformer hybrids and self-attention for enhanced spectral-spatial learning. [...] 4. Analysis of Results In this section, we comprehensively analyze results in var-ious scenarios with three HSI datasets : Indian Pines, Salinas, and Botswana using the two performance metrics: Overall Accuracy (OA) and Cohen\u2019s Kappa coefficient (\u03ba).",
        "score": 0.98975134,
        "raw_content": null,
        "id": 127
    },
    {
        "url": "https://arxiv.org/html/2502.06894v1",
        "title": "AI-Driven HSI: Multimodality, Fusion, Challenges, and the ...",
        "content": "HSI, helping researchers select the right architecture for their problems. [...] intensity measurements are collected across spectral bands and represented as a 3D data cube. The observed intensity at wavelength for pixel can be expressed as: [...] TABLE IV: Deep Learning Models and their Applications in HSI and Multimodal HSI",
        "score": 0.98934746,
        "raw_content": null,
        "id": 128
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-12489-3",
        "title": "Spectral-spatial wave and frequency interactive ...",
        "content": "The proposed framework is designed for HSI. It utilizes a combination of spectral spatial feature extractor with frequency domain transformer encoder block to extract phase and frequency features. [...] Figure 1 depicts the proposed model, this section will elucidate the model\u2019s structure and it\u2019s functioning. The feature pre-processing stage begins by feeding HSI patches into 3D convolutional layer and 2D convolutional layer for low-level feature extraction after applying Feature Augmentation. Shallow feature maps generated by these layers are then processed using pixel operations to tokenize the features before passing them into the transformer encoder Blocks. The core of the model lies in the transformer Blocks, which are repeated N times to enable interactions between tokens representing different spectral spatial locations in the frequency domain. Finally, a classification head, consisting of a Global Average Pooling (GAP) layer is employed to assign a label to each pixel. [...] (2021).\") treated HSI patches as sequential data from the spectral dimension, utilizing group-spectral embedding and a transformer encoder module to capture locally detailed spectral representations. Roy et al.31, 7831\u20137843.",
        "score": 0.98282325,
        "raw_content": null,
        "id": 129
    },
    {
        "url": "https://arxiv.org/html/2508.08107v1",
        "title": "Hyperspectral Imaging",
        "content": "HSI focuses on the optical window of the electromagnetic spectrum (see Fig. 1A), typically covering wavelengths from 380 to 2500 nm. This window usually encompasses the visible light (400-700 nm), near-infrared (NIR), and shortwave infrared (SWIR) regions, as shown in Fig. 1B. Notably, the inclusion of the 380-400 nm region further enhances sensitivity to pigment absorption and subtle surface reflectance variations, particularly in the violet and near-ultraviolet range. An example scene illustrating typical hyperspectral data is shown in Fig. 1C, highlighting that HSI routinely captures over hundreds of spectral channels at high spectral resolution (commonly 5-10 nm). [...] Transform-based approaches reduce dimensionality by projecting the original data into a lower-dimensional subspace that preserves the most informative spectral or spatial-spectral structures. Among these, principal component analysis (PCA)  and minimum noise fraction (MNF)  are two of the most representative methods, often serving as baseline techniques for illustrating DR in HSI due to their easy-interpretable and friendly-practical linearized modeling. More specifically, let be the spectral vector at the -th pixel, where is the number of spectral bands and is the total number of pixels in the HSI image. PCA seeks an orthogonal transformation that maximizes the variance of the projected data:\n\n|  |  |  |\n --- \n|  |  | (5) |\n|  | [...] Table 1: Comparative summary of different HSI platforms.",
        "score": 0.9693242,
        "raw_content": null,
        "id": 130
    },
    {
        "url": "https://www.geeksforgeeks.org/deep-learning/vision-transformer-vit-architecture/",
        "title": "Vision Transformer (ViT) Architecture",
        "content": "Vision Transformer (ViT) is a deep learning architecture that applies the Transformer model to images. Instead of relying on convolutions,",
        "score": 0.78788257,
        "raw_content": null,
        "id": 131
    },
    {
        "url": "https://manimalakumar-29300.medium.com/generative-ai-model-architecture-part3-vision-transformer-363a0a81a7c2",
        "title": "Generative AI Model Architecture \u2014 Part3: Vision Transformer",
        "content": "# Generative AI Model Architecture \u2014 Part3: Vision Transformer. Vision Transformer (ViT) adapts the Transformer architecture \u2014 originally designed to process sequential data \u2014 to make sense of non-sequential data. There were specialized CNN model architectures for each of the fundamental vision tasks: image classification, object detection, segmentation, etc. ViTs develop a broad understanding of the image content, and overcome such limitations of CNNs. LVMs \u2014 the modern computer vision models \u2014 are increasingly built on ViT-based architectures. Since ViTs process image patches in parallel, position embeddings tell the model where each patch belongs. Self-distillation is a student-teacher architecture, where a student model learns to predict the output of a teacher model. This means the teacher is not a fixed, pre-trained model \u2014 over time, as the student improves, the teacher is updated to be a moving average of the student, ensuring the teacher remains a reliable target and promoting progressive learning. Part 4: Generative AI Model Architecture \u2014 Part4: Vision Language Model.",
        "score": 0.7602419,
        "raw_content": null,
        "id": 132
    },
    {
        "url": "https://blog.roboflow.com/vision-transformers/",
        "title": "Vision Transformers Explained: The Future of Computer ...",
        "content": "Transformers are a powerful deep learning architecture originally developed for natural language processing (NLP) tasks such as machine translation,",
        "score": 0.6659544,
        "raw_content": null,
        "id": 133
    },
    {
        "url": "https://www.codecademy.com/article/vision-transformers-working-architecture-explained",
        "title": "How do Vision Transformers Work? Architecture Explained",
        "content": "Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial. This architectural shift enables ViTs to use the powerful self-attention mechanism that made transformers successful in NLP, allowing them to model long-range dependencies and global relationships in images more effectively than CNNs. The ViT architecture consists of four main components: patch embedding, positional encoding, transformer encoder, and classification head. The original vision transformer architecture proposed in the paper \u201cAn Image Is Worth 16X16 Words\u201d contains different components such as patch embedding, positional encoding, transformer encoder, multilayer perceptron head, etc, and looks as follows:. * **Global feature modeling**: In vision transformers, every patch of an image interacts with every other patch during the self-attention mechanism. It\u2019s a self-supervised learning framework for vision transformers that allows a ViT model to learn robust and invariant features from images without needing human-labeled data. Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial.",
        "score": 0.61363083,
        "raw_content": null,
        "id": 134
    },
    {
        "url": "https://www.coursera.org/articles/transformers-vs-convolutional-neural-networks",
        "title": "Transformers vs. Convolutional Neural Networks",
        "content": "Transformers and convolutional neural networks are both powerful deep learning algorithms for computer vision, but they work differently and have different strengths and weaknesses. If you want to use a deep learning neural network to extract meaningful data from images, you may decide to use a convolutional neural network or a visual transformer. Explore transformers versus convolutional neural networks to learn how each of these models works, what you can use them for, and how they compare to each other. A transformer is a deep learning neural network that uses an encoder/decoder architecture to first break down the input into the base characteristics that define the data, then use its understanding of patterns to rebuild the data with original but similar information. Transformer models and convolutional neural networks are both deep learning models that you can use for computer vision, but they are different in how they empower computer vision.",
        "score": 0.5735118,
        "raw_content": null,
        "id": 135
    },
    {
        "url": "https://www.geeksforgeeks.org/deep-learning/vision-transformer-vit-architecture/",
        "title": "Vision Transformer (ViT) Architecture",
        "content": "Vision Transformer (ViT) is a deep learning architecture that applies the Transformer model to images. Instead of relying on convolutions,",
        "score": 0.78788257,
        "raw_content": null,
        "id": 136
    },
    {
        "url": "https://manimalakumar-29300.medium.com/generative-ai-model-architecture-part3-vision-transformer-363a0a81a7c2",
        "title": "Generative AI Model Architecture \u2014 Part3: Vision Transformer",
        "content": "# Generative AI Model Architecture \u2014 Part3: Vision Transformer. Vision Transformer (ViT) adapts the Transformer architecture \u2014 originally designed to process sequential data \u2014 to make sense of non-sequential data. There were specialized CNN model architectures for each of the fundamental vision tasks: image classification, object detection, segmentation, etc. ViTs develop a broad understanding of the image content, and overcome such limitations of CNNs. LVMs \u2014 the modern computer vision models \u2014 are increasingly built on ViT-based architectures. Since ViTs process image patches in parallel, position embeddings tell the model where each patch belongs. Self-distillation is a student-teacher architecture, where a student model learns to predict the output of a teacher model. This means the teacher is not a fixed, pre-trained model \u2014 over time, as the student improves, the teacher is updated to be a moving average of the student, ensuring the teacher remains a reliable target and promoting progressive learning. Part 4: Generative AI Model Architecture \u2014 Part4: Vision Language Model.",
        "score": 0.7602419,
        "raw_content": null,
        "id": 137
    },
    {
        "url": "https://blog.roboflow.com/vision-transformers/",
        "title": "Vision Transformers Explained: The Future of Computer ...",
        "content": "Transformers are a powerful deep learning architecture originally developed for natural language processing (NLP) tasks such as machine translation,",
        "score": 0.6659544,
        "raw_content": null,
        "id": 138
    },
    {
        "url": "https://www.codecademy.com/article/vision-transformers-working-architecture-explained",
        "title": "How do Vision Transformers Work? Architecture Explained",
        "content": "Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial. This architectural shift enables ViTs to use the powerful self-attention mechanism that made transformers successful in NLP, allowing them to model long-range dependencies and global relationships in images more effectively than CNNs. The ViT architecture consists of four main components: patch embedding, positional encoding, transformer encoder, and classification head. The original vision transformer architecture proposed in the paper \u201cAn Image Is Worth 16X16 Words\u201d contains different components such as patch embedding, positional encoding, transformer encoder, multilayer perceptron head, etc, and looks as follows:. * **Global feature modeling**: In vision transformers, every patch of an image interacts with every other patch during the self-attention mechanism. It\u2019s a self-supervised learning framework for vision transformers that allows a ViT model to learn robust and invariant features from images without needing human-labeled data. Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial.",
        "score": 0.61363083,
        "raw_content": null,
        "id": 139
    },
    {
        "url": "https://www.coursera.org/articles/transformers-vs-convolutional-neural-networks",
        "title": "Transformers vs. Convolutional Neural Networks",
        "content": "Transformers and convolutional neural networks are both powerful deep learning algorithms for computer vision, but they work differently and have different strengths and weaknesses. If you want to use a deep learning neural network to extract meaningful data from images, you may decide to use a convolutional neural network or a visual transformer. Explore transformers versus convolutional neural networks to learn how each of these models works, what you can use them for, and how they compare to each other. A transformer is a deep learning neural network that uses an encoder/decoder architecture to first break down the input into the base characteristics that define the data, then use its understanding of patterns to rebuild the data with original but similar information. Transformer models and convolutional neural networks are both deep learning models that you can use for computer vision, but they are different in how they empower computer vision.",
        "score": 0.5735118,
        "raw_content": null,
        "id": 140
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-34756-z",
        "title": "A multiscale transformer with spatial attention for ...",
        "content": "each tailored to extract valuable insights from the multi-dimensional HSI data. Among these approaches, the k-nearest neighbour18, 11514\u201311521. [...] designs and lightweight transformer modules to further reduce computational costs without compromising overall performance, making the framework even more suitable for HSI classification tasks. [...] To tackle these challenges, we propose MTSA-Net, a novel multiscale transformer framework with spatial attention for HSI classification. The model begins with a 3D convolution layer to extract shallow spectral\u2013spatial features, followed by a 2D convolution layer and a spatial attention module. This design reduces feature redundancy, mitigates inaccuracies that often arise in deeper networks, and emphasizes the most discriminative spatial features, thereby alleviating the limited spatial resolution of HSIs. The refined feature vectors are then processed by multiple parallel transformer encoder branches with varying hidden dimensions, enabling simultaneous modeling of fine-grained local details, intermediate relationships, and global representations. Finally, a multiscale feature fusion",
        "score": 0.98975134,
        "raw_content": null,
        "id": 141
    },
    {
        "url": "https://worldmodelbench.github.io/static/pdfs/7_Adaptive_Attention_Guided_Ma.pdf",
        "title": "Adaptive Attention-Guided Masking in Vision Transformers ...",
        "content": "Art. no.\n5511817. 2, 5, 7 Adaptive Attention-Guided Masking in Vision Transformers for Self-Supervised Hyperspectral Feature Learning Supplementary Material 6. Appendix In recent years, world models like Vision Transformers (ViTs)  have demonstrated impressive performance in various computer vision [6, 8, 13] tasks, including hyper-spectral image (HSI) classification . However, their ap-plication to HSI faces several limitations due to the unique challenges of hyperspectral data.\nChallenges. First, the high dimensionality of HSI, with several spectral bands, complicates processing for conven-tional ViTs, which are optimized for RGB images with only three channels. This necessitates extensive modifications to handle spectral complexity. [...] Hyperspectral image (HSI) classification has rapidly ad-vanced with diverse methods tackling spectral-spatial data challenges.\nEarly models like 2-DCNN  combined simple 2D convolutions with fully connected layers, while SPRN  improved spatial features using attention mech-anisms and residual blocks. 3-DCNN  introduced 3D convolutions to jointly capture spectral and spatial infor-mation.\nHybrid models like HybridSN  merged 2D and 3D CNNs for richer spectral-spatial representations.\nThe emergence of transformer-based approaches marked a pivotal shift, emphasizing efficient feature extraction and long-range dependency modeling. GAHT  and Mor-phFormer  leveraged CNN-transformer hybrids and self-attention for enhanced spectral-spatial learning. [...] 4. Analysis of Results In this section, we comprehensively analyze results in var-ious scenarios with three HSI datasets : Indian Pines, Salinas, and Botswana using the two performance metrics: Overall Accuracy (OA) and Cohen\u2019s Kappa coefficient (\u03ba).",
        "score": 0.98975134,
        "raw_content": null,
        "id": 142
    },
    {
        "url": "https://arxiv.org/html/2502.06894v1",
        "title": "AI-Driven HSI: Multimodality, Fusion, Challenges, and the ...",
        "content": "HSI, helping researchers select the right architecture for their problems. [...] intensity measurements are collected across spectral bands and represented as a 3D data cube. The observed intensity at wavelength for pixel can be expressed as: [...] TABLE IV: Deep Learning Models and their Applications in HSI and Multimodal HSI",
        "score": 0.98934746,
        "raw_content": null,
        "id": 143
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-12489-3",
        "title": "Spectral-spatial wave and frequency interactive ...",
        "content": "The proposed framework is designed for HSI. It utilizes a combination of spectral spatial feature extractor with frequency domain transformer encoder block to extract phase and frequency features. [...] Figure 1 depicts the proposed model, this section will elucidate the model\u2019s structure and it\u2019s functioning. The feature pre-processing stage begins by feeding HSI patches into 3D convolutional layer and 2D convolutional layer for low-level feature extraction after applying Feature Augmentation. Shallow feature maps generated by these layers are then processed using pixel operations to tokenize the features before passing them into the transformer encoder Blocks. The core of the model lies in the transformer Blocks, which are repeated N times to enable interactions between tokens representing different spectral spatial locations in the frequency domain. Finally, a classification head, consisting of a Global Average Pooling (GAP) layer is employed to assign a label to each pixel. [...] (2021).\") treated HSI patches as sequential data from the spectral dimension, utilizing group-spectral embedding and a transformer encoder module to capture locally detailed spectral representations. Roy et al.31, 7831\u20137843.",
        "score": 0.98282325,
        "raw_content": null,
        "id": 144
    },
    {
        "url": "https://arxiv.org/html/2508.08107v1",
        "title": "Hyperspectral Imaging",
        "content": "HSI focuses on the optical window of the electromagnetic spectrum (see Fig. 1A), typically covering wavelengths from 380 to 2500 nm. This window usually encompasses the visible light (400-700 nm), near-infrared (NIR), and shortwave infrared (SWIR) regions, as shown in Fig. 1B. Notably, the inclusion of the 380-400 nm region further enhances sensitivity to pigment absorption and subtle surface reflectance variations, particularly in the violet and near-ultraviolet range. An example scene illustrating typical hyperspectral data is shown in Fig. 1C, highlighting that HSI routinely captures over hundreds of spectral channels at high spectral resolution (commonly 5-10 nm). [...] Transform-based approaches reduce dimensionality by projecting the original data into a lower-dimensional subspace that preserves the most informative spectral or spatial-spectral structures. Among these, principal component analysis (PCA)  and minimum noise fraction (MNF)  are two of the most representative methods, often serving as baseline techniques for illustrating DR in HSI due to their easy-interpretable and friendly-practical linearized modeling. More specifically, let be the spectral vector at the -th pixel, where is the number of spectral bands and is the total number of pixels in the HSI image. PCA seeks an orthogonal transformation that maximizes the variance of the projected data:\n\n|  |  |  |\n --- \n|  |  | (5) |\n|  | [...] Table 1: Comparative summary of different HSI platforms.",
        "score": 0.9693242,
        "raw_content": null,
        "id": 145
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-33943-2",
        "title": "Multi-scale boundary-aware network for remote sensing ...",
        "content": "classification of high-resolution remote sensing images. By simultaneously leveraging the local features of CNN and the global dependencies of Transformer through multi-scale self-attention and bidirectional knowledge distillation, it improves the accuracy of remote sensing semantic segmentation. Fan et al.33:17.\") proposed a Multidimensional Information Fusion Network for high-resolution remote sensing image segmentation by integrating CNN and Transformer and introducing frequency information. By multi-scale integration of local details, global semantics and frequency domain features, the model\u2019s ability to identify fine-grained boundaries and scale-varying targets is enhanced, effectively alleviating complex interference factors such as inter-class ambiguity. The hybrid attention [...] Despite significant advances in remote sensing semantic segmentation achieved by existing CNNs, Transformers, and hybrid approaches, high-resolution imagery remains challenging: Insufficient multi-scale feature characterisation prevents the model from capturing boundary features, leading to blurred segmentation boundaries caused by classification errors at the junctures of different semantic categories. Furthermore, recent research on remote sensing image segmentation has underscored the importance of boundary-aware modelling in dense prediction tasks22.\"), 23.\"). These considerations have prompted us to develop a unified framework that synergistically enhances both multi-scale representation and boundary-aware capabilities. To address these challenges, we propose MSBANet\u2014a specially [...] Traditional CNNs have remarkable effects in extracting local spatial details, but their capabilities in modeling global contexts are limited. Although Transformer-based methods can capture long-range dependencies, they often ignore fine structures and boundary information and have high computational complexity. In semantic segmentation tasks, both global information and local information are crucial for accurately understanding the semantic structure of images. Researchers have begun to combine CNN with Transformer models to fully leverage their respective advantages. Chen et al. proposed CTSeg32.\"), a novel CNN and ViT collaborative segmentation framework following the encoder-decoder architecture for land-use/land-cover classification of high-resolution remote sensing images. By",
        "score": 0.38512084,
        "raw_content": null,
        "id": 146
    },
    {
        "url": "https://www.sciencedirect.com/science/article/abs/pii/S1350449525001173",
        "title": "Edge Feature Enhanced Transformer Network for RGB and ...",
        "content": "In real-world object detection scenarios, factors such as rain, fog, occlusion, varying illumination, and low resolution hinder the ability of visible light imaging to achieve high-precision target detection performance . In comparison to RGB single-modality based object detection, RGB-infrared multi-modality based object detection is able to leverage diverse and complementary information. As there are substantial visual disparities between the two modal images. For instance, RGB images are sensitive to color and texture, where objects exhibit clearer texture details compared to that in infrared images during the day while diminishing under nighttime conditions (see Fig. 1(a) and Fig. 1(b)). Instead, infrared (IR) images are sensitive for temperature variation, they enable to offer [...] images achieve high- precision detection and recognition by extracting local spatial\u2013temporal features. However, convolutional operations is not capable for global information perception and struggle to model dependencies between features. Therefore, there is still potential to improve the fusion quality by taking consideration of the global feature learning. [...] In multispectral object detection, the complementary nature of infrared and visible images remains underexploited due to perceptual differences and spatial misalignment between modalities, posing significant challenges to accurate detection. We propose MRT-DETR (Multispectral RT-DETR), an end-to-end multispectral real-time detection framework that addresses weak alignment and complex lighting conditions by combining brightness-aware weighting with deformable alignment. Built on the RT-DETR backbone, our method introduces an Early-stage Deformable Alignment (EDA) module that learns attention-guided offsets at shallow layers to explicitly align infrared features to visible features in the spatial domain. Additionally, a Dual-Branch Brightness Weighting (DBW) module derives patch-wise",
        "score": 0.3059777,
        "raw_content": null,
        "id": 147
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-34756-z",
        "title": "A multiscale transformer with spatial attention for ...",
        "content": "Article  ADS   Google Scholar\n7. Hong, D. et al. Spectralformer: Rethinking hyperspectral image classification with transformers. IEEE Transactions on Geoscience and Remote Sensing 60, 1\u201315 (2021).\n\n   Article   Google Scholar\n8. Zhou, P., Han, J., Cheng, G. & Zhang, B. Learning compact and discriminative stacked autoencoder for hyperspectral image classification. IEEE Transactions on Geoscience and Remote Sensing 57(7), 4823\u20134833 (2019).\n\n   Article  ADS   Google Scholar\n9. Zheng, X., Yuan, Y. & Lu, X. Dimensionality reduction by spatial-spectral preservation in selected bands. IEEE Transactions on Geoscience and Remote Sensing 55(9), 5185\u20135197 (2017). [...] The Houston 2013 (H-13) dataset was gathered using the ITRES CASI-1500 sensor and covers the University of Houston premises and the neighbouring rural regions. This dataset has been publicly employed for assessing the effectiveness of HSI classification methods. This dataset has an image size of \\(349 \\times 1905\\) pixels and includes 144 spectral bands. It encompasses 15 challenging distinct classes, making it a valuable resource for land cover classification studies. The dataset samples have been divided into training, validation, and test sets, with the allocation details provided in Table 4. [...] Furthermore, a hybrid approach incorporating both CNN and Transformer networks48.\") was proposed to effectively calculate spectral-spatial and semantic features for HSI classification. In66.\"), Swin Transformer was presented, which calculates representations through a shifted window strategy. This approach allows for efficient processing of visual data with diverse scales and higher resolutions. Adopting this strategy effectively addresses the challenges presented by scale and resolution disparities, rendering it well-suited for HSI classification.\n\n## Proposed methodology",
        "score": 0.2640859,
        "raw_content": null,
        "id": 148
    },
    {
        "url": "https://arxiv.org/html/2507.22791v1",
        "title": "Modality-Aware Feature Matching: A Comprehensive ...",
        "content": "In this survey, our discussion addresses unique challenges and methodologies related to single-modal feature matching (e.g., RGB, depth, medical imaging) and cross-modal scenarios (e.g., medical image registration, vision-language integration). We delineate the progression from classical detector-based pipelines towards contemporary detector-free solutions. Figure 1 illustrates the overall pipeline of the survey, clearly mapping the evolution of feature matching methodologies across various data modalities. Furthermore, Figure 2 offers representative examples of the results of the matching of modality-awarecharacteristics.\n\n## 2. RGB Images [...] In summary, deep learning-based methods have greatly advanced the state of RGB image matching. They have produced more repeatable detectors (learning where to detect points that survive viewpoint/illumination changes), more discriminative descriptors (learning how to describe patches for higher distinctiveness), and even entirely new paradigms for matching (learning to match with attention mechanisms, and end-to-end trainable pipelines). Table 1 highlights some of the influential methods from both the handcrafted and deep learning eras. These modern techniques, when evaluated on challenging benchmarks, significantly outperform classic methods in terms of matching precision and robustness, although often at a higher computational cost. Nonetheless, due to their task-specific optimizations [...] ## 2. RGB Images\n\nFeature matching in RGB images traditionally follows a pipeline of feature detection, feature description, and feature matching. Over the decades, a lot of methods have been proposed for each stage of this pipeline, from handcrafted approaches to more recent deep learning-based techniques.\n\n### 2.1. Handcrafted Feature Detectors and Descriptors",
        "score": 0.26285648,
        "raw_content": null,
        "id": 149
    },
    {
        "url": "https://link.springer.com/article/10.1007/s10462-025-11169-y",
        "title": "A review of hyperspectral image classification based on graph ...",
        "content": "\")) of hyperspectral imaging (HSI) pose significant challenges to achieving accurate HSIC (Sawant and Prabukumar 2020 A survey of band selection techniques for hyperspectral image classification. J Spectr Imaging 9(1):5. \n                  \n                  \n                \")). Specifically, the large number of spectral bands and data points increases computational complexity (Ullah et al. 2024 Conventional to deep ensemble methods for hyperspectral image classification: a comprehensive survey. IEEE J Sel Top Appl Earth Obs Remote Sens. [...] ### 2.2 Technologies for hyperspectral image classification\n\nOptical remote sensing images can be categorized into three main types based on the number of spectral bands: panchromatic images with a single band, multiband RGB images, and hyperspectral images (HSI) containing hundreds of continuous, narrow spectral bands (Mather and Tso 2016 Classification methods for remotely sensed data. CRC Press, Boca Raton\")). HSI, with its unique technical advantages, achieves a deep integration of imaging and spectral technologies, enabling comprehensive data acquisition in both two-dimensional spatial geometry and one-dimensional spectral information. [...] Although deep learning has achieved remarkable breakthroughs in hyperspectral image classification (HSIC), the unique characteristics of hyperspectral image (HSI) data still present numerous challenges (Wan et al. 2020a Hyperspectral image classification with context-aware dynamic graph convolutional network. IEEE Trans Geosci Remote Sens 59(1):597\u2013612.",
        "score": 0.21413255,
        "raw_content": null,
        "id": 150
    },
    {
        "url": "https://eureka.patsnap.com/article/self-attention-in-vision-transformers-why-it-captures-long-range-dependencies",
        "title": "Self-Attention in Vision Transformers: Why It Captures Long-Range Dependencies",
        "content": "Conclusion  \n  \nVision Transformers, with their self-attention mechanisms, have demonstrated a remarkable ability to capture long-range dependencies in images, setting a new standard in computer vision. By considering every part of an image in relation to every other part, ViTs provide a comprehensive understanding that surpasses traditional methods. As research continues to advance, we can expect Vision Transformers to play an increasingly prominent role in the future of image processing, unlocking new possibilities for innovation and application. [...] Capturing Long-Range Dependencies  \n  \nOne of the standout features of self-attention in ViTs is its ability to capture long-range dependencies. Traditional CNNs are constrained by the size of their receptive fields, which limits their ability to consider distant relationships in an image. In contrast, self-attention allows each image patch to attend to all other patches, regardless of their spatial distance. [...] This is achieved by dividing an image into a sequence of patches, treating these patches similarly to words in a sentence. Each patch is linearly embedded into a fixed-size vector, and these vectors are then passed through layers of self-attention. This process allows the model to weigh the importance of different parts of the image when making predictions, effectively capturing both local and global context.  \n  \nCapturing Long-Range Dependencies",
        "score": 0.99985456,
        "raw_content": null,
        "id": 151
    },
    {
        "url": "https://eureka.patsnap.com/article/how-do-transformers-handle-long-range-dependencies",
        "title": "How Do Transformers Handle Long-Range Dependencies?",
        "content": "Self-Attention Mechanism  \n  \nAt the heart of the transformer architecture is the self-attention mechanism. This mechanism allows transformers to weigh the importance of each word in a sequence relative to others, regardless of their distance. Self-attention computes a representation of each word by considering the entire sequence, enabling the model to capture dependencies over long ranges efficiently.  \n  \nThe self-attention mechanism works by creating three vectors for each word: query, key, and value. These vectors are used to calculate attention scores, which determine how much focus should be given to each word in the sequence. This process allows transformers to create context-aware representations of words, making them adept at understanding intricate dependencies. [...] Layer Stacking and Multi-Head Attention  \n  \nTransformers consist of multiple layers, each containing a self-attention mechanism followed by feed-forward neural networks. These layers are stacked on top of each other, enabling the model to learn complex patterns and dependencies. Moreover, transformers employ a technique called multi-head attention, where multiple self-attention mechanisms run in parallel at each layer. This allows the model to focus on different parts of the sequence simultaneously, enhancing its ability to capture a variety of dependencies.  \n  \nHandling Long Sequences [...] Conclusion  \n  \nTransformers have revolutionized the way we handle long-range dependencies in NLP tasks. Their innovative architecture, characterized by self-attention mechanisms, positional encoding, and multi-head attention, allows them to effectively capture complex relationships in sequences. As a result, transformers have set new benchmarks in NLP, proving to be a powerful tool for processing language data over long ranges. With ongoing research and development, the capabilities of transformers continue to expand, promising even more sophisticated handling of long-range dependencies in the future.",
        "score": 0.9994642,
        "raw_content": null,
        "id": 152
    },
    {
        "url": "https://www.mdpi.com/2072-4292/17/21/3532",
        "title": "Advances on Multimodal Remote Sensing Foundation ...",
        "content": "The Vision Transformer (ViT) is a DL model that uses the Transformer network architecture to handle visual image tasks. Its core technologies include the multi-head self-attention mechanism (MHSA) and positional encoding [83,84,85]. The basic principle of the Vision Transformer is to divide the image into several small blocks containing various ground feature pixels, regard each small block as an image sequence unit, and then extract the spatial and temporal features of the image through the encoder\u2013decoder of the Transformer layer. The self-attention mechanism can perform weighted summation for each position in the input image sequence and extract the global dependence of the image sequence by modeling interactions among any pixels in the input image sequence . The Vision Transformer",
        "score": 0.9992084,
        "raw_content": null,
        "id": 153
    },
    {
        "url": "https://towardsai.net/p/machine-learning/attention-is-all-you-need-a-deep-dive-into-the-revolutionary-transformer-architecture",
        "title": "Attention Is All You Need - A Deep Dive into the ...",
        "content": "In summary, attention mechanisms in the Transformer allow the model to dynamically focus on different parts of the input sequence, capturing both local and long-range dependencies. The scaled dot-product attention mechanism is particularly efficient and effective, making it a cornerstone of the Transformer architecture. In the next section, we will explore how self-attention uses this mechanism to capture relationships between words within a sequence.\n\n5. Self-Attention in Detail\n\nSelf-attention is a specific type of attention where the queries, keys, and values all come from the same source. In the context of the Transformer, this means that each position in a sequence can attend to all positions in the same sequence, allowing the model to capture intricate relationships between words. [...] Image 20\nWhy Self-Attention Works\n\nSelf-attention has several key advantages over traditional sequence processing methods:\n\n1.   Captures Long-Range Dependencies: Unlike RNNs, which struggle with long-range dependencies, self-attention connects any two positions directly, regardless of their distance in the sequence.\n\n2. Parallel Computation: All positions can be processed in parallel, unlike the sequential nature of RNNs.\n\n3. Interpretable Attention Weights: The attention weights provide insights into which words the model is focusing on, adding a level of interpretability.\n\n4. Constant Path Length: The number of operations required to connect positions in the network is constant, not dependent on sequence length.\n\nImage 21 [...] Transformer-XL (2019): Extended the Transformer with a segment-level recurrence mechanism and relative positional encoding, enabling it to learn dependencies beyond a fixed-length context.\n   Reformer (2020): Used locality-sensitive hashing to reduce the computational complexity of self-attention from O(n\u00b2) to O(n log n), making it more efficient for long sequences.\n   Linformer (2020): Reduced the complexity of self-attention to O(n) by projecting the length dimension of keys and values.\n   Performer (2020): Approximated the attention matrix using kernelization techniques, reducing memory and computational requirements.\n   Vision Transformer (ViT, 2020): Adapted the Transformer for computer vision by treating image patches as sequence elements.",
        "score": 0.9989183,
        "raw_content": null,
        "id": 154
    },
    {
        "url": "https://arxiv.org/html/2410.16602v3",
        "title": "Foundation Models for Remote Sensing and Earth ...",
        "content": "Transformers. Transformers [25, 26], as depicted in Fig. 6, are designed to process sequence data using self-attention mechanisms, enabling them to capture relationships between data points regardless of their positional distance. Unlike CNNs, which emphasize local features, transformers excel at modeling global dependencies, making them particularly effective for long-range interactions. A significant advantage of transformers in FMs is their ability to integrate across multiple modalities, enabling them to process a diverse range of inputs, such as visual data (e.g., images, depth, thermal) and non-visual data (e.g., text, 3D point clouds, audio). This cross-modality integration paves the way for unified RSFMs capable of handling a wide spectrum of geospatial data modalities.",
        "score": 0.998259,
        "raw_content": null,
        "id": 155
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-34756-z",
        "title": "A multiscale transformer with spatial attention for ...",
        "content": "Similarly, in41, 67 (2017).\"), an extended investigation was conducted on 3D CNNs for spectral\u2013spatial classification, utilizing input cubes with reduced spatial dimensions from HSIs. These models were designed to generate thematic maps by directly processing raw HSIs. Despite the achievements of CNN-based methods in extracting spatial and spectral information, they possess specific constraints. In40, 6232\u20136251 (2016).\"), a 3D CNN was employed to directly and efficiently learn spectral-spatial features from the original HSI, showcasing promising outcomes in terms of classification performance. Furthermore, a hybrid approach incorporating both CNN and Transformer networks48.\") was proposed to effectively calculate spectral-spatial and semantic features for HSI classification. In this work, a novel framework, MTSA-Net, is introduced for HSI classification, harnessing the combined strengths of spatial attention and multiscale transformers to effectively utilize the spatial-spectral information in hyperspectral data. Spectral-spatial feature extraction for hyperspectral image classification: A dimension reduction and deep learning approach. Learning deep hierarchical spatial-spectral features for hyperspectral image classification based on residual 3d\u20132d cnn.",
        "score": 0.9971328,
        "raw_content": null,
        "id": 156
    },
    {
        "url": "https://link.springer.com/article/10.1007/s10462-025-11169-y",
        "title": "A review of hyperspectral image classification based on graph ...",
        "content": "*et al.* A review of hyperspectral image classification based on graph neural networks. Hybrid models based on graph neural networks for hyperspectral image (HSI) classification combine GNNs with other deep learning models. \")) combined the multi-view deep autoencoder (MVDAE) and the semi-supervised graph convolutional network (SSGCN) to propose a novel approach for spectral-spatial classification of HSI, called MV-DNNet. MV-DNNet leverages a small number of labeled samples to integrate spectral and spatial features, significantly enhancing hyperspectral image classification performance. * Bai J, Ding B, Xiao Z, Jiao L, Chen H, Regan AC (2021) Hyperspectral image classification based on deep attention graph convolutional network. * Dong Y, Liu Q, Du B, Zhang L (2022) Weighted feature fusion of convolutional neural network and graph attention network for hyperspectral image classification. * Yang P, Tong L, Qian B, Gao Z, Yu J, Xiao C (2020) Hyperspectral image classification with spectral and spatial graph using inductive representation learning network.",
        "score": 0.99101454,
        "raw_content": null,
        "id": 157
    },
    {
        "url": "https://ieeexplore.ieee.org/iel8/4609443/4609444/10877784.pdf",
        "title": "Frequency-Aware Hierarchical Mamba for Hyperspectral ...",
        "content": "by P Zhuang \u00b7 2025 \u00b7 Cited by 19 \u2014 For HSI classification, MambaHSI [27] utilized Mamba blocks to si- multaneously model long-range spatial interactions and extract spectral features, achieving",
        "score": 0.9522199,
        "raw_content": null,
        "id": 158
    },
    {
        "url": "https://www.sciencedirect.com/science/article/abs/pii/S0925231225011002",
        "title": "A comprehensive survey for Hyperspectral Image ...",
        "content": "by M Ahmad \u00b7 2025 \u00b7 Cited by 49 \u2014 Unlike traditional methods, Hyperspectral Images (HSIs) provide a continuous spectrum through numerous narrow bands, enabling precise material characterization",
        "score": 0.9092645,
        "raw_content": null,
        "id": 159
    },
    {
        "url": "https://www.mdpi.com/2072-4292/17/14/2489",
        "title": "Spatial and Spectral Structure-Aware Mamba Network for ...",
        "content": "Current Mamba-based HSI classification methods [24,25,26,27] typically flatten the 2D spatial structure into the 1D sequence and then use a fixed scanning strategy to extract spatial features from the 1D sequence, which inevitably changes the spatial relationship between pixels, destroys the inherent contextual information in the image, and affects the classification results. Classification maps produced by various methods applied to the Pavia University dataset: (**a**) ground truth; (**b**) 2DCNN; (**c**) 3DCNN; (**d**) HybridSN; (**e**) SPRN; (**f**) SpectralFormer; (**g**) SSFTT; (**h**) MorphFormer; (**i**) GSC-VIT; (**j**) 3DSS-Mamba; (**k**) SS-Mamba; (**l**) SSUM; (**m**) DADFMamba. Classification maps produced by various methods applied to the Pavia University dataset: (**a**) ground truth; (**b**) 2DCNN; (**c**) 3DCNN; (**d**) HybridSN; (**e**) SPRN; (**f**) SpectralFormer; (**g**) SSFTT; (**h**) MorphFormer; (**i**) GSC-VIT; (**j**) 3DSS-Mamba; (**k**) SS-Mamba; (**l**) SSUM; (**m**) DADFMamba. Classification maps produced by various methods applied to the Pavia University dataset: (**a**) ground truth; (**b**) 2DCNN; (**c**) 3DCNN; (**d**) HybridSN; (**e**) SPRN; (**f**) SpectralFormer; (**g**) SSFTT; (**h**) MorphFormer; (**i**) GSC-VIT; (**j**) 3DSS-Mamba; (**k**) SS-Mamba; (**l**) SSUM; (**m**) DADFMamba.",
        "score": 0.8864204,
        "raw_content": null,
        "id": 160
    },
    {
        "url": "https://arxiv.org/html/2502.06894v1",
        "title": "AI-Driven HSI: Multimodality, Fusion, Challenges, and the ...",
        "content": "intensity measurements are collected across spectral bands and represented as a 3D data cube. The observed intensity at wavelength for pixel can be expressed as: [...] 1. 1.\n\n   What is HSI, HSI image, signature, HSI Pixel, and hypercube?\n2. 2.\n\n   How do single-modal and multimodal HSI systems differ in data acquisition, fusion, challenges, and AI tools for processing HSI images?\n3. 3.\n\n   What is the role of AI in enhancing HSI systems, particularly in learning, classification, and challenges?\n4. 4.\n\n   What is the CAGR of the HSI industry, and what factors contribute to its growth?\n5. 5.\n\n   What are the current challenges, open research issues, and emerging trends in HSI technology. [...] ### III-D Hyperspectral Signature: HSI Pixel\n\nHyperspectral imaging signatures represent the unique spectral characteristics of materials or objects. HSI captures and processes data across the electromagnetic spectrum, providing spectral information for each pixel. Materials reflect, absorb, or emit electromagnetic radiation differently across wavelengths, creating distinct spectral \u201dfingerprints.\u201d These signatures enable the identification and analysis of materials based on their spectral properties. For example, vegetation, water, soil, and minerals each have distinct signatures, and subtle variations in plant species allow for differentiation .\n\n## IV HSI Image Processing and Pre-Processing",
        "score": 0.97462875,
        "raw_content": null,
        "id": 161
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-34756-z",
        "title": "A multiscale transformer with spatial attention for ...",
        "content": "each tailored to extract valuable insights from the multi-dimensional HSI data. Among these approaches, the k-nearest neighbour18, 11514\u201311521. [...] The generalization capability and effectiveness of the proposed MTSA-Net model have been validated through extensive experiments conducted on five benchmark HSI datasets, demonstrating superior performance compared to state-of-the-art approaches.\n\nThe remainder of this work is summarized as follows. Related works to HSI are reviewed in Sect. \u201cRelated work\u201d. The Sect. \u201cProposed methodology\u201d elaborates on the proposed methodology. Section \"Experiment and analysis\" illustrates the HSI datasets, experimental settings, and offers an in-depth analysis of classification results and ablation studies. Finally, conclusions are provided in Sect. \u201cConclusion\u201d.\n\n## Related work\n\n### CNN-based HSI classification frameworks [...] designs and lightweight transformer modules to further reduce computational costs without compromising overall performance, making the framework even more suitable for HSI classification tasks.",
        "score": 0.97344345,
        "raw_content": null,
        "id": 162
    },
    {
        "url": "https://arxiv.org/html/2508.08107v1",
        "title": "Hyperspectral Imaging",
        "content": "HSI focuses on the optical window of the electromagnetic spectrum (see Fig. 1A), typically covering wavelengths from 380 to 2500 nm. This window usually encompasses the visible light (400-700 nm), near-infrared (NIR), and shortwave infrared (SWIR) regions, as shown in Fig. 1B. Notably, the inclusion of the 380-400 nm region further enhances sensitivity to pigment absorption and subtle surface reflectance variations, particularly in the violet and near-ultraviolet range. An example scene illustrating typical hyperspectral data is shown in Fig. 1C, highlighting that HSI routinely captures over hundreds of spectral channels at high spectral resolution (commonly 5-10 nm). [...] Transform-based approaches reduce dimensionality by projecting the original data into a lower-dimensional subspace that preserves the most informative spectral or spatial-spectral structures. Among these, principal component analysis (PCA)  and minimum noise fraction (MNF)  are two of the most representative methods, often serving as baseline techniques for illustrating DR in HSI due to their easy-interpretable and friendly-practical linearized modeling. More specifically, let be the spectral vector at the -th pixel, where is the number of spectral bands and is the total number of pixels in the HSI image. PCA seeks an orthogonal transformation that maximizes the variance of the projected data:\n\n|  |  |  |\n --- \n|  |  | (5) |\n|  | [...] sources like multispectral or RGB images. This can be expressed as , where denotes a learnable spectral upsampling operator, and () is the reference high-resolution HSI data.",
        "score": 0.9518632,
        "raw_content": null,
        "id": 163
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-12489-3",
        "title": "Spectral-spatial wave and frequency interactive ...",
        "content": "The proposed framework is designed for HSI. It utilizes a combination of spectral spatial feature extractor with frequency domain transformer encoder block to extract phase and frequency features. [...] .\"), vision transformers have gained significant attention due to their remarkable success, prompting researchers to explore their application in image processing and computer vision. In contrast to convolutional networks, transformers offer a fresh perspective and new possibilities for improving performance at the attention level. The key factor behind the transformer\u2019s success lies in its ability to effectively model long-range relationships, overcoming limitations faced by traditional architectures. The transformer has been recently introduced in HSI. Hong et al.29 and He et al.30. [...] (2021).\") treated HSI patches as sequential data from the spectral dimension, utilizing group-spectral embedding and a transformer encoder module to capture locally detailed spectral representations. Roy et al.31, 7831\u20137843.",
        "score": 0.9449947,
        "raw_content": null,
        "id": 164
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-28351-5",
        "title": "Transfer learning models for wheat ear detection on multi ...",
        "content": "the RGB into the HSI (hue, saturation, intensity) color model113.\") and by applying thresholding within the hue channel, followed by a morphological closing operation for noise reduction. The final region denoting the frame is obtained using the connected component algorithm, implemented within the OpenCV library114.\"),115. Accessed: October 28, 2022, [...] .\") with autofocus and dual optical image stabilization is used for imaging. The smartphone was positioned 1 meter above the crop canopy such that the whole ROI, bounded by a 0.5 m x 0.5 m wooden frame, was in the center of the image and occupied the middle part of the one wheat plot (see Fig. 2b). Although a standardized protocol for image acquisition is not established in the literature, a GSD of 0.3 mm is recommended by57.\"), and this recommendation was followed in our study. Acquired raw images were further appropriately rotated such that the wooden frame was parallel to the image axis, and the ROI was segmented by extracting all pixels within the wooden frame (see Fig. 2). First, the frame was segmented by converting the RGB into the HSI (hue, saturation, intensity)",
        "score": 0.92944044,
        "raw_content": null,
        "id": 165
    },
    {
        "url": "https://www.geeksforgeeks.org/deep-learning/vision-transformer-vit-architecture/",
        "title": "Vision Transformer (ViT) Architecture",
        "content": "Vision Transformer (ViT) is a deep learning architecture that applies the Transformer model to images. Instead of relying on convolutions,",
        "score": 0.78788257,
        "raw_content": null,
        "id": 166
    },
    {
        "url": "https://manimalakumar-29300.medium.com/generative-ai-model-architecture-part3-vision-transformer-363a0a81a7c2",
        "title": "Generative AI Model Architecture \u2014 Part3: Vision Transformer",
        "content": "# Generative AI Model Architecture \u2014 Part3: Vision Transformer. Vision Transformer (ViT) adapts the Transformer architecture \u2014 originally designed to process sequential data \u2014 to make sense of non-sequential data. There were specialized CNN model architectures for each of the fundamental vision tasks: image classification, object detection, segmentation, etc. ViTs develop a broad understanding of the image content, and overcome such limitations of CNNs. LVMs \u2014 the modern computer vision models \u2014 are increasingly built on ViT-based architectures. Since ViTs process image patches in parallel, position embeddings tell the model where each patch belongs. Self-distillation is a student-teacher architecture, where a student model learns to predict the output of a teacher model. This means the teacher is not a fixed, pre-trained model \u2014 over time, as the student improves, the teacher is updated to be a moving average of the student, ensuring the teacher remains a reliable target and promoting progressive learning. Part 4: Generative AI Model Architecture \u2014 Part4: Vision Language Model.",
        "score": 0.7602419,
        "raw_content": null,
        "id": 167
    },
    {
        "url": "https://blog.roboflow.com/vision-transformers/",
        "title": "Vision Transformers Explained: The Future of Computer ...",
        "content": "Transformers are a powerful deep learning architecture originally developed for natural language processing (NLP) tasks such as machine translation,",
        "score": 0.6659544,
        "raw_content": null,
        "id": 168
    },
    {
        "url": "https://www.codecademy.com/article/vision-transformers-working-architecture-explained",
        "title": "How do Vision Transformers Work? Architecture Explained",
        "content": "Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial. This architectural shift enables ViTs to use the powerful self-attention mechanism that made transformers successful in NLP, allowing them to model long-range dependencies and global relationships in images more effectively than CNNs. The ViT architecture consists of four main components: patch embedding, positional encoding, transformer encoder, and classification head. The original vision transformer architecture proposed in the paper \u201cAn Image Is Worth 16X16 Words\u201d contains different components such as patch embedding, positional encoding, transformer encoder, multilayer perceptron head, etc, and looks as follows:. * **Global feature modeling**: In vision transformers, every patch of an image interacts with every other patch during the self-attention mechanism. It\u2019s a self-supervised learning framework for vision transformers that allows a ViT model to learn robust and invariant features from images without needing human-labeled data. Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial.",
        "score": 0.61363083,
        "raw_content": null,
        "id": 169
    },
    {
        "url": "https://www.coursera.org/articles/transformers-vs-convolutional-neural-networks",
        "title": "Transformers vs. Convolutional Neural Networks",
        "content": "Transformers and convolutional neural networks are both powerful deep learning algorithms for computer vision, but they work differently and have different strengths and weaknesses. If you want to use a deep learning neural network to extract meaningful data from images, you may decide to use a convolutional neural network or a visual transformer. Explore transformers versus convolutional neural networks to learn how each of these models works, what you can use them for, and how they compare to each other. A transformer is a deep learning neural network that uses an encoder/decoder architecture to first break down the input into the base characteristics that define the data, then use its understanding of patterns to rebuild the data with original but similar information. Transformer models and convolutional neural networks are both deep learning models that you can use for computer vision, but they are different in how they empower computer vision.",
        "score": 0.5735118,
        "raw_content": null,
        "id": 170
    },
    {
        "url": "https://www.geeksforgeeks.org/deep-learning/vision-transformer-vit-architecture/",
        "title": "Vision Transformer (ViT) Architecture",
        "content": "Vision Transformer (ViT) is a deep learning architecture that applies the Transformer model to images. Instead of relying on convolutions,",
        "score": 0.78788257,
        "raw_content": null,
        "id": 171
    },
    {
        "url": "https://manimalakumar-29300.medium.com/generative-ai-model-architecture-part3-vision-transformer-363a0a81a7c2",
        "title": "Generative AI Model Architecture \u2014 Part3: Vision Transformer",
        "content": "# Generative AI Model Architecture \u2014 Part3: Vision Transformer. Vision Transformer (ViT) adapts the Transformer architecture \u2014 originally designed to process sequential data \u2014 to make sense of non-sequential data. There were specialized CNN model architectures for each of the fundamental vision tasks: image classification, object detection, segmentation, etc. ViTs develop a broad understanding of the image content, and overcome such limitations of CNNs. LVMs \u2014 the modern computer vision models \u2014 are increasingly built on ViT-based architectures. Since ViTs process image patches in parallel, position embeddings tell the model where each patch belongs. Self-distillation is a student-teacher architecture, where a student model learns to predict the output of a teacher model. This means the teacher is not a fixed, pre-trained model \u2014 over time, as the student improves, the teacher is updated to be a moving average of the student, ensuring the teacher remains a reliable target and promoting progressive learning. Part 4: Generative AI Model Architecture \u2014 Part4: Vision Language Model.",
        "score": 0.7602419,
        "raw_content": null,
        "id": 172
    },
    {
        "url": "https://blog.roboflow.com/vision-transformers/",
        "title": "Vision Transformers Explained: The Future of Computer ...",
        "content": "Transformers are a powerful deep learning architecture originally developed for natural language processing (NLP) tasks such as machine translation,",
        "score": 0.6659544,
        "raw_content": null,
        "id": 173
    },
    {
        "url": "https://www.codecademy.com/article/vision-transformers-working-architecture-explained",
        "title": "How do Vision Transformers Work? Architecture Explained",
        "content": "Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial. This architectural shift enables ViTs to use the powerful self-attention mechanism that made transformers successful in NLP, allowing them to model long-range dependencies and global relationships in images more effectively than CNNs. The ViT architecture consists of four main components: patch embedding, positional encoding, transformer encoder, and classification head. The original vision transformer architecture proposed in the paper \u201cAn Image Is Worth 16X16 Words\u201d contains different components such as patch embedding, positional encoding, transformer encoder, multilayer perceptron head, etc, and looks as follows:. * **Global feature modeling**: In vision transformers, every patch of an image interacts with every other patch during the self-attention mechanism. It\u2019s a self-supervised learning framework for vision transformers that allows a ViT model to learn robust and invariant features from images without needing human-labeled data. Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial.",
        "score": 0.61363083,
        "raw_content": null,
        "id": 174
    },
    {
        "url": "https://www.coursera.org/articles/transformers-vs-convolutional-neural-networks",
        "title": "Transformers vs. Convolutional Neural Networks",
        "content": "Transformers and convolutional neural networks are both powerful deep learning algorithms for computer vision, but they work differently and have different strengths and weaknesses. If you want to use a deep learning neural network to extract meaningful data from images, you may decide to use a convolutional neural network or a visual transformer. Explore transformers versus convolutional neural networks to learn how each of these models works, what you can use them for, and how they compare to each other. A transformer is a deep learning neural network that uses an encoder/decoder architecture to first break down the input into the base characteristics that define the data, then use its understanding of patterns to rebuild the data with original but similar information. Transformer models and convolutional neural networks are both deep learning models that you can use for computer vision, but they are different in how they empower computer vision.",
        "score": 0.5735118,
        "raw_content": null,
        "id": 175
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-34756-z",
        "title": "A multiscale transformer with spatial attention for ...",
        "content": "each tailored to extract valuable insights from the multi-dimensional HSI data. Among these approaches, the k-nearest neighbour18, 11514\u201311521. [...] designs and lightweight transformer modules to further reduce computational costs without compromising overall performance, making the framework even more suitable for HSI classification tasks. [...] To tackle these challenges, we propose MTSA-Net, a novel multiscale transformer framework with spatial attention for HSI classification. The model begins with a 3D convolution layer to extract shallow spectral\u2013spatial features, followed by a 2D convolution layer and a spatial attention module. This design reduces feature redundancy, mitigates inaccuracies that often arise in deeper networks, and emphasizes the most discriminative spatial features, thereby alleviating the limited spatial resolution of HSIs. The refined feature vectors are then processed by multiple parallel transformer encoder branches with varying hidden dimensions, enabling simultaneous modeling of fine-grained local details, intermediate relationships, and global representations. Finally, a multiscale feature fusion",
        "score": 0.98975134,
        "raw_content": null,
        "id": 176
    },
    {
        "url": "https://worldmodelbench.github.io/static/pdfs/7_Adaptive_Attention_Guided_Ma.pdf",
        "title": "Adaptive Attention-Guided Masking in Vision Transformers ...",
        "content": "Art. no.\n5511817. 2, 5, 7 Adaptive Attention-Guided Masking in Vision Transformers for Self-Supervised Hyperspectral Feature Learning Supplementary Material 6. Appendix In recent years, world models like Vision Transformers (ViTs)  have demonstrated impressive performance in various computer vision [6, 8, 13] tasks, including hyper-spectral image (HSI) classification . However, their ap-plication to HSI faces several limitations due to the unique challenges of hyperspectral data.\nChallenges. First, the high dimensionality of HSI, with several spectral bands, complicates processing for conven-tional ViTs, which are optimized for RGB images with only three channels. This necessitates extensive modifications to handle spectral complexity. [...] Hyperspectral image (HSI) classification has rapidly ad-vanced with diverse methods tackling spectral-spatial data challenges.\nEarly models like 2-DCNN  combined simple 2D convolutions with fully connected layers, while SPRN  improved spatial features using attention mech-anisms and residual blocks. 3-DCNN  introduced 3D convolutions to jointly capture spectral and spatial infor-mation.\nHybrid models like HybridSN  merged 2D and 3D CNNs for richer spectral-spatial representations.\nThe emergence of transformer-based approaches marked a pivotal shift, emphasizing efficient feature extraction and long-range dependency modeling. GAHT  and Mor-phFormer  leveraged CNN-transformer hybrids and self-attention for enhanced spectral-spatial learning. [...] 4. Analysis of Results In this section, we comprehensively analyze results in var-ious scenarios with three HSI datasets : Indian Pines, Salinas, and Botswana using the two performance metrics: Overall Accuracy (OA) and Cohen\u2019s Kappa coefficient (\u03ba).",
        "score": 0.98975134,
        "raw_content": null,
        "id": 177
    },
    {
        "url": "https://arxiv.org/html/2502.06894v1",
        "title": "AI-Driven HSI: Multimodality, Fusion, Challenges, and the ...",
        "content": "HSI, helping researchers select the right architecture for their problems. [...] intensity measurements are collected across spectral bands and represented as a 3D data cube. The observed intensity at wavelength for pixel can be expressed as: [...] TABLE IV: Deep Learning Models and their Applications in HSI and Multimodal HSI",
        "score": 0.98934746,
        "raw_content": null,
        "id": 178
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-12489-3",
        "title": "Spectral-spatial wave and frequency interactive ...",
        "content": "The proposed framework is designed for HSI. It utilizes a combination of spectral spatial feature extractor with frequency domain transformer encoder block to extract phase and frequency features. [...] Figure 1 depicts the proposed model, this section will elucidate the model\u2019s structure and it\u2019s functioning. The feature pre-processing stage begins by feeding HSI patches into 3D convolutional layer and 2D convolutional layer for low-level feature extraction after applying Feature Augmentation. Shallow feature maps generated by these layers are then processed using pixel operations to tokenize the features before passing them into the transformer encoder Blocks. The core of the model lies in the transformer Blocks, which are repeated N times to enable interactions between tokens representing different spectral spatial locations in the frequency domain. Finally, a classification head, consisting of a Global Average Pooling (GAP) layer is employed to assign a label to each pixel. [...] (2021).\") treated HSI patches as sequential data from the spectral dimension, utilizing group-spectral embedding and a transformer encoder module to capture locally detailed spectral representations. Roy et al.31, 7831\u20137843.",
        "score": 0.98282325,
        "raw_content": null,
        "id": 179
    },
    {
        "url": "https://arxiv.org/html/2508.08107v1",
        "title": "Hyperspectral Imaging",
        "content": "HSI focuses on the optical window of the electromagnetic spectrum (see Fig. 1A), typically covering wavelengths from 380 to 2500 nm. This window usually encompasses the visible light (400-700 nm), near-infrared (NIR), and shortwave infrared (SWIR) regions, as shown in Fig. 1B. Notably, the inclusion of the 380-400 nm region further enhances sensitivity to pigment absorption and subtle surface reflectance variations, particularly in the violet and near-ultraviolet range. An example scene illustrating typical hyperspectral data is shown in Fig. 1C, highlighting that HSI routinely captures over hundreds of spectral channels at high spectral resolution (commonly 5-10 nm). [...] Transform-based approaches reduce dimensionality by projecting the original data into a lower-dimensional subspace that preserves the most informative spectral or spatial-spectral structures. Among these, principal component analysis (PCA)  and minimum noise fraction (MNF)  are two of the most representative methods, often serving as baseline techniques for illustrating DR in HSI due to their easy-interpretable and friendly-practical linearized modeling. More specifically, let be the spectral vector at the -th pixel, where is the number of spectral bands and is the total number of pixels in the HSI image. PCA seeks an orthogonal transformation that maximizes the variance of the projected data:\n\n|  |  |  |\n --- \n|  |  | (5) |\n|  | [...] Table 1: Comparative summary of different HSI platforms.",
        "score": 0.9693242,
        "raw_content": null,
        "id": 180
    },
    {
        "url": "https://www.geeksforgeeks.org/deep-learning/vision-transformer-vit-architecture/",
        "title": "Vision Transformer (ViT) Architecture",
        "content": "Vision Transformer (ViT) is a deep learning architecture that applies the Transformer model to images. Instead of relying on convolutions,",
        "score": 0.78788257,
        "raw_content": null,
        "id": 181
    },
    {
        "url": "https://manimalakumar-29300.medium.com/generative-ai-model-architecture-part3-vision-transformer-363a0a81a7c2",
        "title": "Generative AI Model Architecture \u2014 Part3: Vision Transformer",
        "content": "# Generative AI Model Architecture \u2014 Part3: Vision Transformer. Vision Transformer (ViT) adapts the Transformer architecture \u2014 originally designed to process sequential data \u2014 to make sense of non-sequential data. There were specialized CNN model architectures for each of the fundamental vision tasks: image classification, object detection, segmentation, etc. ViTs develop a broad understanding of the image content, and overcome such limitations of CNNs. LVMs \u2014 the modern computer vision models \u2014 are increasingly built on ViT-based architectures. Since ViTs process image patches in parallel, position embeddings tell the model where each patch belongs. Self-distillation is a student-teacher architecture, where a student model learns to predict the output of a teacher model. This means the teacher is not a fixed, pre-trained model \u2014 over time, as the student improves, the teacher is updated to be a moving average of the student, ensuring the teacher remains a reliable target and promoting progressive learning. Part 4: Generative AI Model Architecture \u2014 Part4: Vision Language Model.",
        "score": 0.7602419,
        "raw_content": null,
        "id": 182
    },
    {
        "url": "https://blog.roboflow.com/vision-transformers/",
        "title": "Vision Transformers Explained: The Future of Computer ...",
        "content": "Transformers are a powerful deep learning architecture originally developed for natural language processing (NLP) tasks such as machine translation,",
        "score": 0.6659544,
        "raw_content": null,
        "id": 183
    },
    {
        "url": "https://www.codecademy.com/article/vision-transformers-working-architecture-explained",
        "title": "How do Vision Transformers Work? Architecture Explained",
        "content": "Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial. This architectural shift enables ViTs to use the powerful self-attention mechanism that made transformers successful in NLP, allowing them to model long-range dependencies and global relationships in images more effectively than CNNs. The ViT architecture consists of four main components: patch embedding, positional encoding, transformer encoder, and classification head. The original vision transformer architecture proposed in the paper \u201cAn Image Is Worth 16X16 Words\u201d contains different components such as patch embedding, positional encoding, transformer encoder, multilayer perceptron head, etc, and looks as follows:. * **Global feature modeling**: In vision transformers, every patch of an image interacts with every other patch during the self-attention mechanism. It\u2019s a self-supervised learning framework for vision transformers that allows a ViT model to learn robust and invariant features from images without needing human-labeled data. Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial.",
        "score": 0.61363083,
        "raw_content": null,
        "id": 184
    },
    {
        "url": "https://www.coursera.org/articles/transformers-vs-convolutional-neural-networks",
        "title": "Transformers vs. Convolutional Neural Networks",
        "content": "Transformers and convolutional neural networks are both powerful deep learning algorithms for computer vision, but they work differently and have different strengths and weaknesses. If you want to use a deep learning neural network to extract meaningful data from images, you may decide to use a convolutional neural network or a visual transformer. Explore transformers versus convolutional neural networks to learn how each of these models works, what you can use them for, and how they compare to each other. A transformer is a deep learning neural network that uses an encoder/decoder architecture to first break down the input into the base characteristics that define the data, then use its understanding of patterns to rebuild the data with original but similar information. Transformer models and convolutional neural networks are both deep learning models that you can use for computer vision, but they are different in how they empower computer vision.",
        "score": 0.5735118,
        "raw_content": null,
        "id": 185
    },
    {
        "url": "https://www.geeksforgeeks.org/deep-learning/vision-transformer-vit-architecture/",
        "title": "Vision Transformer (ViT) Architecture",
        "content": "Vision Transformer (ViT) is a deep learning architecture that applies the Transformer model to images. Instead of relying on convolutions,",
        "score": 0.78788257,
        "raw_content": null,
        "id": 186
    },
    {
        "url": "https://manimalakumar-29300.medium.com/generative-ai-model-architecture-part3-vision-transformer-363a0a81a7c2",
        "title": "Generative AI Model Architecture \u2014 Part3: Vision Transformer",
        "content": "# Generative AI Model Architecture \u2014 Part3: Vision Transformer. Vision Transformer (ViT) adapts the Transformer architecture \u2014 originally designed to process sequential data \u2014 to make sense of non-sequential data. There were specialized CNN model architectures for each of the fundamental vision tasks: image classification, object detection, segmentation, etc. ViTs develop a broad understanding of the image content, and overcome such limitations of CNNs. LVMs \u2014 the modern computer vision models \u2014 are increasingly built on ViT-based architectures. Since ViTs process image patches in parallel, position embeddings tell the model where each patch belongs. Self-distillation is a student-teacher architecture, where a student model learns to predict the output of a teacher model. This means the teacher is not a fixed, pre-trained model \u2014 over time, as the student improves, the teacher is updated to be a moving average of the student, ensuring the teacher remains a reliable target and promoting progressive learning. Part 4: Generative AI Model Architecture \u2014 Part4: Vision Language Model.",
        "score": 0.7602419,
        "raw_content": null,
        "id": 187
    },
    {
        "url": "https://blog.roboflow.com/vision-transformers/",
        "title": "Vision Transformers Explained: The Future of Computer ...",
        "content": "Transformers are a powerful deep learning architecture originally developed for natural language processing (NLP) tasks such as machine translation,",
        "score": 0.6659544,
        "raw_content": null,
        "id": 188
    },
    {
        "url": "https://www.codecademy.com/article/vision-transformers-working-architecture-explained",
        "title": "How do Vision Transformers Work? Architecture Explained",
        "content": "Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial. This architectural shift enables ViTs to use the powerful self-attention mechanism that made transformers successful in NLP, allowing them to model long-range dependencies and global relationships in images more effectively than CNNs. The ViT architecture consists of four main components: patch embedding, positional encoding, transformer encoder, and classification head. The original vision transformer architecture proposed in the paper \u201cAn Image Is Worth 16X16 Words\u201d contains different components such as patch embedding, positional encoding, transformer encoder, multilayer perceptron head, etc, and looks as follows:. * **Global feature modeling**: In vision transformers, every patch of an image interacts with every other patch during the self-attention mechanism. It\u2019s a self-supervised learning framework for vision transformers that allows a ViT model to learn robust and invariant features from images without needing human-labeled data. Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial.",
        "score": 0.61363083,
        "raw_content": null,
        "id": 189
    },
    {
        "url": "https://www.coursera.org/articles/transformers-vs-convolutional-neural-networks",
        "title": "Transformers vs. Convolutional Neural Networks",
        "content": "Transformers and convolutional neural networks are both powerful deep learning algorithms for computer vision, but they work differently and have different strengths and weaknesses. If you want to use a deep learning neural network to extract meaningful data from images, you may decide to use a convolutional neural network or a visual transformer. Explore transformers versus convolutional neural networks to learn how each of these models works, what you can use them for, and how they compare to each other. A transformer is a deep learning neural network that uses an encoder/decoder architecture to first break down the input into the base characteristics that define the data, then use its understanding of patterns to rebuild the data with original but similar information. Transformer models and convolutional neural networks are both deep learning models that you can use for computer vision, but they are different in how they empower computer vision.",
        "score": 0.5735118,
        "raw_content": null,
        "id": 190
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-34756-z",
        "title": "A multiscale transformer with spatial attention for ...",
        "content": "each tailored to extract valuable insights from the multi-dimensional HSI data. Among these approaches, the k-nearest neighbour18, 11514\u201311521. [...] designs and lightweight transformer modules to further reduce computational costs without compromising overall performance, making the framework even more suitable for HSI classification tasks. [...] To tackle these challenges, we propose MTSA-Net, a novel multiscale transformer framework with spatial attention for HSI classification. The model begins with a 3D convolution layer to extract shallow spectral\u2013spatial features, followed by a 2D convolution layer and a spatial attention module. This design reduces feature redundancy, mitigates inaccuracies that often arise in deeper networks, and emphasizes the most discriminative spatial features, thereby alleviating the limited spatial resolution of HSIs. The refined feature vectors are then processed by multiple parallel transformer encoder branches with varying hidden dimensions, enabling simultaneous modeling of fine-grained local details, intermediate relationships, and global representations. Finally, a multiscale feature fusion",
        "score": 0.98975134,
        "raw_content": null,
        "id": 191
    },
    {
        "url": "https://worldmodelbench.github.io/static/pdfs/7_Adaptive_Attention_Guided_Ma.pdf",
        "title": "Adaptive Attention-Guided Masking in Vision Transformers ...",
        "content": "Art. no.\n5511817. 2, 5, 7 Adaptive Attention-Guided Masking in Vision Transformers for Self-Supervised Hyperspectral Feature Learning Supplementary Material 6. Appendix In recent years, world models like Vision Transformers (ViTs)  have demonstrated impressive performance in various computer vision [6, 8, 13] tasks, including hyper-spectral image (HSI) classification . However, their ap-plication to HSI faces several limitations due to the unique challenges of hyperspectral data.\nChallenges. First, the high dimensionality of HSI, with several spectral bands, complicates processing for conven-tional ViTs, which are optimized for RGB images with only three channels. This necessitates extensive modifications to handle spectral complexity. [...] Hyperspectral image (HSI) classification has rapidly ad-vanced with diverse methods tackling spectral-spatial data challenges.\nEarly models like 2-DCNN  combined simple 2D convolutions with fully connected layers, while SPRN  improved spatial features using attention mech-anisms and residual blocks. 3-DCNN  introduced 3D convolutions to jointly capture spectral and spatial infor-mation.\nHybrid models like HybridSN  merged 2D and 3D CNNs for richer spectral-spatial representations.\nThe emergence of transformer-based approaches marked a pivotal shift, emphasizing efficient feature extraction and long-range dependency modeling. GAHT  and Mor-phFormer  leveraged CNN-transformer hybrids and self-attention for enhanced spectral-spatial learning. [...] 4. Analysis of Results In this section, we comprehensively analyze results in var-ious scenarios with three HSI datasets : Indian Pines, Salinas, and Botswana using the two performance metrics: Overall Accuracy (OA) and Cohen\u2019s Kappa coefficient (\u03ba).",
        "score": 0.98975134,
        "raw_content": null,
        "id": 192
    },
    {
        "url": "https://arxiv.org/html/2502.06894v1",
        "title": "AI-Driven HSI: Multimodality, Fusion, Challenges, and the ...",
        "content": "HSI, helping researchers select the right architecture for their problems. [...] intensity measurements are collected across spectral bands and represented as a 3D data cube. The observed intensity at wavelength for pixel can be expressed as: [...] TABLE IV: Deep Learning Models and their Applications in HSI and Multimodal HSI",
        "score": 0.98934746,
        "raw_content": null,
        "id": 193
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-12489-3",
        "title": "Spectral-spatial wave and frequency interactive ...",
        "content": "The proposed framework is designed for HSI. It utilizes a combination of spectral spatial feature extractor with frequency domain transformer encoder block to extract phase and frequency features. [...] Figure 1 depicts the proposed model, this section will elucidate the model\u2019s structure and it\u2019s functioning. The feature pre-processing stage begins by feeding HSI patches into 3D convolutional layer and 2D convolutional layer for low-level feature extraction after applying Feature Augmentation. Shallow feature maps generated by these layers are then processed using pixel operations to tokenize the features before passing them into the transformer encoder Blocks. The core of the model lies in the transformer Blocks, which are repeated N times to enable interactions between tokens representing different spectral spatial locations in the frequency domain. Finally, a classification head, consisting of a Global Average Pooling (GAP) layer is employed to assign a label to each pixel. [...] (2021).\") treated HSI patches as sequential data from the spectral dimension, utilizing group-spectral embedding and a transformer encoder module to capture locally detailed spectral representations. Roy et al.31, 7831\u20137843.",
        "score": 0.98282325,
        "raw_content": null,
        "id": 194
    },
    {
        "url": "https://arxiv.org/html/2508.08107v1",
        "title": "Hyperspectral Imaging",
        "content": "HSI focuses on the optical window of the electromagnetic spectrum (see Fig. 1A), typically covering wavelengths from 380 to 2500 nm. This window usually encompasses the visible light (400-700 nm), near-infrared (NIR), and shortwave infrared (SWIR) regions, as shown in Fig. 1B. Notably, the inclusion of the 380-400 nm region further enhances sensitivity to pigment absorption and subtle surface reflectance variations, particularly in the violet and near-ultraviolet range. An example scene illustrating typical hyperspectral data is shown in Fig. 1C, highlighting that HSI routinely captures over hundreds of spectral channels at high spectral resolution (commonly 5-10 nm). [...] Transform-based approaches reduce dimensionality by projecting the original data into a lower-dimensional subspace that preserves the most informative spectral or spatial-spectral structures. Among these, principal component analysis (PCA)  and minimum noise fraction (MNF)  are two of the most representative methods, often serving as baseline techniques for illustrating DR in HSI due to their easy-interpretable and friendly-practical linearized modeling. More specifically, let be the spectral vector at the -th pixel, where is the number of spectral bands and is the total number of pixels in the HSI image. PCA seeks an orthogonal transformation that maximizes the variance of the projected data:\n\n|  |  |  |\n --- \n|  |  | (5) |\n|  | [...] Table 1: Comparative summary of different HSI platforms.",
        "score": 0.9693242,
        "raw_content": null,
        "id": 195
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-33943-2",
        "title": "Multi-scale boundary-aware network for remote sensing ...",
        "content": "classification of high-resolution remote sensing images. By simultaneously leveraging the local features of CNN and the global dependencies of Transformer through multi-scale self-attention and bidirectional knowledge distillation, it improves the accuracy of remote sensing semantic segmentation. Fan et al.33:17.\") proposed a Multidimensional Information Fusion Network for high-resolution remote sensing image segmentation by integrating CNN and Transformer and introducing frequency information. By multi-scale integration of local details, global semantics and frequency domain features, the model\u2019s ability to identify fine-grained boundaries and scale-varying targets is enhanced, effectively alleviating complex interference factors such as inter-class ambiguity. The hybrid attention [...] Despite significant advances in remote sensing semantic segmentation achieved by existing CNNs, Transformers, and hybrid approaches, high-resolution imagery remains challenging: Insufficient multi-scale feature characterisation prevents the model from capturing boundary features, leading to blurred segmentation boundaries caused by classification errors at the junctures of different semantic categories. Furthermore, recent research on remote sensing image segmentation has underscored the importance of boundary-aware modelling in dense prediction tasks22.\"), 23.\"). These considerations have prompted us to develop a unified framework that synergistically enhances both multi-scale representation and boundary-aware capabilities. To address these challenges, we propose MSBANet\u2014a specially [...] Traditional CNNs have remarkable effects in extracting local spatial details, but their capabilities in modeling global contexts are limited. Although Transformer-based methods can capture long-range dependencies, they often ignore fine structures and boundary information and have high computational complexity. In semantic segmentation tasks, both global information and local information are crucial for accurately understanding the semantic structure of images. Researchers have begun to combine CNN with Transformer models to fully leverage their respective advantages. Chen et al. proposed CTSeg32.\"), a novel CNN and ViT collaborative segmentation framework following the encoder-decoder architecture for land-use/land-cover classification of high-resolution remote sensing images. By",
        "score": 0.38512084,
        "raw_content": null,
        "id": 196
    },
    {
        "url": "https://www.sciencedirect.com/science/article/abs/pii/S1350449525001173",
        "title": "Edge Feature Enhanced Transformer Network for RGB and ...",
        "content": "In real-world object detection scenarios, factors such as rain, fog, occlusion, varying illumination, and low resolution hinder the ability of visible light imaging to achieve high-precision target detection performance . In comparison to RGB single-modality based object detection, RGB-infrared multi-modality based object detection is able to leverage diverse and complementary information. As there are substantial visual disparities between the two modal images. For instance, RGB images are sensitive to color and texture, where objects exhibit clearer texture details compared to that in infrared images during the day while diminishing under nighttime conditions (see Fig. 1(a) and Fig. 1(b)). Instead, infrared (IR) images are sensitive for temperature variation, they enable to offer [...] images achieve high- precision detection and recognition by extracting local spatial\u2013temporal features. However, convolutional operations is not capable for global information perception and struggle to model dependencies between features. Therefore, there is still potential to improve the fusion quality by taking consideration of the global feature learning. [...] In multispectral object detection, the complementary nature of infrared and visible images remains underexploited due to perceptual differences and spatial misalignment between modalities, posing significant challenges to accurate detection. We propose MRT-DETR (Multispectral RT-DETR), an end-to-end multispectral real-time detection framework that addresses weak alignment and complex lighting conditions by combining brightness-aware weighting with deformable alignment. Built on the RT-DETR backbone, our method introduces an Early-stage Deformable Alignment (EDA) module that learns attention-guided offsets at shallow layers to explicitly align infrared features to visible features in the spatial domain. Additionally, a Dual-Branch Brightness Weighting (DBW) module derives patch-wise",
        "score": 0.3059777,
        "raw_content": null,
        "id": 197
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-34756-z",
        "title": "A multiscale transformer with spatial attention for ...",
        "content": "Article  ADS   Google Scholar\n7. Hong, D. et al. Spectralformer: Rethinking hyperspectral image classification with transformers. IEEE Transactions on Geoscience and Remote Sensing 60, 1\u201315 (2021).\n\n   Article   Google Scholar\n8. Zhou, P., Han, J., Cheng, G. & Zhang, B. Learning compact and discriminative stacked autoencoder for hyperspectral image classification. IEEE Transactions on Geoscience and Remote Sensing 57(7), 4823\u20134833 (2019).\n\n   Article  ADS   Google Scholar\n9. Zheng, X., Yuan, Y. & Lu, X. Dimensionality reduction by spatial-spectral preservation in selected bands. IEEE Transactions on Geoscience and Remote Sensing 55(9), 5185\u20135197 (2017). [...] The Houston 2013 (H-13) dataset was gathered using the ITRES CASI-1500 sensor and covers the University of Houston premises and the neighbouring rural regions. This dataset has been publicly employed for assessing the effectiveness of HSI classification methods. This dataset has an image size of \\(349 \\times 1905\\) pixels and includes 144 spectral bands. It encompasses 15 challenging distinct classes, making it a valuable resource for land cover classification studies. The dataset samples have been divided into training, validation, and test sets, with the allocation details provided in Table 4. [...] Furthermore, a hybrid approach incorporating both CNN and Transformer networks48.\") was proposed to effectively calculate spectral-spatial and semantic features for HSI classification. In66.\"), Swin Transformer was presented, which calculates representations through a shifted window strategy. This approach allows for efficient processing of visual data with diverse scales and higher resolutions. Adopting this strategy effectively addresses the challenges presented by scale and resolution disparities, rendering it well-suited for HSI classification.\n\n## Proposed methodology",
        "score": 0.2640859,
        "raw_content": null,
        "id": 198
    },
    {
        "url": "https://arxiv.org/html/2507.22791v1",
        "title": "Modality-Aware Feature Matching: A Comprehensive ...",
        "content": "In this survey, our discussion addresses unique challenges and methodologies related to single-modal feature matching (e.g., RGB, depth, medical imaging) and cross-modal scenarios (e.g., medical image registration, vision-language integration). We delineate the progression from classical detector-based pipelines towards contemporary detector-free solutions. Figure 1 illustrates the overall pipeline of the survey, clearly mapping the evolution of feature matching methodologies across various data modalities. Furthermore, Figure 2 offers representative examples of the results of the matching of modality-awarecharacteristics.\n\n## 2. RGB Images [...] In summary, deep learning-based methods have greatly advanced the state of RGB image matching. They have produced more repeatable detectors (learning where to detect points that survive viewpoint/illumination changes), more discriminative descriptors (learning how to describe patches for higher distinctiveness), and even entirely new paradigms for matching (learning to match with attention mechanisms, and end-to-end trainable pipelines). Table 1 highlights some of the influential methods from both the handcrafted and deep learning eras. These modern techniques, when evaluated on challenging benchmarks, significantly outperform classic methods in terms of matching precision and robustness, although often at a higher computational cost. Nonetheless, due to their task-specific optimizations [...] ## 2. RGB Images\n\nFeature matching in RGB images traditionally follows a pipeline of feature detection, feature description, and feature matching. Over the decades, a lot of methods have been proposed for each stage of this pipeline, from handcrafted approaches to more recent deep learning-based techniques.\n\n### 2.1. Handcrafted Feature Detectors and Descriptors",
        "score": 0.26285648,
        "raw_content": null,
        "id": 199
    },
    {
        "url": "https://link.springer.com/article/10.1007/s10462-025-11169-y",
        "title": "A review of hyperspectral image classification based on graph ...",
        "content": "\")) of hyperspectral imaging (HSI) pose significant challenges to achieving accurate HSIC (Sawant and Prabukumar 2020 A survey of band selection techniques for hyperspectral image classification. J Spectr Imaging 9(1):5. \n                  \n                  \n                \")). Specifically, the large number of spectral bands and data points increases computational complexity (Ullah et al. 2024 Conventional to deep ensemble methods for hyperspectral image classification: a comprehensive survey. IEEE J Sel Top Appl Earth Obs Remote Sens. [...] ### 2.2 Technologies for hyperspectral image classification\n\nOptical remote sensing images can be categorized into three main types based on the number of spectral bands: panchromatic images with a single band, multiband RGB images, and hyperspectral images (HSI) containing hundreds of continuous, narrow spectral bands (Mather and Tso 2016 Classification methods for remotely sensed data. CRC Press, Boca Raton\")). HSI, with its unique technical advantages, achieves a deep integration of imaging and spectral technologies, enabling comprehensive data acquisition in both two-dimensional spatial geometry and one-dimensional spectral information. [...] Although deep learning has achieved remarkable breakthroughs in hyperspectral image classification (HSIC), the unique characteristics of hyperspectral image (HSI) data still present numerous challenges (Wan et al. 2020a Hyperspectral image classification with context-aware dynamic graph convolutional network. IEEE Trans Geosci Remote Sens 59(1):597\u2013612.",
        "score": 0.21413255,
        "raw_content": null,
        "id": 200
    },
    {
        "url": "https://www.geeksforgeeks.org/deep-learning/vision-transformer-vit-architecture/",
        "title": "Vision Transformer (ViT) Architecture",
        "content": "Vision Transformer (ViT) is a deep learning architecture that applies the Transformer model to images. Instead of relying on convolutions,",
        "score": 0.78788257,
        "raw_content": null,
        "id": 201
    },
    {
        "url": "https://manimalakumar-29300.medium.com/generative-ai-model-architecture-part3-vision-transformer-363a0a81a7c2",
        "title": "Generative AI Model Architecture \u2014 Part3: Vision Transformer",
        "content": "# Generative AI Model Architecture \u2014 Part3: Vision Transformer. Vision Transformer (ViT) adapts the Transformer architecture \u2014 originally designed to process sequential data \u2014 to make sense of non-sequential data. There were specialized CNN model architectures for each of the fundamental vision tasks: image classification, object detection, segmentation, etc. ViTs develop a broad understanding of the image content, and overcome such limitations of CNNs. LVMs \u2014 the modern computer vision models \u2014 are increasingly built on ViT-based architectures. Since ViTs process image patches in parallel, position embeddings tell the model where each patch belongs. Self-distillation is a student-teacher architecture, where a student model learns to predict the output of a teacher model. This means the teacher is not a fixed, pre-trained model \u2014 over time, as the student improves, the teacher is updated to be a moving average of the student, ensuring the teacher remains a reliable target and promoting progressive learning. Part 4: Generative AI Model Architecture \u2014 Part4: Vision Language Model.",
        "score": 0.7602419,
        "raw_content": null,
        "id": 202
    },
    {
        "url": "https://blog.roboflow.com/vision-transformers/",
        "title": "Vision Transformers Explained: The Future of Computer ...",
        "content": "Transformers are a powerful deep learning architecture originally developed for natural language processing (NLP) tasks such as machine translation,",
        "score": 0.6659544,
        "raw_content": null,
        "id": 203
    },
    {
        "url": "https://www.codecademy.com/article/vision-transformers-working-architecture-explained",
        "title": "How do Vision Transformers Work? Architecture Explained",
        "content": "Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial. This architectural shift enables ViTs to use the powerful self-attention mechanism that made transformers successful in NLP, allowing them to model long-range dependencies and global relationships in images more effectively than CNNs. The ViT architecture consists of four main components: patch embedding, positional encoding, transformer encoder, and classification head. The original vision transformer architecture proposed in the paper \u201cAn Image Is Worth 16X16 Words\u201d contains different components such as patch embedding, positional encoding, transformer encoder, multilayer perceptron head, etc, and looks as follows:. * **Global feature modeling**: In vision transformers, every patch of an image interacts with every other patch during the self-attention mechanism. It\u2019s a self-supervised learning framework for vision transformers that allows a ViT model to learn robust and invariant features from images without needing human-labeled data. Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial.",
        "score": 0.61363083,
        "raw_content": null,
        "id": 204
    },
    {
        "url": "https://www.coursera.org/articles/transformers-vs-convolutional-neural-networks",
        "title": "Transformers vs. Convolutional Neural Networks",
        "content": "Transformers and convolutional neural networks are both powerful deep learning algorithms for computer vision, but they work differently and have different strengths and weaknesses. If you want to use a deep learning neural network to extract meaningful data from images, you may decide to use a convolutional neural network or a visual transformer. Explore transformers versus convolutional neural networks to learn how each of these models works, what you can use them for, and how they compare to each other. A transformer is a deep learning neural network that uses an encoder/decoder architecture to first break down the input into the base characteristics that define the data, then use its understanding of patterns to rebuild the data with original but similar information. Transformer models and convolutional neural networks are both deep learning models that you can use for computer vision, but they are different in how they empower computer vision.",
        "score": 0.5735118,
        "raw_content": null,
        "id": 205
    },
    {
        "url": "https://www.geeksforgeeks.org/deep-learning/vision-transformer-vit-architecture/",
        "title": "Vision Transformer (ViT) Architecture",
        "content": "Vision Transformer (ViT) is a deep learning architecture that applies the Transformer model to images. Instead of relying on convolutions,",
        "score": 0.78788257,
        "raw_content": null,
        "id": 206
    },
    {
        "url": "https://manimalakumar-29300.medium.com/generative-ai-model-architecture-part3-vision-transformer-363a0a81a7c2",
        "title": "Generative AI Model Architecture \u2014 Part3: Vision Transformer",
        "content": "# Generative AI Model Architecture \u2014 Part3: Vision Transformer. Vision Transformer (ViT) adapts the Transformer architecture \u2014 originally designed to process sequential data \u2014 to make sense of non-sequential data. There were specialized CNN model architectures for each of the fundamental vision tasks: image classification, object detection, segmentation, etc. ViTs develop a broad understanding of the image content, and overcome such limitations of CNNs. LVMs \u2014 the modern computer vision models \u2014 are increasingly built on ViT-based architectures. Since ViTs process image patches in parallel, position embeddings tell the model where each patch belongs. Self-distillation is a student-teacher architecture, where a student model learns to predict the output of a teacher model. This means the teacher is not a fixed, pre-trained model \u2014 over time, as the student improves, the teacher is updated to be a moving average of the student, ensuring the teacher remains a reliable target and promoting progressive learning. Part 4: Generative AI Model Architecture \u2014 Part4: Vision Language Model.",
        "score": 0.7602419,
        "raw_content": null,
        "id": 207
    },
    {
        "url": "https://blog.roboflow.com/vision-transformers/",
        "title": "Vision Transformers Explained: The Future of Computer ...",
        "content": "Transformers are a powerful deep learning architecture originally developed for natural language processing (NLP) tasks such as machine translation,",
        "score": 0.6659544,
        "raw_content": null,
        "id": 208
    },
    {
        "url": "https://www.codecademy.com/article/vision-transformers-working-architecture-explained",
        "title": "How do Vision Transformers Work? Architecture Explained",
        "content": "Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial. This architectural shift enables ViTs to use the powerful self-attention mechanism that made transformers successful in NLP, allowing them to model long-range dependencies and global relationships in images more effectively than CNNs. The ViT architecture consists of four main components: patch embedding, positional encoding, transformer encoder, and classification head. The original vision transformer architecture proposed in the paper \u201cAn Image Is Worth 16X16 Words\u201d contains different components such as patch embedding, positional encoding, transformer encoder, multilayer perceptron head, etc, and looks as follows:. * **Global feature modeling**: In vision transformers, every patch of an image interacts with every other patch during the self-attention mechanism. It\u2019s a self-supervised learning framework for vision transformers that allows a ViT model to learn robust and invariant features from images without needing human-labeled data. Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial.",
        "score": 0.61363083,
        "raw_content": null,
        "id": 209
    },
    {
        "url": "https://www.coursera.org/articles/transformers-vs-convolutional-neural-networks",
        "title": "Transformers vs. Convolutional Neural Networks",
        "content": "Transformers and convolutional neural networks are both powerful deep learning algorithms for computer vision, but they work differently and have different strengths and weaknesses. If you want to use a deep learning neural network to extract meaningful data from images, you may decide to use a convolutional neural network or a visual transformer. Explore transformers versus convolutional neural networks to learn how each of these models works, what you can use them for, and how they compare to each other. A transformer is a deep learning neural network that uses an encoder/decoder architecture to first break down the input into the base characteristics that define the data, then use its understanding of patterns to rebuild the data with original but similar information. Transformer models and convolutional neural networks are both deep learning models that you can use for computer vision, but they are different in how they empower computer vision.",
        "score": 0.5735118,
        "raw_content": null,
        "id": 210
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-34756-z",
        "title": "A multiscale transformer with spatial attention for ...",
        "content": "each tailored to extract valuable insights from the multi-dimensional HSI data. Among these approaches, the k-nearest neighbour18, 11514\u201311521. [...] designs and lightweight transformer modules to further reduce computational costs without compromising overall performance, making the framework even more suitable for HSI classification tasks. [...] To tackle these challenges, we propose MTSA-Net, a novel multiscale transformer framework with spatial attention for HSI classification. The model begins with a 3D convolution layer to extract shallow spectral\u2013spatial features, followed by a 2D convolution layer and a spatial attention module. This design reduces feature redundancy, mitigates inaccuracies that often arise in deeper networks, and emphasizes the most discriminative spatial features, thereby alleviating the limited spatial resolution of HSIs. The refined feature vectors are then processed by multiple parallel transformer encoder branches with varying hidden dimensions, enabling simultaneous modeling of fine-grained local details, intermediate relationships, and global representations. Finally, a multiscale feature fusion",
        "score": 0.98975134,
        "raw_content": null,
        "id": 211
    },
    {
        "url": "https://worldmodelbench.github.io/static/pdfs/7_Adaptive_Attention_Guided_Ma.pdf",
        "title": "Adaptive Attention-Guided Masking in Vision Transformers ...",
        "content": "Art. no.\n5511817. 2, 5, 7 Adaptive Attention-Guided Masking in Vision Transformers for Self-Supervised Hyperspectral Feature Learning Supplementary Material 6. Appendix In recent years, world models like Vision Transformers (ViTs)  have demonstrated impressive performance in various computer vision [6, 8, 13] tasks, including hyper-spectral image (HSI) classification . However, their ap-plication to HSI faces several limitations due to the unique challenges of hyperspectral data.\nChallenges. First, the high dimensionality of HSI, with several spectral bands, complicates processing for conven-tional ViTs, which are optimized for RGB images with only three channels. This necessitates extensive modifications to handle spectral complexity. [...] Hyperspectral image (HSI) classification has rapidly ad-vanced with diverse methods tackling spectral-spatial data challenges.\nEarly models like 2-DCNN  combined simple 2D convolutions with fully connected layers, while SPRN  improved spatial features using attention mech-anisms and residual blocks. 3-DCNN  introduced 3D convolutions to jointly capture spectral and spatial infor-mation.\nHybrid models like HybridSN  merged 2D and 3D CNNs for richer spectral-spatial representations.\nThe emergence of transformer-based approaches marked a pivotal shift, emphasizing efficient feature extraction and long-range dependency modeling. GAHT  and Mor-phFormer  leveraged CNN-transformer hybrids and self-attention for enhanced spectral-spatial learning. [...] 4. Analysis of Results In this section, we comprehensively analyze results in var-ious scenarios with three HSI datasets : Indian Pines, Salinas, and Botswana using the two performance metrics: Overall Accuracy (OA) and Cohen\u2019s Kappa coefficient (\u03ba).",
        "score": 0.98975134,
        "raw_content": null,
        "id": 212
    },
    {
        "url": "https://arxiv.org/html/2502.06894v1",
        "title": "AI-Driven HSI: Multimodality, Fusion, Challenges, and the ...",
        "content": "HSI, helping researchers select the right architecture for their problems. [...] intensity measurements are collected across spectral bands and represented as a 3D data cube. The observed intensity at wavelength for pixel can be expressed as: [...] TABLE IV: Deep Learning Models and their Applications in HSI and Multimodal HSI",
        "score": 0.98934746,
        "raw_content": null,
        "id": 213
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-12489-3",
        "title": "Spectral-spatial wave and frequency interactive ...",
        "content": "The proposed framework is designed for HSI. It utilizes a combination of spectral spatial feature extractor with frequency domain transformer encoder block to extract phase and frequency features. [...] Figure 1 depicts the proposed model, this section will elucidate the model\u2019s structure and it\u2019s functioning. The feature pre-processing stage begins by feeding HSI patches into 3D convolutional layer and 2D convolutional layer for low-level feature extraction after applying Feature Augmentation. Shallow feature maps generated by these layers are then processed using pixel operations to tokenize the features before passing them into the transformer encoder Blocks. The core of the model lies in the transformer Blocks, which are repeated N times to enable interactions between tokens representing different spectral spatial locations in the frequency domain. Finally, a classification head, consisting of a Global Average Pooling (GAP) layer is employed to assign a label to each pixel. [...] (2021).\") treated HSI patches as sequential data from the spectral dimension, utilizing group-spectral embedding and a transformer encoder module to capture locally detailed spectral representations. Roy et al.31, 7831\u20137843.",
        "score": 0.98282325,
        "raw_content": null,
        "id": 214
    },
    {
        "url": "https://arxiv.org/html/2508.08107v1",
        "title": "Hyperspectral Imaging",
        "content": "HSI focuses on the optical window of the electromagnetic spectrum (see Fig. 1A), typically covering wavelengths from 380 to 2500 nm. This window usually encompasses the visible light (400-700 nm), near-infrared (NIR), and shortwave infrared (SWIR) regions, as shown in Fig. 1B. Notably, the inclusion of the 380-400 nm region further enhances sensitivity to pigment absorption and subtle surface reflectance variations, particularly in the violet and near-ultraviolet range. An example scene illustrating typical hyperspectral data is shown in Fig. 1C, highlighting that HSI routinely captures over hundreds of spectral channels at high spectral resolution (commonly 5-10 nm). [...] Transform-based approaches reduce dimensionality by projecting the original data into a lower-dimensional subspace that preserves the most informative spectral or spatial-spectral structures. Among these, principal component analysis (PCA)  and minimum noise fraction (MNF)  are two of the most representative methods, often serving as baseline techniques for illustrating DR in HSI due to their easy-interpretable and friendly-practical linearized modeling. More specifically, let be the spectral vector at the -th pixel, where is the number of spectral bands and is the total number of pixels in the HSI image. PCA seeks an orthogonal transformation that maximizes the variance of the projected data:\n\n|  |  |  |\n --- \n|  |  | (5) |\n|  | [...] Table 1: Comparative summary of different HSI platforms.",
        "score": 0.9693242,
        "raw_content": null,
        "id": 215
    },
    {
        "url": "https://www.geeksforgeeks.org/deep-learning/vision-transformer-vit-architecture/",
        "title": "Vision Transformer (ViT) Architecture",
        "content": "Vision Transformer (ViT) is a deep learning architecture that applies the Transformer model to images. Instead of relying on convolutions,",
        "score": 0.78788257,
        "raw_content": null,
        "id": 216
    },
    {
        "url": "https://manimalakumar-29300.medium.com/generative-ai-model-architecture-part3-vision-transformer-363a0a81a7c2",
        "title": "Generative AI Model Architecture \u2014 Part3: Vision Transformer",
        "content": "# Generative AI Model Architecture \u2014 Part3: Vision Transformer. Vision Transformer (ViT) adapts the Transformer architecture \u2014 originally designed to process sequential data \u2014 to make sense of non-sequential data. There were specialized CNN model architectures for each of the fundamental vision tasks: image classification, object detection, segmentation, etc. ViTs develop a broad understanding of the image content, and overcome such limitations of CNNs. LVMs \u2014 the modern computer vision models \u2014 are increasingly built on ViT-based architectures. Since ViTs process image patches in parallel, position embeddings tell the model where each patch belongs. Self-distillation is a student-teacher architecture, where a student model learns to predict the output of a teacher model. This means the teacher is not a fixed, pre-trained model \u2014 over time, as the student improves, the teacher is updated to be a moving average of the student, ensuring the teacher remains a reliable target and promoting progressive learning. Part 4: Generative AI Model Architecture \u2014 Part4: Vision Language Model.",
        "score": 0.7602419,
        "raw_content": null,
        "id": 217
    },
    {
        "url": "https://blog.roboflow.com/vision-transformers/",
        "title": "Vision Transformers Explained: The Future of Computer ...",
        "content": "Transformers are a powerful deep learning architecture originally developed for natural language processing (NLP) tasks such as machine translation,",
        "score": 0.6659544,
        "raw_content": null,
        "id": 218
    },
    {
        "url": "https://www.codecademy.com/article/vision-transformers-working-architecture-explained",
        "title": "How do Vision Transformers Work? Architecture Explained",
        "content": "Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial. This architectural shift enables ViTs to use the powerful self-attention mechanism that made transformers successful in NLP, allowing them to model long-range dependencies and global relationships in images more effectively than CNNs. The ViT architecture consists of four main components: patch embedding, positional encoding, transformer encoder, and classification head. The original vision transformer architecture proposed in the paper \u201cAn Image Is Worth 16X16 Words\u201d contains different components such as patch embedding, positional encoding, transformer encoder, multilayer perceptron head, etc, and looks as follows:. * **Global feature modeling**: In vision transformers, every patch of an image interacts with every other patch during the self-attention mechanism. It\u2019s a self-supervised learning framework for vision transformers that allows a ViT model to learn robust and invariant features from images without needing human-labeled data. Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial.",
        "score": 0.61363083,
        "raw_content": null,
        "id": 219
    },
    {
        "url": "https://www.coursera.org/articles/transformers-vs-convolutional-neural-networks",
        "title": "Transformers vs. Convolutional Neural Networks",
        "content": "Transformers and convolutional neural networks are both powerful deep learning algorithms for computer vision, but they work differently and have different strengths and weaknesses. If you want to use a deep learning neural network to extract meaningful data from images, you may decide to use a convolutional neural network or a visual transformer. Explore transformers versus convolutional neural networks to learn how each of these models works, what you can use them for, and how they compare to each other. A transformer is a deep learning neural network that uses an encoder/decoder architecture to first break down the input into the base characteristics that define the data, then use its understanding of patterns to rebuild the data with original but similar information. Transformer models and convolutional neural networks are both deep learning models that you can use for computer vision, but they are different in how they empower computer vision.",
        "score": 0.5735118,
        "raw_content": null,
        "id": 220
    },
    {
        "url": "https://www.geeksforgeeks.org/deep-learning/vision-transformer-vit-architecture/",
        "title": "Vision Transformer (ViT) Architecture",
        "content": "Vision Transformer (ViT) is a deep learning architecture that applies the Transformer model to images. Instead of relying on convolutions,",
        "score": 0.78788257,
        "raw_content": null,
        "id": 221
    },
    {
        "url": "https://manimalakumar-29300.medium.com/generative-ai-model-architecture-part3-vision-transformer-363a0a81a7c2",
        "title": "Generative AI Model Architecture \u2014 Part3: Vision Transformer",
        "content": "# Generative AI Model Architecture \u2014 Part3: Vision Transformer. Vision Transformer (ViT) adapts the Transformer architecture \u2014 originally designed to process sequential data \u2014 to make sense of non-sequential data. There were specialized CNN model architectures for each of the fundamental vision tasks: image classification, object detection, segmentation, etc. ViTs develop a broad understanding of the image content, and overcome such limitations of CNNs. LVMs \u2014 the modern computer vision models \u2014 are increasingly built on ViT-based architectures. Since ViTs process image patches in parallel, position embeddings tell the model where each patch belongs. Self-distillation is a student-teacher architecture, where a student model learns to predict the output of a teacher model. This means the teacher is not a fixed, pre-trained model \u2014 over time, as the student improves, the teacher is updated to be a moving average of the student, ensuring the teacher remains a reliable target and promoting progressive learning. Part 4: Generative AI Model Architecture \u2014 Part4: Vision Language Model.",
        "score": 0.7602419,
        "raw_content": null,
        "id": 222
    },
    {
        "url": "https://blog.roboflow.com/vision-transformers/",
        "title": "Vision Transformers Explained: The Future of Computer ...",
        "content": "Transformers are a powerful deep learning architecture originally developed for natural language processing (NLP) tasks such as machine translation,",
        "score": 0.6659544,
        "raw_content": null,
        "id": 223
    },
    {
        "url": "https://www.codecademy.com/article/vision-transformers-working-architecture-explained",
        "title": "How do Vision Transformers Work? Architecture Explained",
        "content": "Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial. This architectural shift enables ViTs to use the powerful self-attention mechanism that made transformers successful in NLP, allowing them to model long-range dependencies and global relationships in images more effectively than CNNs. The ViT architecture consists of four main components: patch embedding, positional encoding, transformer encoder, and classification head. The original vision transformer architecture proposed in the paper \u201cAn Image Is Worth 16X16 Words\u201d contains different components such as patch embedding, positional encoding, transformer encoder, multilayer perceptron head, etc, and looks as follows:. * **Global feature modeling**: In vision transformers, every patch of an image interacts with every other patch during the self-attention mechanism. It\u2019s a self-supervised learning framework for vision transformers that allows a ViT model to learn robust and invariant features from images without needing human-labeled data. Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial.",
        "score": 0.61363083,
        "raw_content": null,
        "id": 224
    },
    {
        "url": "https://www.coursera.org/articles/transformers-vs-convolutional-neural-networks",
        "title": "Transformers vs. Convolutional Neural Networks",
        "content": "Transformers and convolutional neural networks are both powerful deep learning algorithms for computer vision, but they work differently and have different strengths and weaknesses. If you want to use a deep learning neural network to extract meaningful data from images, you may decide to use a convolutional neural network or a visual transformer. Explore transformers versus convolutional neural networks to learn how each of these models works, what you can use them for, and how they compare to each other. A transformer is a deep learning neural network that uses an encoder/decoder architecture to first break down the input into the base characteristics that define the data, then use its understanding of patterns to rebuild the data with original but similar information. Transformer models and convolutional neural networks are both deep learning models that you can use for computer vision, but they are different in how they empower computer vision.",
        "score": 0.5735118,
        "raw_content": null,
        "id": 225
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-34756-z",
        "title": "A multiscale transformer with spatial attention for ...",
        "content": "each tailored to extract valuable insights from the multi-dimensional HSI data. Among these approaches, the k-nearest neighbour18, 11514\u201311521. [...] designs and lightweight transformer modules to further reduce computational costs without compromising overall performance, making the framework even more suitable for HSI classification tasks. [...] To tackle these challenges, we propose MTSA-Net, a novel multiscale transformer framework with spatial attention for HSI classification. The model begins with a 3D convolution layer to extract shallow spectral\u2013spatial features, followed by a 2D convolution layer and a spatial attention module. This design reduces feature redundancy, mitigates inaccuracies that often arise in deeper networks, and emphasizes the most discriminative spatial features, thereby alleviating the limited spatial resolution of HSIs. The refined feature vectors are then processed by multiple parallel transformer encoder branches with varying hidden dimensions, enabling simultaneous modeling of fine-grained local details, intermediate relationships, and global representations. Finally, a multiscale feature fusion",
        "score": 0.98975134,
        "raw_content": null,
        "id": 226
    },
    {
        "url": "https://worldmodelbench.github.io/static/pdfs/7_Adaptive_Attention_Guided_Ma.pdf",
        "title": "Adaptive Attention-Guided Masking in Vision Transformers ...",
        "content": "Art. no.\n5511817. 2, 5, 7 Adaptive Attention-Guided Masking in Vision Transformers for Self-Supervised Hyperspectral Feature Learning Supplementary Material 6. Appendix In recent years, world models like Vision Transformers (ViTs)  have demonstrated impressive performance in various computer vision [6, 8, 13] tasks, including hyper-spectral image (HSI) classification . However, their ap-plication to HSI faces several limitations due to the unique challenges of hyperspectral data.\nChallenges. First, the high dimensionality of HSI, with several spectral bands, complicates processing for conven-tional ViTs, which are optimized for RGB images with only three channels. This necessitates extensive modifications to handle spectral complexity. [...] Hyperspectral image (HSI) classification has rapidly ad-vanced with diverse methods tackling spectral-spatial data challenges.\nEarly models like 2-DCNN  combined simple 2D convolutions with fully connected layers, while SPRN  improved spatial features using attention mech-anisms and residual blocks. 3-DCNN  introduced 3D convolutions to jointly capture spectral and spatial infor-mation.\nHybrid models like HybridSN  merged 2D and 3D CNNs for richer spectral-spatial representations.\nThe emergence of transformer-based approaches marked a pivotal shift, emphasizing efficient feature extraction and long-range dependency modeling. GAHT  and Mor-phFormer  leveraged CNN-transformer hybrids and self-attention for enhanced spectral-spatial learning. [...] 4. Analysis of Results In this section, we comprehensively analyze results in var-ious scenarios with three HSI datasets : Indian Pines, Salinas, and Botswana using the two performance metrics: Overall Accuracy (OA) and Cohen\u2019s Kappa coefficient (\u03ba).",
        "score": 0.98975134,
        "raw_content": null,
        "id": 227
    },
    {
        "url": "https://arxiv.org/html/2502.06894v1",
        "title": "AI-Driven HSI: Multimodality, Fusion, Challenges, and the ...",
        "content": "HSI, helping researchers select the right architecture for their problems. [...] intensity measurements are collected across spectral bands and represented as a 3D data cube. The observed intensity at wavelength for pixel can be expressed as: [...] TABLE IV: Deep Learning Models and their Applications in HSI and Multimodal HSI",
        "score": 0.98934746,
        "raw_content": null,
        "id": 228
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-12489-3",
        "title": "Spectral-spatial wave and frequency interactive ...",
        "content": "The proposed framework is designed for HSI. It utilizes a combination of spectral spatial feature extractor with frequency domain transformer encoder block to extract phase and frequency features. [...] Figure 1 depicts the proposed model, this section will elucidate the model\u2019s structure and it\u2019s functioning. The feature pre-processing stage begins by feeding HSI patches into 3D convolutional layer and 2D convolutional layer for low-level feature extraction after applying Feature Augmentation. Shallow feature maps generated by these layers are then processed using pixel operations to tokenize the features before passing them into the transformer encoder Blocks. The core of the model lies in the transformer Blocks, which are repeated N times to enable interactions between tokens representing different spectral spatial locations in the frequency domain. Finally, a classification head, consisting of a Global Average Pooling (GAP) layer is employed to assign a label to each pixel. [...] (2021).\") treated HSI patches as sequential data from the spectral dimension, utilizing group-spectral embedding and a transformer encoder module to capture locally detailed spectral representations. Roy et al.31, 7831\u20137843.",
        "score": 0.98282325,
        "raw_content": null,
        "id": 229
    },
    {
        "url": "https://arxiv.org/html/2508.08107v1",
        "title": "Hyperspectral Imaging",
        "content": "HSI focuses on the optical window of the electromagnetic spectrum (see Fig. 1A), typically covering wavelengths from 380 to 2500 nm. This window usually encompasses the visible light (400-700 nm), near-infrared (NIR), and shortwave infrared (SWIR) regions, as shown in Fig. 1B. Notably, the inclusion of the 380-400 nm region further enhances sensitivity to pigment absorption and subtle surface reflectance variations, particularly in the violet and near-ultraviolet range. An example scene illustrating typical hyperspectral data is shown in Fig. 1C, highlighting that HSI routinely captures over hundreds of spectral channels at high spectral resolution (commonly 5-10 nm). [...] Transform-based approaches reduce dimensionality by projecting the original data into a lower-dimensional subspace that preserves the most informative spectral or spatial-spectral structures. Among these, principal component analysis (PCA)  and minimum noise fraction (MNF)  are two of the most representative methods, often serving as baseline techniques for illustrating DR in HSI due to their easy-interpretable and friendly-practical linearized modeling. More specifically, let be the spectral vector at the -th pixel, where is the number of spectral bands and is the total number of pixels in the HSI image. PCA seeks an orthogonal transformation that maximizes the variance of the projected data:\n\n|  |  |  |\n --- \n|  |  | (5) |\n|  | [...] Table 1: Comparative summary of different HSI platforms.",
        "score": 0.9693242,
        "raw_content": null,
        "id": 230
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-33943-2",
        "title": "Multi-scale boundary-aware network for remote sensing ...",
        "content": "classification of high-resolution remote sensing images. By simultaneously leveraging the local features of CNN and the global dependencies of Transformer through multi-scale self-attention and bidirectional knowledge distillation, it improves the accuracy of remote sensing semantic segmentation. Fan et al.33:17.\") proposed a Multidimensional Information Fusion Network for high-resolution remote sensing image segmentation by integrating CNN and Transformer and introducing frequency information. By multi-scale integration of local details, global semantics and frequency domain features, the model\u2019s ability to identify fine-grained boundaries and scale-varying targets is enhanced, effectively alleviating complex interference factors such as inter-class ambiguity. The hybrid attention [...] Despite significant advances in remote sensing semantic segmentation achieved by existing CNNs, Transformers, and hybrid approaches, high-resolution imagery remains challenging: Insufficient multi-scale feature characterisation prevents the model from capturing boundary features, leading to blurred segmentation boundaries caused by classification errors at the junctures of different semantic categories. Furthermore, recent research on remote sensing image segmentation has underscored the importance of boundary-aware modelling in dense prediction tasks22.\"), 23.\"). These considerations have prompted us to develop a unified framework that synergistically enhances both multi-scale representation and boundary-aware capabilities. To address these challenges, we propose MSBANet\u2014a specially [...] Traditional CNNs have remarkable effects in extracting local spatial details, but their capabilities in modeling global contexts are limited. Although Transformer-based methods can capture long-range dependencies, they often ignore fine structures and boundary information and have high computational complexity. In semantic segmentation tasks, both global information and local information are crucial for accurately understanding the semantic structure of images. Researchers have begun to combine CNN with Transformer models to fully leverage their respective advantages. Chen et al. proposed CTSeg32.\"), a novel CNN and ViT collaborative segmentation framework following the encoder-decoder architecture for land-use/land-cover classification of high-resolution remote sensing images. By",
        "score": 0.38512084,
        "raw_content": null,
        "id": 231
    },
    {
        "url": "https://www.sciencedirect.com/science/article/abs/pii/S1350449525001173",
        "title": "Edge Feature Enhanced Transformer Network for RGB and ...",
        "content": "In real-world object detection scenarios, factors such as rain, fog, occlusion, varying illumination, and low resolution hinder the ability of visible light imaging to achieve high-precision target detection performance . In comparison to RGB single-modality based object detection, RGB-infrared multi-modality based object detection is able to leverage diverse and complementary information. As there are substantial visual disparities between the two modal images. For instance, RGB images are sensitive to color and texture, where objects exhibit clearer texture details compared to that in infrared images during the day while diminishing under nighttime conditions (see Fig. 1(a) and Fig. 1(b)). Instead, infrared (IR) images are sensitive for temperature variation, they enable to offer [...] images achieve high- precision detection and recognition by extracting local spatial\u2013temporal features. However, convolutional operations is not capable for global information perception and struggle to model dependencies between features. Therefore, there is still potential to improve the fusion quality by taking consideration of the global feature learning. [...] In multispectral object detection, the complementary nature of infrared and visible images remains underexploited due to perceptual differences and spatial misalignment between modalities, posing significant challenges to accurate detection. We propose MRT-DETR (Multispectral RT-DETR), an end-to-end multispectral real-time detection framework that addresses weak alignment and complex lighting conditions by combining brightness-aware weighting with deformable alignment. Built on the RT-DETR backbone, our method introduces an Early-stage Deformable Alignment (EDA) module that learns attention-guided offsets at shallow layers to explicitly align infrared features to visible features in the spatial domain. Additionally, a Dual-Branch Brightness Weighting (DBW) module derives patch-wise",
        "score": 0.3059777,
        "raw_content": null,
        "id": 232
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-34756-z",
        "title": "A multiscale transformer with spatial attention for ...",
        "content": "Article  ADS   Google Scholar\n7. Hong, D. et al. Spectralformer: Rethinking hyperspectral image classification with transformers. IEEE Transactions on Geoscience and Remote Sensing 60, 1\u201315 (2021).\n\n   Article   Google Scholar\n8. Zhou, P., Han, J., Cheng, G. & Zhang, B. Learning compact and discriminative stacked autoencoder for hyperspectral image classification. IEEE Transactions on Geoscience and Remote Sensing 57(7), 4823\u20134833 (2019).\n\n   Article  ADS   Google Scholar\n9. Zheng, X., Yuan, Y. & Lu, X. Dimensionality reduction by spatial-spectral preservation in selected bands. IEEE Transactions on Geoscience and Remote Sensing 55(9), 5185\u20135197 (2017). [...] The Houston 2013 (H-13) dataset was gathered using the ITRES CASI-1500 sensor and covers the University of Houston premises and the neighbouring rural regions. This dataset has been publicly employed for assessing the effectiveness of HSI classification methods. This dataset has an image size of \\(349 \\times 1905\\) pixels and includes 144 spectral bands. It encompasses 15 challenging distinct classes, making it a valuable resource for land cover classification studies. The dataset samples have been divided into training, validation, and test sets, with the allocation details provided in Table 4. [...] Furthermore, a hybrid approach incorporating both CNN and Transformer networks48.\") was proposed to effectively calculate spectral-spatial and semantic features for HSI classification. In66.\"), Swin Transformer was presented, which calculates representations through a shifted window strategy. This approach allows for efficient processing of visual data with diverse scales and higher resolutions. Adopting this strategy effectively addresses the challenges presented by scale and resolution disparities, rendering it well-suited for HSI classification.\n\n## Proposed methodology",
        "score": 0.2640859,
        "raw_content": null,
        "id": 233
    },
    {
        "url": "https://arxiv.org/html/2507.22791v1",
        "title": "Modality-Aware Feature Matching: A Comprehensive ...",
        "content": "In this survey, our discussion addresses unique challenges and methodologies related to single-modal feature matching (e.g., RGB, depth, medical imaging) and cross-modal scenarios (e.g., medical image registration, vision-language integration). We delineate the progression from classical detector-based pipelines towards contemporary detector-free solutions. Figure 1 illustrates the overall pipeline of the survey, clearly mapping the evolution of feature matching methodologies across various data modalities. Furthermore, Figure 2 offers representative examples of the results of the matching of modality-awarecharacteristics.\n\n## 2. RGB Images [...] In summary, deep learning-based methods have greatly advanced the state of RGB image matching. They have produced more repeatable detectors (learning where to detect points that survive viewpoint/illumination changes), more discriminative descriptors (learning how to describe patches for higher distinctiveness), and even entirely new paradigms for matching (learning to match with attention mechanisms, and end-to-end trainable pipelines). Table 1 highlights some of the influential methods from both the handcrafted and deep learning eras. These modern techniques, when evaluated on challenging benchmarks, significantly outperform classic methods in terms of matching precision and robustness, although often at a higher computational cost. Nonetheless, due to their task-specific optimizations [...] ## 2. RGB Images\n\nFeature matching in RGB images traditionally follows a pipeline of feature detection, feature description, and feature matching. Over the decades, a lot of methods have been proposed for each stage of this pipeline, from handcrafted approaches to more recent deep learning-based techniques.\n\n### 2.1. Handcrafted Feature Detectors and Descriptors",
        "score": 0.26285648,
        "raw_content": null,
        "id": 234
    },
    {
        "url": "https://link.springer.com/article/10.1007/s10462-025-11169-y",
        "title": "A review of hyperspectral image classification based on graph ...",
        "content": "\")) of hyperspectral imaging (HSI) pose significant challenges to achieving accurate HSIC (Sawant and Prabukumar 2020 A survey of band selection techniques for hyperspectral image classification. J Spectr Imaging 9(1):5. \n                  \n                  \n                \")). Specifically, the large number of spectral bands and data points increases computational complexity (Ullah et al. 2024 Conventional to deep ensemble methods for hyperspectral image classification: a comprehensive survey. IEEE J Sel Top Appl Earth Obs Remote Sens. [...] ### 2.2 Technologies for hyperspectral image classification\n\nOptical remote sensing images can be categorized into three main types based on the number of spectral bands: panchromatic images with a single band, multiband RGB images, and hyperspectral images (HSI) containing hundreds of continuous, narrow spectral bands (Mather and Tso 2016 Classification methods for remotely sensed data. CRC Press, Boca Raton\")). HSI, with its unique technical advantages, achieves a deep integration of imaging and spectral technologies, enabling comprehensive data acquisition in both two-dimensional spatial geometry and one-dimensional spectral information. [...] Although deep learning has achieved remarkable breakthroughs in hyperspectral image classification (HSIC), the unique characteristics of hyperspectral image (HSI) data still present numerous challenges (Wan et al. 2020a Hyperspectral image classification with context-aware dynamic graph convolutional network. IEEE Trans Geosci Remote Sens 59(1):597\u2013612.",
        "score": 0.21413255,
        "raw_content": null,
        "id": 235
    },
    {
        "url": "https://eureka.patsnap.com/article/self-attention-in-vision-transformers-why-it-captures-long-range-dependencies",
        "title": "Self-Attention in Vision Transformers: Why It Captures Long-Range Dependencies",
        "content": "Conclusion  \n  \nVision Transformers, with their self-attention mechanisms, have demonstrated a remarkable ability to capture long-range dependencies in images, setting a new standard in computer vision. By considering every part of an image in relation to every other part, ViTs provide a comprehensive understanding that surpasses traditional methods. As research continues to advance, we can expect Vision Transformers to play an increasingly prominent role in the future of image processing, unlocking new possibilities for innovation and application. [...] Capturing Long-Range Dependencies  \n  \nOne of the standout features of self-attention in ViTs is its ability to capture long-range dependencies. Traditional CNNs are constrained by the size of their receptive fields, which limits their ability to consider distant relationships in an image. In contrast, self-attention allows each image patch to attend to all other patches, regardless of their spatial distance. [...] This is achieved by dividing an image into a sequence of patches, treating these patches similarly to words in a sentence. Each patch is linearly embedded into a fixed-size vector, and these vectors are then passed through layers of self-attention. This process allows the model to weigh the importance of different parts of the image when making predictions, effectively capturing both local and global context.  \n  \nCapturing Long-Range Dependencies",
        "score": 0.99985456,
        "raw_content": null,
        "id": 236
    },
    {
        "url": "https://eureka.patsnap.com/article/how-do-transformers-handle-long-range-dependencies",
        "title": "How Do Transformers Handle Long-Range Dependencies?",
        "content": "Self-Attention Mechanism  \n  \nAt the heart of the transformer architecture is the self-attention mechanism. This mechanism allows transformers to weigh the importance of each word in a sequence relative to others, regardless of their distance. Self-attention computes a representation of each word by considering the entire sequence, enabling the model to capture dependencies over long ranges efficiently.  \n  \nThe self-attention mechanism works by creating three vectors for each word: query, key, and value. These vectors are used to calculate attention scores, which determine how much focus should be given to each word in the sequence. This process allows transformers to create context-aware representations of words, making them adept at understanding intricate dependencies. [...] Layer Stacking and Multi-Head Attention  \n  \nTransformers consist of multiple layers, each containing a self-attention mechanism followed by feed-forward neural networks. These layers are stacked on top of each other, enabling the model to learn complex patterns and dependencies. Moreover, transformers employ a technique called multi-head attention, where multiple self-attention mechanisms run in parallel at each layer. This allows the model to focus on different parts of the sequence simultaneously, enhancing its ability to capture a variety of dependencies.  \n  \nHandling Long Sequences [...] Conclusion  \n  \nTransformers have revolutionized the way we handle long-range dependencies in NLP tasks. Their innovative architecture, characterized by self-attention mechanisms, positional encoding, and multi-head attention, allows them to effectively capture complex relationships in sequences. As a result, transformers have set new benchmarks in NLP, proving to be a powerful tool for processing language data over long ranges. With ongoing research and development, the capabilities of transformers continue to expand, promising even more sophisticated handling of long-range dependencies in the future.",
        "score": 0.9994642,
        "raw_content": null,
        "id": 237
    },
    {
        "url": "https://www.mdpi.com/2072-4292/17/21/3532",
        "title": "Advances on Multimodal Remote Sensing Foundation ...",
        "content": "The Vision Transformer (ViT) is a DL model that uses the Transformer network architecture to handle visual image tasks. Its core technologies include the multi-head self-attention mechanism (MHSA) and positional encoding [83,84,85]. The basic principle of the Vision Transformer is to divide the image into several small blocks containing various ground feature pixels, regard each small block as an image sequence unit, and then extract the spatial and temporal features of the image through the encoder\u2013decoder of the Transformer layer. The self-attention mechanism can perform weighted summation for each position in the input image sequence and extract the global dependence of the image sequence by modeling interactions among any pixels in the input image sequence . The Vision Transformer",
        "score": 0.9992084,
        "raw_content": null,
        "id": 238
    },
    {
        "url": "https://towardsai.net/p/machine-learning/attention-is-all-you-need-a-deep-dive-into-the-revolutionary-transformer-architecture",
        "title": "Attention Is All You Need - A Deep Dive into the ...",
        "content": "In summary, attention mechanisms in the Transformer allow the model to dynamically focus on different parts of the input sequence, capturing both local and long-range dependencies. The scaled dot-product attention mechanism is particularly efficient and effective, making it a cornerstone of the Transformer architecture. In the next section, we will explore how self-attention uses this mechanism to capture relationships between words within a sequence.\n\n5. Self-Attention in Detail\n\nSelf-attention is a specific type of attention where the queries, keys, and values all come from the same source. In the context of the Transformer, this means that each position in a sequence can attend to all positions in the same sequence, allowing the model to capture intricate relationships between words. [...] Image 20\nWhy Self-Attention Works\n\nSelf-attention has several key advantages over traditional sequence processing methods:\n\n1.   Captures Long-Range Dependencies: Unlike RNNs, which struggle with long-range dependencies, self-attention connects any two positions directly, regardless of their distance in the sequence.\n\n2. Parallel Computation: All positions can be processed in parallel, unlike the sequential nature of RNNs.\n\n3. Interpretable Attention Weights: The attention weights provide insights into which words the model is focusing on, adding a level of interpretability.\n\n4. Constant Path Length: The number of operations required to connect positions in the network is constant, not dependent on sequence length.\n\nImage 21 [...] Transformer-XL (2019): Extended the Transformer with a segment-level recurrence mechanism and relative positional encoding, enabling it to learn dependencies beyond a fixed-length context.\n   Reformer (2020): Used locality-sensitive hashing to reduce the computational complexity of self-attention from O(n\u00b2) to O(n log n), making it more efficient for long sequences.\n   Linformer (2020): Reduced the complexity of self-attention to O(n) by projecting the length dimension of keys and values.\n   Performer (2020): Approximated the attention matrix using kernelization techniques, reducing memory and computational requirements.\n   Vision Transformer (ViT, 2020): Adapted the Transformer for computer vision by treating image patches as sequence elements.",
        "score": 0.9989183,
        "raw_content": null,
        "id": 239
    },
    {
        "url": "https://arxiv.org/html/2410.16602v3",
        "title": "Foundation Models for Remote Sensing and Earth ...",
        "content": "Transformers. Transformers [25, 26], as depicted in Fig. 6, are designed to process sequence data using self-attention mechanisms, enabling them to capture relationships between data points regardless of their positional distance. Unlike CNNs, which emphasize local features, transformers excel at modeling global dependencies, making them particularly effective for long-range interactions. A significant advantage of transformers in FMs is their ability to integrate across multiple modalities, enabling them to process a diverse range of inputs, such as visual data (e.g., images, depth, thermal) and non-visual data (e.g., text, 3D point clouds, audio). This cross-modality integration paves the way for unified RSFMs capable of handling a wide spectrum of geospatial data modalities.",
        "score": 0.998259,
        "raw_content": null,
        "id": 240
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-34756-z",
        "title": "A multiscale transformer with spatial attention for ...",
        "content": "Similarly, in41, 67 (2017).\"), an extended investigation was conducted on 3D CNNs for spectral\u2013spatial classification, utilizing input cubes with reduced spatial dimensions from HSIs. These models were designed to generate thematic maps by directly processing raw HSIs. Despite the achievements of CNN-based methods in extracting spatial and spectral information, they possess specific constraints. In40, 6232\u20136251 (2016).\"), a 3D CNN was employed to directly and efficiently learn spectral-spatial features from the original HSI, showcasing promising outcomes in terms of classification performance. Furthermore, a hybrid approach incorporating both CNN and Transformer networks48.\") was proposed to effectively calculate spectral-spatial and semantic features for HSI classification. In this work, a novel framework, MTSA-Net, is introduced for HSI classification, harnessing the combined strengths of spatial attention and multiscale transformers to effectively utilize the spatial-spectral information in hyperspectral data. Spectral-spatial feature extraction for hyperspectral image classification: A dimension reduction and deep learning approach. Learning deep hierarchical spatial-spectral features for hyperspectral image classification based on residual 3d\u20132d cnn.",
        "score": 0.9971328,
        "raw_content": null,
        "id": 241
    },
    {
        "url": "https://link.springer.com/article/10.1007/s10462-025-11169-y",
        "title": "A review of hyperspectral image classification based on graph ...",
        "content": "*et al.* A review of hyperspectral image classification based on graph neural networks. Hybrid models based on graph neural networks for hyperspectral image (HSI) classification combine GNNs with other deep learning models. \")) combined the multi-view deep autoencoder (MVDAE) and the semi-supervised graph convolutional network (SSGCN) to propose a novel approach for spectral-spatial classification of HSI, called MV-DNNet. MV-DNNet leverages a small number of labeled samples to integrate spectral and spatial features, significantly enhancing hyperspectral image classification performance. * Bai J, Ding B, Xiao Z, Jiao L, Chen H, Regan AC (2021) Hyperspectral image classification based on deep attention graph convolutional network. * Dong Y, Liu Q, Du B, Zhang L (2022) Weighted feature fusion of convolutional neural network and graph attention network for hyperspectral image classification. * Yang P, Tong L, Qian B, Gao Z, Yu J, Xiao C (2020) Hyperspectral image classification with spectral and spatial graph using inductive representation learning network.",
        "score": 0.99101454,
        "raw_content": null,
        "id": 242
    },
    {
        "url": "https://ieeexplore.ieee.org/iel8/4609443/4609444/10877784.pdf",
        "title": "Frequency-Aware Hierarchical Mamba for Hyperspectral ...",
        "content": "by P Zhuang \u00b7 2025 \u00b7 Cited by 19 \u2014 For HSI classification, MambaHSI [27] utilized Mamba blocks to si- multaneously model long-range spatial interactions and extract spectral features, achieving",
        "score": 0.9522199,
        "raw_content": null,
        "id": 243
    },
    {
        "url": "https://www.sciencedirect.com/science/article/abs/pii/S0925231225011002",
        "title": "A comprehensive survey for Hyperspectral Image ...",
        "content": "by M Ahmad \u00b7 2025 \u00b7 Cited by 49 \u2014 Unlike traditional methods, Hyperspectral Images (HSIs) provide a continuous spectrum through numerous narrow bands, enabling precise material characterization",
        "score": 0.9092645,
        "raw_content": null,
        "id": 244
    },
    {
        "url": "https://www.mdpi.com/2072-4292/17/14/2489",
        "title": "Spatial and Spectral Structure-Aware Mamba Network for ...",
        "content": "Current Mamba-based HSI classification methods [24,25,26,27] typically flatten the 2D spatial structure into the 1D sequence and then use a fixed scanning strategy to extract spatial features from the 1D sequence, which inevitably changes the spatial relationship between pixels, destroys the inherent contextual information in the image, and affects the classification results. Classification maps produced by various methods applied to the Pavia University dataset: (**a**) ground truth; (**b**) 2DCNN; (**c**) 3DCNN; (**d**) HybridSN; (**e**) SPRN; (**f**) SpectralFormer; (**g**) SSFTT; (**h**) MorphFormer; (**i**) GSC-VIT; (**j**) 3DSS-Mamba; (**k**) SS-Mamba; (**l**) SSUM; (**m**) DADFMamba. Classification maps produced by various methods applied to the Pavia University dataset: (**a**) ground truth; (**b**) 2DCNN; (**c**) 3DCNN; (**d**) HybridSN; (**e**) SPRN; (**f**) SpectralFormer; (**g**) SSFTT; (**h**) MorphFormer; (**i**) GSC-VIT; (**j**) 3DSS-Mamba; (**k**) SS-Mamba; (**l**) SSUM; (**m**) DADFMamba. Classification maps produced by various methods applied to the Pavia University dataset: (**a**) ground truth; (**b**) 2DCNN; (**c**) 3DCNN; (**d**) HybridSN; (**e**) SPRN; (**f**) SpectralFormer; (**g**) SSFTT; (**h**) MorphFormer; (**i**) GSC-VIT; (**j**) 3DSS-Mamba; (**k**) SS-Mamba; (**l**) SSUM; (**m**) DADFMamba.",
        "score": 0.8864204,
        "raw_content": null,
        "id": 245
    },
    {
        "url": "https://www.geeksforgeeks.org/deep-learning/vision-transformer-vit-architecture/",
        "title": "Vision Transformer (ViT) Architecture",
        "content": "Vision Transformer (ViT) is a deep learning architecture that applies the Transformer model to images. Instead of relying on convolutions,",
        "score": 0.78788257,
        "raw_content": null,
        "id": 246
    },
    {
        "url": "https://manimalakumar-29300.medium.com/generative-ai-model-architecture-part3-vision-transformer-363a0a81a7c2",
        "title": "Generative AI Model Architecture \u2014 Part3: Vision Transformer",
        "content": "# Generative AI Model Architecture \u2014 Part3: Vision Transformer. Vision Transformer (ViT) adapts the Transformer architecture \u2014 originally designed to process sequential data \u2014 to make sense of non-sequential data. There were specialized CNN model architectures for each of the fundamental vision tasks: image classification, object detection, segmentation, etc. ViTs develop a broad understanding of the image content, and overcome such limitations of CNNs. LVMs \u2014 the modern computer vision models \u2014 are increasingly built on ViT-based architectures. Since ViTs process image patches in parallel, position embeddings tell the model where each patch belongs. Self-distillation is a student-teacher architecture, where a student model learns to predict the output of a teacher model. This means the teacher is not a fixed, pre-trained model \u2014 over time, as the student improves, the teacher is updated to be a moving average of the student, ensuring the teacher remains a reliable target and promoting progressive learning. Part 4: Generative AI Model Architecture \u2014 Part4: Vision Language Model.",
        "score": 0.7602419,
        "raw_content": null,
        "id": 247
    },
    {
        "url": "https://blog.roboflow.com/vision-transformers/",
        "title": "Vision Transformers Explained: The Future of Computer ...",
        "content": "Transformers are a powerful deep learning architecture originally developed for natural language processing (NLP) tasks such as machine translation,",
        "score": 0.6659544,
        "raw_content": null,
        "id": 248
    },
    {
        "url": "https://www.codecademy.com/article/vision-transformers-working-architecture-explained",
        "title": "How do Vision Transformers Work? Architecture Explained",
        "content": "Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial. This architectural shift enables ViTs to use the powerful self-attention mechanism that made transformers successful in NLP, allowing them to model long-range dependencies and global relationships in images more effectively than CNNs. The ViT architecture consists of four main components: patch embedding, positional encoding, transformer encoder, and classification head. The original vision transformer architecture proposed in the paper \u201cAn Image Is Worth 16X16 Words\u201d contains different components such as patch embedding, positional encoding, transformer encoder, multilayer perceptron head, etc, and looks as follows:. * **Global feature modeling**: In vision transformers, every patch of an image interacts with every other patch during the self-attention mechanism. It\u2019s a self-supervised learning framework for vision transformers that allows a ViT model to learn robust and invariant features from images without needing human-labeled data. Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial.",
        "score": 0.61363083,
        "raw_content": null,
        "id": 249
    },
    {
        "url": "https://www.coursera.org/articles/transformers-vs-convolutional-neural-networks",
        "title": "Transformers vs. Convolutional Neural Networks",
        "content": "Transformers and convolutional neural networks are both powerful deep learning algorithms for computer vision, but they work differently and have different strengths and weaknesses. If you want to use a deep learning neural network to extract meaningful data from images, you may decide to use a convolutional neural network or a visual transformer. Explore transformers versus convolutional neural networks to learn how each of these models works, what you can use them for, and how they compare to each other. A transformer is a deep learning neural network that uses an encoder/decoder architecture to first break down the input into the base characteristics that define the data, then use its understanding of patterns to rebuild the data with original but similar information. Transformer models and convolutional neural networks are both deep learning models that you can use for computer vision, but they are different in how they empower computer vision.",
        "score": 0.5735118,
        "raw_content": null,
        "id": 250
    },
    {
        "url": "https://www.geeksforgeeks.org/deep-learning/vision-transformer-vit-architecture/",
        "title": "Vision Transformer (ViT) Architecture",
        "content": "Vision Transformer (ViT) is a deep learning architecture that applies the Transformer model to images. Instead of relying on convolutions,",
        "score": 0.78788257,
        "raw_content": null,
        "id": 251
    },
    {
        "url": "https://manimalakumar-29300.medium.com/generative-ai-model-architecture-part3-vision-transformer-363a0a81a7c2",
        "title": "Generative AI Model Architecture \u2014 Part3: Vision Transformer",
        "content": "# Generative AI Model Architecture \u2014 Part3: Vision Transformer. Vision Transformer (ViT) adapts the Transformer architecture \u2014 originally designed to process sequential data \u2014 to make sense of non-sequential data. There were specialized CNN model architectures for each of the fundamental vision tasks: image classification, object detection, segmentation, etc. ViTs develop a broad understanding of the image content, and overcome such limitations of CNNs. LVMs \u2014 the modern computer vision models \u2014 are increasingly built on ViT-based architectures. Since ViTs process image patches in parallel, position embeddings tell the model where each patch belongs. Self-distillation is a student-teacher architecture, where a student model learns to predict the output of a teacher model. This means the teacher is not a fixed, pre-trained model \u2014 over time, as the student improves, the teacher is updated to be a moving average of the student, ensuring the teacher remains a reliable target and promoting progressive learning. Part 4: Generative AI Model Architecture \u2014 Part4: Vision Language Model.",
        "score": 0.7602419,
        "raw_content": null,
        "id": 252
    },
    {
        "url": "https://blog.roboflow.com/vision-transformers/",
        "title": "Vision Transformers Explained: The Future of Computer ...",
        "content": "Transformers are a powerful deep learning architecture originally developed for natural language processing (NLP) tasks such as machine translation,",
        "score": 0.6659544,
        "raw_content": null,
        "id": 253
    },
    {
        "url": "https://www.codecademy.com/article/vision-transformers-working-architecture-explained",
        "title": "How do Vision Transformers Work? Architecture Explained",
        "content": "Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial. This architectural shift enables ViTs to use the powerful self-attention mechanism that made transformers successful in NLP, allowing them to model long-range dependencies and global relationships in images more effectively than CNNs. The ViT architecture consists of four main components: patch embedding, positional encoding, transformer encoder, and classification head. The original vision transformer architecture proposed in the paper \u201cAn Image Is Worth 16X16 Words\u201d contains different components such as patch embedding, positional encoding, transformer encoder, multilayer perceptron head, etc, and looks as follows:. * **Global feature modeling**: In vision transformers, every patch of an image interacts with every other patch during the self-attention mechanism. It\u2019s a self-supervised learning framework for vision transformers that allows a ViT model to learn robust and invariant features from images without needing human-labeled data. Learn how to use Python to build image classification models using CNNs and vision transformers in this PyTorch tutorial.",
        "score": 0.61363083,
        "raw_content": null,
        "id": 254
    },
    {
        "url": "https://www.coursera.org/articles/transformers-vs-convolutional-neural-networks",
        "title": "Transformers vs. Convolutional Neural Networks",
        "content": "Transformers and convolutional neural networks are both powerful deep learning algorithms for computer vision, but they work differently and have different strengths and weaknesses. If you want to use a deep learning neural network to extract meaningful data from images, you may decide to use a convolutional neural network or a visual transformer. Explore transformers versus convolutional neural networks to learn how each of these models works, what you can use them for, and how they compare to each other. A transformer is a deep learning neural network that uses an encoder/decoder architecture to first break down the input into the base characteristics that define the data, then use its understanding of patterns to rebuild the data with original but similar information. Transformer models and convolutional neural networks are both deep learning models that you can use for computer vision, but they are different in how they empower computer vision.",
        "score": 0.5735118,
        "raw_content": null,
        "id": 255
    },
    {
        "url": "https://www.nature.com/articles/s41598-025-34756-z",
        "title": "A multiscale transformer with spatial attention for ...",
        "content": "each tailored to extract valuable insights from the multi-dimensional HSI data. Among these approaches, the k-nearest neighbour18, 11514\u201311521. [...] designs and lightweight transformer modules to further reduce computational costs without compromising overall performance, making the framework even more suitable for HSI classification tasks. [...] To tackle these challenges, we propose MTSA-Net, a novel multiscale transformer framework with spatial attention for HSI classification. The model begins with a 3D convolution layer to extract shallow spectral\u2013spatial features, followed by a 2D convolution layer and a spatial attention module. This design reduces feature redundancy, mitigates inaccuracies that often arise in deeper networks, and emphasizes the most discriminative spatial features, thereby alleviating the limited spatial resolution of HSIs. The refined feature vectors are then processed by multiple parallel transformer encoder branches with varying hidden dimensions, enabling simultaneous modeling of fine-grained local details, intermediate relationships, and global representations. Finally, a multiscale feature fusion",
        "score": 0.98975134,
        "raw_content": null,
        "id": 256
    },
    {
        "url": "https://worldmodelbench.github.io/static/pdfs/7_Adaptive_Attention_Guided_Ma.pdf",
        "title": "Adaptive Attention-Guided Masking in Vision Transformers ...",
        "content": "Art. no.\n5511817. 2, 5, 7 Adaptive Attention-Guided Masking in Vision Transformers for Self-Supervised Hyperspectral Feature Learning Supplementary Material 6. Appendix In recent years, world models like Vision Transformers (ViTs)  have demonstrated impressive performance in various computer vision [6, 8, 13] tasks, including hyper-spectral image (HSI) classification . However, their ap-plication to HSI faces several limitations due to the unique challenges of hyperspectral data.\nChallenges. First, the high dimensionality of HSI, with several spectral bands, complicates processing for conven-tional ViTs, which are optimized for RGB images with only three channels. This necessitates extensive modifications to handle spectral complexity. [...] Hyperspectral image (HSI) classification has rapidly ad-vanced with diverse methods tackling spectral-spatial data challenges.\nEarly models like 2-DCNN  combined simple 2D convolutions with fully connected layers, while SPRN  improved spatial features using attention mech-anisms and residual blocks. 3-DCNN  introduced 3D convolutions to jointly capture spectral and spatial infor-mation.\nHybrid models like HybridSN  merged 2D and 3D CNNs for richer spectral-spatial representations.\nThe emergence of transformer-based approaches marked a pivotal shift, emphasizing efficient feature extraction and long-range dependency modeling. GAHT  and Mor-phFormer  leveraged CNN-transformer hybrids and self-attention for enhanced spectral-spatial learning. [...] 4. Analysis of Results In this section, we comprehensively analyze results in var-ious scenarios with three HSI datasets : Indian Pines, Salinas, and Botswana using the two performance metrics: Overall Accuracy (OA) and Cohen\u2019s Kappa coefficient (\u03ba).",
        "score": 0.98975134,
        "raw_content": null,
        "id": 257
    },
    {
        "url": "https://www.geeksforgeeks.org/deep-learning/vision-transformer-vit-architecture/",
        "title": "Vision Transformer (ViT) Architecture",
        "content": "Vision Transformer (ViT) is a deep learning architecture that applies the Transformer model to images. Instead of relying on convolutions,",
        "score": 0.78788257,
        "raw_content": null,
        "id": 258
    }
] pixel can be expressed as: [...
] TABLE IV: Deep Learning Models and their Applications in HSI and Multimodal HSI", "score": 0.98934746, "raw_content": null, "id": 258}]